{"ast":null,"code":"/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { upcastType, util } from '@tensorflow/tfjs-core';\nimport { mapActivationToShaderProgram } from '../kernel_utils/kernel_funcs_utils';\nimport { MatMulPackedProgram } from '../mulmat_packed_gpu';\nimport { multiply } from './Multiply';\nimport { reshape } from './Reshape';\nimport { sum } from './Sum';\nimport { transpose } from './Transpose'; // Empirically determined minimal shared dimension in matmul before we forward\n// to a.mul(b).sum() in order to take advantage of GPU parallelism. See\n// https://github.com/tensorflow/tfjs-core/pull/1379 for benchmarks.\n\nexport var MATMUL_SHARED_DIM_THRESHOLD = 1000;\nexport function batchMatMulImpl(_ref) {\n  var a = _ref.a,\n      b = _ref.b,\n      transposeA = _ref.transposeA,\n      transposeB = _ref.transposeB,\n      backend = _ref.backend,\n      _ref$bias = _ref.bias,\n      bias = _ref$bias === void 0 ? null : _ref$bias,\n      _ref$preluActivationW = _ref.preluActivationWeights,\n      preluActivationWeights = _ref$preluActivationW === void 0 ? null : _ref$preluActivationW,\n      _ref$leakyreluAlpha = _ref.leakyreluAlpha,\n      leakyreluAlpha = _ref$leakyreluAlpha === void 0 ? 0 : _ref$leakyreluAlpha,\n      _ref$activation = _ref.activation,\n      activation = _ref$activation === void 0 ? null : _ref$activation;\n  var aRank = a.shape.length;\n  var bRank = b.shape.length;\n  var innerShapeA = transposeA ? a.shape[aRank - 2] : a.shape[aRank - 1];\n  var innerShapeB = transposeB ? b.shape[bRank - 1] : b.shape[bRank - 2];\n  var outerShapeA = transposeA ? a.shape[aRank - 1] : a.shape[aRank - 2];\n  var outerShapeB = transposeB ? b.shape[bRank - 2] : b.shape[bRank - 1];\n  var outerDimsA = a.shape.slice(0, -2);\n  var outerDimsB = b.shape.slice(0, -2);\n  var batchDimA = util.sizeFromShape(outerDimsA);\n  var batchDimB = util.sizeFromShape(outerDimsB);\n  var batchDimsCompatible = batchDimA === batchDimB || batchDimA === 1 || batchDimB === 1;\n  util.assert(aRank >= 2 && bRank >= 2 && batchDimsCompatible, function () {\n    return \"Error in matMul: the input batch dimensions must either be the \" + \"same or at least one input batch dimension must be 1. Got input \" + \"batch dimensions of (\".concat(outerDimsA, \") and (\").concat(outerDimsB, \").\");\n  });\n  var outShapeOuterDims = batchDimA > batchDimB ? a.shape.slice(0, -2) : b.shape.slice(0, -2);\n  var outShape = outShapeOuterDims.concat([outerShapeA, outerShapeB]);\n  util.assert(innerShapeA === innerShapeB, function () {\n    return \"Error in matMul: inner shapes (\".concat(innerShapeA, \") and (\") + \"\".concat(innerShapeB, \") of Tensors with shapes \").concat(a.shape, \" and \") + \"\".concat(b.shape, \" and transposeA=\").concat(transposeA) + \" and transposeB=\".concat(transposeB, \" must match.\");\n  });\n  var a3dShape = transposeA ? [batchDimA, innerShapeA, outerShapeA] : [batchDimA, outerShapeA, innerShapeA];\n  var b3dShape = transposeB ? [batchDimB, outerShapeB, innerShapeB] : [batchDimB, innerShapeB, outerShapeB]; // The rest of the implementation is designed to operate on rank-3 tensors\n\n  var a3d = reshape({\n    inputs: {\n      x: a\n    },\n    backend: backend,\n    attrs: {\n      shape: a3dShape\n    }\n  });\n  var b3d = reshape({\n    inputs: {\n      x: b\n    },\n    backend: backend,\n    attrs: {\n      shape: b3dShape\n    }\n  });\n  var intermediates = [a3d, b3d];\n  var batchDim = Math.max(batchDimA, batchDimB);\n  var sharedDim = transposeA ? a3d.shape[1] : a3d.shape[2];\n  var hasBias = bias != null;\n  var hasPreluActivationWeights = preluActivationWeights != null;\n  var hasLeakyreluAlpha = activation === 'leakyrelu';\n  var fusedActivation = activation != null ? mapActivationToShaderProgram(activation, true) : null;\n  var containsFusedOps = hasBias || hasPreluActivationWeights || hasLeakyreluAlpha || fusedActivation != null;\n  var out; // Since the matrices are vectors, it is faster to call mul().sum()\n  // because sum() is O(sqrt(N)) due to divide-and-conquer.\n\n  if ((outerShapeA === 1 || outerShapeB === 1) && sharedDim > MATMUL_SHARED_DIM_THRESHOLD && containsFusedOps === false) {\n    var aVec = a3d;\n    var bVec = b3d;\n\n    if (transposeA) {\n      aVec = transpose({\n        inputs: {\n          x: a3d\n        },\n        backend: backend,\n        attrs: {\n          perm: [0, 2, 1]\n        }\n      });\n      intermediates.push(aVec);\n    }\n\n    if (transposeB) {\n      bVec = transpose({\n        inputs: {\n          x: b3d\n        },\n        backend: backend,\n        attrs: {\n          perm: [0, 2, 1]\n        }\n      });\n      intermediates.push(bVec);\n    }\n\n    var shouldReshapeA = outerShapeB !== 1;\n    var shouldReshapeB = outerShapeB === 1;\n    var aVec3d = aVec;\n\n    if (shouldReshapeA) {\n      aVec3d = reshape({\n        inputs: {\n          x: aVec\n        },\n        backend: backend,\n        attrs: {\n          shape: [batchDim, sharedDim, 1]\n        }\n      });\n      intermediates.push(aVec3d);\n    }\n\n    var axis = outerShapeB === 1 ? 2 : 1;\n    var bVec3d = bVec;\n\n    if (shouldReshapeB) {\n      bVec3d = reshape({\n        inputs: {\n          x: bVec\n        },\n        backend: backend,\n        attrs: {\n          shape: [batchDim, 1, sharedDim]\n        }\n      });\n      intermediates.push(bVec3d);\n    }\n\n    var product = multiply({\n      inputs: {\n        a: aVec3d,\n        b: bVec3d\n      },\n      backend: backend\n    });\n    out = sum({\n      inputs: {\n        x: product\n      },\n      backend: backend,\n      attrs: {\n        axis: axis,\n        keepDims: true\n      }\n    });\n    intermediates.push(product);\n  } else {\n    var dtype = upcastType(a.dtype, b.dtype);\n    var program = new MatMulPackedProgram(a3dShape, b3dShape, [batchDim, outerShapeA, outerShapeB], transposeA, transposeB, hasBias, fusedActivation, hasPreluActivationWeights, hasLeakyreluAlpha);\n    var inputs = [a3d, b3d];\n\n    if (bias != null) {\n      inputs.push(bias);\n    }\n\n    if (hasPreluActivationWeights) {\n      inputs.push(preluActivationWeights);\n    }\n\n    if (hasLeakyreluAlpha) {\n      var $leakyreluAlpha = backend.makeTensorInfo([], 'float32', util.createScalarValue(leakyreluAlpha, 'float32'));\n      inputs.push($leakyreluAlpha);\n      intermediates.push($leakyreluAlpha);\n    }\n\n    out = backend.runWebGLProgram(program, inputs, dtype);\n  }\n\n  var outReshaped = reshape({\n    inputs: {\n      x: out\n    },\n    backend: backend,\n    attrs: {\n      shape: outShape\n    }\n  });\n  intermediates.push(out);\n\n  for (var _i = 0, _intermediates = intermediates; _i < _intermediates.length; _i++) {\n    var i = _intermediates[_i];\n    backend.disposeIntermediateTensorInfo(i);\n  }\n\n  return outReshaped;\n}","map":{"version":3,"sources":["../../src/kernels/BatchMatMul_impl.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;AAeG;AAEH,SAAkC,UAAlC,EAA8C,IAA9C,QAAyD,uBAAzD;AAGA,SAAQ,4BAAR,QAA2C,oCAA3C;AACA,SAAQ,mBAAR,QAAkC,sBAAlC;AAEA,SAAQ,QAAR,QAAuB,YAAvB;AACA,SAAQ,OAAR,QAAsB,WAAtB;AACA,SAAQ,GAAR,QAAkB,OAAlB;AACA,SAAQ,SAAR,QAAwB,aAAxB,C,CAEA;AACA;AACA;;AACA,OAAO,IAAM,2BAA2B,GAAG,IAApC;AAcP,OAAM,SAAU,eAAV,OAUc;AAAA,MATlB,CASkB,QATlB,CASkB;AAAA,MARlB,CAQkB,QARlB,CAQkB;AAAA,MAPlB,UAOkB,QAPlB,UAOkB;AAAA,MANlB,UAMkB,QANlB,UAMkB;AAAA,MALlB,OAKkB,QALlB,OAKkB;AAAA,uBAJlB,IAIkB;AAAA,MAJlB,IAIkB,0BAJX,IAIW;AAAA,mCAHlB,sBAGkB;AAAA,MAHlB,sBAGkB,sCAHO,IAGP;AAAA,iCAFlB,cAEkB;AAAA,MAFlB,cAEkB,oCAFD,CAEC;AAAA,6BADlB,UACkB;AAAA,MADlB,UACkB,gCADL,IACK;AAClB,MAAM,KAAK,GAAG,CAAC,CAAC,KAAF,CAAQ,MAAtB;AACA,MAAM,KAAK,GAAG,CAAC,CAAC,KAAF,CAAQ,MAAtB;AAEA,MAAM,WAAW,GAAG,UAAU,GAAG,CAAC,CAAC,KAAF,CAAQ,KAAK,GAAG,CAAhB,CAAH,GAAwB,CAAC,CAAC,KAAF,CAAQ,KAAK,GAAG,CAAhB,CAAtD;AACA,MAAM,WAAW,GAAG,UAAU,GAAG,CAAC,CAAC,KAAF,CAAQ,KAAK,GAAG,CAAhB,CAAH,GAAwB,CAAC,CAAC,KAAF,CAAQ,KAAK,GAAG,CAAhB,CAAtD;AAEA,MAAM,WAAW,GAAG,UAAU,GAAG,CAAC,CAAC,KAAF,CAAQ,KAAK,GAAG,CAAhB,CAAH,GAAwB,CAAC,CAAC,KAAF,CAAQ,KAAK,GAAG,CAAhB,CAAtD;AACA,MAAM,WAAW,GAAG,UAAU,GAAG,CAAC,CAAC,KAAF,CAAQ,KAAK,GAAG,CAAhB,CAAH,GAAwB,CAAC,CAAC,KAAF,CAAQ,KAAK,GAAG,CAAhB,CAAtD;AAEA,MAAM,UAAU,GAAG,CAAC,CAAC,KAAF,CAAQ,KAAR,CAAc,CAAd,EAAiB,CAAC,CAAlB,CAAnB;AACA,MAAM,UAAU,GAAG,CAAC,CAAC,KAAF,CAAQ,KAAR,CAAc,CAAd,EAAiB,CAAC,CAAlB,CAAnB;AAEA,MAAM,SAAS,GAAG,IAAI,CAAC,aAAL,CAAmB,UAAnB,CAAlB;AACA,MAAM,SAAS,GAAG,IAAI,CAAC,aAAL,CAAmB,UAAnB,CAAlB;AAEA,MAAM,mBAAmB,GACrB,SAAS,KAAK,SAAd,IAA2B,SAAS,KAAK,CAAzC,IAA8C,SAAS,KAAK,CADhE;AAGA,EAAA,IAAI,CAAC,MAAL,CACI,KAAK,IAAI,CAAT,IAAc,KAAK,IAAI,CAAvB,IAA4B,mBADhC,EAEI;AAAA,WAAM,wKAEsB,UAFtB,oBAE0C,UAF1C,OAAN;AAAA,GAFJ;AAMA,MAAM,iBAAiB,GACnB,SAAS,GAAG,SAAZ,GAAwB,CAAC,CAAC,KAAF,CAAQ,KAAR,CAAc,CAAd,EAAiB,CAAC,CAAlB,CAAxB,GAA+C,CAAC,CAAC,KAAF,CAAQ,KAAR,CAAc,CAAd,EAAiB,CAAC,CAAlB,CADnD;AAEA,MAAM,QAAQ,GAAG,iBAAiB,CAAC,MAAlB,CAAyB,CAAC,WAAD,EAAc,WAAd,CAAzB,CAAjB;AAEA,EAAA,IAAI,CAAC,MAAL,CACI,WAAW,KAAK,WADpB,EAEI;AAAA,WAAM,yCAAkC,WAAlC,yBACC,WADD,sCACwC,CAAC,CAAC,KAD1C,uBAEC,CAAC,CAAC,KAFH,6BAE2B,UAF3B,8BAGiB,UAHjB,iBAAN;AAAA,GAFJ;AAOA,MAAM,QAAQ,GAA6B,UAAU,GACjD,CAAC,SAAD,EAAY,WAAZ,EAAyB,WAAzB,CADiD,GAEjD,CAAC,SAAD,EAAY,WAAZ,EAAyB,WAAzB,CAFJ;AAGA,MAAM,QAAQ,GAA6B,UAAU,GACjD,CAAC,SAAD,EAAY,WAAZ,EAAyB,WAAzB,CADiD,GAEjD,CAAC,SAAD,EAAY,WAAZ,EAAyB,WAAzB,CAFJ,CAvCkB,CA2ClB;;AACA,MAAM,GAAG,GAAG,OAAO,CAAC;AAAC,IAAA,MAAM,EAAE;AAAC,MAAA,CAAC,EAAE;AAAJ,KAAT;AAAiB,IAAA,OAAO,EAAP,OAAjB;AAA0B,IAAA,KAAK,EAAE;AAAC,MAAA,KAAK,EAAE;AAAR;AAAjC,GAAD,CAAnB;AACA,MAAM,GAAG,GAAG,OAAO,CAAC;AAAC,IAAA,MAAM,EAAE;AAAC,MAAA,CAAC,EAAE;AAAJ,KAAT;AAAiB,IAAA,OAAO,EAAP,OAAjB;AAA0B,IAAA,KAAK,EAAE;AAAC,MAAA,KAAK,EAAE;AAAR;AAAjC,GAAD,CAAnB;AAEA,MAAM,aAAa,GAAiB,CAAC,GAAD,EAAM,GAAN,CAApC;AAEA,MAAM,QAAQ,GAAG,IAAI,CAAC,GAAL,CAAS,SAAT,EAAoB,SAApB,CAAjB;AACA,MAAM,SAAS,GAAG,UAAU,GAAG,GAAG,CAAC,KAAJ,CAAU,CAAV,CAAH,GAAkB,GAAG,CAAC,KAAJ,CAAU,CAAV,CAA9C;AAEA,MAAM,OAAO,GAAG,IAAI,IAAI,IAAxB;AACA,MAAM,yBAAyB,GAAG,sBAAsB,IAAI,IAA5D;AACA,MAAM,iBAAiB,GAAG,UAAU,KAAK,WAAzC;AACA,MAAM,eAAe,GAAG,UAAU,IAAI,IAAd,GACpB,4BAA4B,CAAC,UAAD,EAAa,IAAb,CADR,GAEpB,IAFJ;AAGA,MAAM,gBAAgB,GAAG,OAAO,IAAI,yBAAX,IACrB,iBADqB,IACA,eAAe,IAAI,IAD5C;AAEA,MAAI,GAAJ,CA5DkB,CA8DlB;AACA;;AACA,MAAI,CAAC,WAAW,KAAK,CAAhB,IAAqB,WAAW,KAAK,CAAtC,KACA,SAAS,GAAG,2BADZ,IAC2C,gBAAgB,KAAK,KADpE,EAC2E;AACzE,QAAI,IAAI,GAAG,GAAX;AACA,QAAI,IAAI,GAAG,GAAX;;AACA,QAAI,UAAJ,EAAgB;AACd,MAAA,IAAI,GAAG,SAAS,CAAC;AAAC,QAAA,MAAM,EAAE;AAAC,UAAA,CAAC,EAAE;AAAJ,SAAT;AAAmB,QAAA,OAAO,EAAP,OAAnB;AAA4B,QAAA,KAAK,EAAE;AAAC,UAAA,IAAI,EAAE,CAAC,CAAD,EAAI,CAAJ,EAAO,CAAP;AAAP;AAAnC,OAAD,CAAhB;AACA,MAAA,aAAa,CAAC,IAAd,CAAmB,IAAnB;AACD;;AACD,QAAI,UAAJ,EAAgB;AACd,MAAA,IAAI,GAAG,SAAS,CAAC;AAAC,QAAA,MAAM,EAAE;AAAC,UAAA,CAAC,EAAE;AAAJ,SAAT;AAAmB,QAAA,OAAO,EAAP,OAAnB;AAA4B,QAAA,KAAK,EAAE;AAAC,UAAA,IAAI,EAAE,CAAC,CAAD,EAAI,CAAJ,EAAO,CAAP;AAAP;AAAnC,OAAD,CAAhB;AACA,MAAA,aAAa,CAAC,IAAd,CAAmB,IAAnB;AACD;;AAED,QAAM,cAAc,GAAG,WAAW,KAAK,CAAvC;AACA,QAAM,cAAc,GAAG,WAAW,KAAK,CAAvC;AAEA,QAAI,MAAM,GAAG,IAAb;;AACA,QAAI,cAAJ,EAAoB;AAClB,MAAA,MAAM,GAAG,OAAO,CAAC;AACf,QAAA,MAAM,EAAE;AAAC,UAAA,CAAC,EAAE;AAAJ,SADO;AAEf,QAAA,OAAO,EAAP,OAFe;AAGf,QAAA,KAAK,EAAE;AAAC,UAAA,KAAK,EAAE,CAAC,QAAD,EAAW,SAAX,EAAsB,CAAtB;AAAR;AAHQ,OAAD,CAAhB;AAMA,MAAA,aAAa,CAAC,IAAd,CAAmB,MAAnB;AACD;;AAED,QAAM,IAAI,GAAG,WAAW,KAAK,CAAhB,GAAoB,CAApB,GAAwB,CAArC;AAEA,QAAI,MAAM,GAAG,IAAb;;AACA,QAAI,cAAJ,EAAoB;AAClB,MAAA,MAAM,GAAG,OAAO,CAAC;AACf,QAAA,MAAM,EAAE;AAAC,UAAA,CAAC,EAAE;AAAJ,SADO;AAEf,QAAA,OAAO,EAAP,OAFe;AAGf,QAAA,KAAK,EAAE;AAAC,UAAA,KAAK,EAAE,CAAC,QAAD,EAAW,CAAX,EAAc,SAAd;AAAR;AAHQ,OAAD,CAAhB;AAMA,MAAA,aAAa,CAAC,IAAd,CAAmB,MAAnB;AACD;;AAED,QAAM,OAAO,GAAG,QAAQ,CAAC;AAAC,MAAA,MAAM,EAAE;AAAC,QAAA,CAAC,EAAE,MAAJ;AAAY,QAAA,CAAC,EAAE;AAAf,OAAT;AAAiC,MAAA,OAAO,EAAP;AAAjC,KAAD,CAAxB;AACA,IAAA,GAAG,GAAG,GAAG,CAAC;AAAC,MAAA,MAAM,EAAE;AAAC,QAAA,CAAC,EAAE;AAAJ,OAAT;AAAuB,MAAA,OAAO,EAAP,OAAvB;AAAgC,MAAA,KAAK,EAAE;AAAC,QAAA,IAAI,EAAJ,IAAD;AAAO,QAAA,QAAQ,EAAE;AAAjB;AAAvC,KAAD,CAAT;AACA,IAAA,aAAa,CAAC,IAAd,CAAmB,OAAnB;AACD,GA3CD,MA2CO;AACL,QAAM,KAAK,GAAG,UAAU,CAAC,CAAC,CAAC,KAAH,EAAU,CAAC,CAAC,KAAZ,CAAxB;AAEA,QAAM,OAAO,GAAG,IAAI,mBAAJ,CACZ,QADY,EACF,QADE,EACQ,CAAC,QAAD,EAAW,WAAX,EAAwB,WAAxB,CADR,EAC8C,UAD9C,EAEZ,UAFY,EAEA,OAFA,EAES,eAFT,EAE0B,yBAF1B,EAGZ,iBAHY,CAAhB;AAKA,QAAM,MAAM,GAAiB,CAAC,GAAD,EAAM,GAAN,CAA7B;;AACA,QAAI,IAAI,IAAI,IAAZ,EAAkB;AAChB,MAAA,MAAM,CAAC,IAAP,CAAY,IAAZ;AACD;;AACD,QAAI,yBAAJ,EAA+B;AAC7B,MAAA,MAAM,CAAC,IAAP,CAAY,sBAAZ;AACD;;AACD,QAAI,iBAAJ,EAAuB;AACrB,UAAM,eAAe,GAAG,OAAO,CAAC,cAAR,CACpB,EADoB,EAChB,SADgB,EAEpB,IAAI,CAAC,iBAAL,CAAuB,cAAvB,EAA0D,SAA1D,CAFoB,CAAxB;AAGA,MAAA,MAAM,CAAC,IAAP,CAAY,eAAZ;AACA,MAAA,aAAa,CAAC,IAAd,CAAmB,eAAnB;AACD;;AAED,IAAA,GAAG,GAAG,OAAO,CAAC,eAAR,CAAwB,OAAxB,EAAiC,MAAjC,EAAyC,KAAzC,CAAN;AACD;;AAED,MAAM,WAAW,GACb,OAAO,CAAC;AAAC,IAAA,MAAM,EAAE;AAAC,MAAA,CAAC,EAAE;AAAJ,KAAT;AAAmB,IAAA,OAAO,EAAP,OAAnB;AAA4B,IAAA,KAAK,EAAE;AAAC,MAAA,KAAK,EAAE;AAAR;AAAnC,GAAD,CADX;AAEA,EAAA,aAAa,CAAC,IAAd,CAAmB,GAAnB;;AACA,oCAAgB,aAAhB,oCAA+B;AAA1B,QAAM,CAAC,qBAAP;AACH,IAAA,OAAO,CAAC,6BAAR,CAAsC,CAAtC;AACD;;AACD,SAAO,WAAP;AACD","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { upcastType, util } from '@tensorflow/tfjs-core';\nimport { mapActivationToShaderProgram } from '../kernel_utils/kernel_funcs_utils';\nimport { MatMulPackedProgram } from '../mulmat_packed_gpu';\nimport { multiply } from './Multiply';\nimport { reshape } from './Reshape';\nimport { sum } from './Sum';\nimport { transpose } from './Transpose';\n// Empirically determined minimal shared dimension in matmul before we forward\n// to a.mul(b).sum() in order to take advantage of GPU parallelism. See\n// https://github.com/tensorflow/tfjs-core/pull/1379 for benchmarks.\nexport const MATMUL_SHARED_DIM_THRESHOLD = 1000;\nexport function batchMatMulImpl({ a, b, transposeA, transposeB, backend, bias = null, preluActivationWeights = null, leakyreluAlpha = 0, activation = null }) {\n    const aRank = a.shape.length;\n    const bRank = b.shape.length;\n    const innerShapeA = transposeA ? a.shape[aRank - 2] : a.shape[aRank - 1];\n    const innerShapeB = transposeB ? b.shape[bRank - 1] : b.shape[bRank - 2];\n    const outerShapeA = transposeA ? a.shape[aRank - 1] : a.shape[aRank - 2];\n    const outerShapeB = transposeB ? b.shape[bRank - 2] : b.shape[bRank - 1];\n    const outerDimsA = a.shape.slice(0, -2);\n    const outerDimsB = b.shape.slice(0, -2);\n    const batchDimA = util.sizeFromShape(outerDimsA);\n    const batchDimB = util.sizeFromShape(outerDimsB);\n    const batchDimsCompatible = batchDimA === batchDimB || batchDimA === 1 || batchDimB === 1;\n    util.assert(aRank >= 2 && bRank >= 2 && batchDimsCompatible, () => `Error in matMul: the input batch dimensions must either be the ` +\n        `same or at least one input batch dimension must be 1. Got input ` +\n        `batch dimensions of (${outerDimsA}) and (${outerDimsB}).`);\n    const outShapeOuterDims = batchDimA > batchDimB ? a.shape.slice(0, -2) : b.shape.slice(0, -2);\n    const outShape = outShapeOuterDims.concat([outerShapeA, outerShapeB]);\n    util.assert(innerShapeA === innerShapeB, () => `Error in matMul: inner shapes (${innerShapeA}) and (` +\n        `${innerShapeB}) of Tensors with shapes ${a.shape} and ` +\n        `${b.shape} and transposeA=${transposeA}` +\n        ` and transposeB=${transposeB} must match.`);\n    const a3dShape = transposeA ?\n        [batchDimA, innerShapeA, outerShapeA] :\n        [batchDimA, outerShapeA, innerShapeA];\n    const b3dShape = transposeB ?\n        [batchDimB, outerShapeB, innerShapeB] :\n        [batchDimB, innerShapeB, outerShapeB];\n    // The rest of the implementation is designed to operate on rank-3 tensors\n    const a3d = reshape({ inputs: { x: a }, backend, attrs: { shape: a3dShape } });\n    const b3d = reshape({ inputs: { x: b }, backend, attrs: { shape: b3dShape } });\n    const intermediates = [a3d, b3d];\n    const batchDim = Math.max(batchDimA, batchDimB);\n    const sharedDim = transposeA ? a3d.shape[1] : a3d.shape[2];\n    const hasBias = bias != null;\n    const hasPreluActivationWeights = preluActivationWeights != null;\n    const hasLeakyreluAlpha = activation === 'leakyrelu';\n    const fusedActivation = activation != null ?\n        mapActivationToShaderProgram(activation, true) :\n        null;\n    const containsFusedOps = hasBias || hasPreluActivationWeights ||\n        hasLeakyreluAlpha || fusedActivation != null;\n    let out;\n    // Since the matrices are vectors, it is faster to call mul().sum()\n    // because sum() is O(sqrt(N)) due to divide-and-conquer.\n    if ((outerShapeA === 1 || outerShapeB === 1) &&\n        sharedDim > MATMUL_SHARED_DIM_THRESHOLD && containsFusedOps === false) {\n        let aVec = a3d;\n        let bVec = b3d;\n        if (transposeA) {\n            aVec = transpose({ inputs: { x: a3d }, backend, attrs: { perm: [0, 2, 1] } });\n            intermediates.push(aVec);\n        }\n        if (transposeB) {\n            bVec = transpose({ inputs: { x: b3d }, backend, attrs: { perm: [0, 2, 1] } });\n            intermediates.push(bVec);\n        }\n        const shouldReshapeA = outerShapeB !== 1;\n        const shouldReshapeB = outerShapeB === 1;\n        let aVec3d = aVec;\n        if (shouldReshapeA) {\n            aVec3d = reshape({\n                inputs: { x: aVec },\n                backend,\n                attrs: { shape: [batchDim, sharedDim, 1] }\n            });\n            intermediates.push(aVec3d);\n        }\n        const axis = outerShapeB === 1 ? 2 : 1;\n        let bVec3d = bVec;\n        if (shouldReshapeB) {\n            bVec3d = reshape({\n                inputs: { x: bVec },\n                backend,\n                attrs: { shape: [batchDim, 1, sharedDim] }\n            });\n            intermediates.push(bVec3d);\n        }\n        const product = multiply({ inputs: { a: aVec3d, b: bVec3d }, backend });\n        out = sum({ inputs: { x: product }, backend, attrs: { axis, keepDims: true } });\n        intermediates.push(product);\n    }\n    else {\n        const dtype = upcastType(a.dtype, b.dtype);\n        const program = new MatMulPackedProgram(a3dShape, b3dShape, [batchDim, outerShapeA, outerShapeB], transposeA, transposeB, hasBias, fusedActivation, hasPreluActivationWeights, hasLeakyreluAlpha);\n        const inputs = [a3d, b3d];\n        if (bias != null) {\n            inputs.push(bias);\n        }\n        if (hasPreluActivationWeights) {\n            inputs.push(preluActivationWeights);\n        }\n        if (hasLeakyreluAlpha) {\n            const $leakyreluAlpha = backend.makeTensorInfo([], 'float32', util.createScalarValue(leakyreluAlpha, 'float32'));\n            inputs.push($leakyreluAlpha);\n            intermediates.push($leakyreluAlpha);\n        }\n        out = backend.runWebGLProgram(program, inputs, dtype);\n    }\n    const outReshaped = reshape({ inputs: { x: out }, backend, attrs: { shape: outShape } });\n    intermediates.push(out);\n    for (const i of intermediates) {\n        backend.disposeIntermediateTensorInfo(i);\n    }\n    return outReshaped;\n}\n//# sourceMappingURL=BatchMatMul_impl.js.map"]},"metadata":{},"sourceType":"module"}