{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n// Layer activation functions\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { serialization, tidy } from '@tensorflow/tfjs-core';\nimport * as K from './backend/tfjs_backend';\nimport { deserializeKerasObject } from './utils/generic_utils';\n/**\n * Base class for Activations.\n *\n * Special note: due to cross-language compatibility reasons, the\n * static readonly className field in this family of classes must be set to\n * the initialLowerCamelCase name of the activation.\n */\n\nexport class Activation extends serialization.Serializable {\n  getConfig() {\n    return {};\n  }\n\n}\n/**\n * Exponential linear unit (ELU).\n * Reference: https://arxiv.org/abs/1511.07289\n */\n\nexport class Elu extends Activation {\n  /**\n   * Calculate the activation function.\n   *\n   * @param x: Input.\n   * @param alpha: Scaling factor the negative section.\n   * @return Output of the ELU activation.\n   */\n  apply(x, alpha = 1) {\n    return K.elu(x, alpha);\n  }\n\n}\n/** @nocollapse */\n\nElu.className = 'elu';\nserialization.registerClass(Elu);\n/**\n * Scaled Exponential Linear Unit. (Klambauer et al., 2017).\n * Reference: Self-Normalizing Neural Networks, https://arxiv.org/abs/1706.02515\n * Notes:\n *   - To be used together with the initialization \"lecunNormal\".\n *   - To be used together with the dropout variant \"AlphaDropout\".\n */\n\nexport class Selu extends Activation {\n  apply(x) {\n    return tfc.selu(x);\n  }\n\n}\n/** @nocollapse */\n\nSelu.className = 'selu';\nserialization.registerClass(Selu);\n/**\n *  Rectified linear unit\n */\n\nexport class Relu extends Activation {\n  apply(x) {\n    return tfc.relu(x);\n  }\n\n}\n/** @nocollapse */\n\nRelu.className = 'relu';\nserialization.registerClass(Relu);\n/**\n * Rectified linear unit activation maxing out at 6.0.\n */\n\nexport class Relu6 extends Activation {\n  apply(x) {\n    return tidy(() => tfc.minimum(6.0, tfc.relu(x)));\n  }\n\n}\n/** @nocollapse */\n\nRelu6.className = 'relu6';\nserialization.registerClass(Relu6); //* Linear activation (no-op) */\n\nexport class Linear extends Activation {\n  apply(x) {\n    return x;\n  }\n\n}\n/** @nocollapse */\n\nLinear.className = 'linear';\nserialization.registerClass(Linear);\n/**\n * Sigmoid activation function.\n */\n\nexport class Sigmoid extends Activation {\n  apply(x) {\n    return tfc.sigmoid(x);\n  }\n\n}\n/** @nocollapse */\n\nSigmoid.className = 'sigmoid';\nserialization.registerClass(Sigmoid);\n/**\n * Segment-wise linear approximation of sigmoid.\n */\n\nexport class HardSigmoid extends Activation {\n  apply(x) {\n    return K.hardSigmoid(x);\n  }\n\n}\n/** @nocollapse */\n\nHardSigmoid.className = 'hardSigmoid';\nserialization.registerClass(HardSigmoid);\n/**\n * Softplus activation function.\n */\n\nexport class Softplus extends Activation {\n  apply(x) {\n    return tfc.softplus(x);\n  }\n\n}\n/** @nocollapse */\n\nSoftplus.className = 'softplus';\nserialization.registerClass(Softplus);\n/**\n * Softsign activation function.\n */\n\nexport class Softsign extends Activation {\n  apply(x) {\n    return K.softsign(x);\n  }\n\n}\n/** @nocollapse */\n\nSoftsign.className = 'softsign';\nserialization.registerClass(Softsign);\n/**\n * Hyperbolic tangent function.\n */\n\nexport class Tanh extends Activation {\n  apply(x) {\n    return tfc.tanh(x);\n  }\n\n}\n/** @nocollapse */\n\nTanh.className = 'tanh';\nserialization.registerClass(Tanh);\n/**\n * Softmax activation function\n */\n\nexport class Softmax extends Activation {\n  /**\n   * Calculate the activation function.\n   *\n   * @param x Tensor.\n   * @param axis Integer, axis along which the softmax normalization is applied.\n   * Invalid if < 2, as softmax across 1 (the batch dimension) is assumed to be\n   * an error.\n   *\n   * @returns a Tensor of the same shape as x\n   *\n   * @throws ValueError: In case `dim(x) < 2`.\n   */\n  apply(x, axis = -1) {\n    return tfc.softmax(x, axis);\n  }\n\n}\n/** @nocollapse */\n\nSoftmax.className = 'softmax';\nserialization.registerClass(Softmax);\n/**\n * Log softmax activation function\n */\n\nexport class LogSoftmax extends Activation {\n  /**\n   * Calculate the activation function of log softmax:\n   * log( exp(x_i) / sum(exp(x)) )\n   *\n   * @param x Tensor.\n   * @param axis Integer, axis along which the softmax normalization is applied.\n   * Invalid if < 2, as softmax across 1 (the batch dimension) is assumed to be\n   * an error.\n   *\n   * @returns a Tensor of the same shape as x\n   *\n   * @throws ValueError: In case `dim(x) < 2`.\n   */\n  apply(x, axis = -1) {\n    return tfc.logSoftmax(x, axis);\n  }\n\n}\n/** @nocollapse */\n\nLogSoftmax.className = 'logSoftmax';\nserialization.registerClass(LogSoftmax);\n/**\n * Swish activation function\n */\n\nexport class Swish extends Activation {\n  /**\n   * Calculate the activation function.\n   *\n   * @param x Tensor.\n   * @param alpha Scaling factor for the sigmoid function.\n   * @returns a Tensor of the same shape as x\n   */\n  apply(x, alpha = 1) {\n    return tidy(() => tfc.sigmoid(x.mul(alpha)).mul(x));\n  }\n\n}\n/** @nocollapse */\n\nSwish.className = 'swish';\nserialization.registerClass(Swish);\nexport function serializeActivation(activation) {\n  return activation.getClassName();\n}\nexport function deserializeActivation(config, customObjects = {}) {\n  return deserializeKerasObject(config, serialization.SerializationMap.getMap().classNameMap, customObjects, 'activation');\n}\nexport function getActivation(identifier) {\n  if (identifier == null) {\n    const config = {};\n    config['className'] = 'linear';\n    config['config'] = {};\n    return deserializeActivation(config);\n  }\n\n  if (typeof identifier === 'string') {\n    const config = {};\n    config['className'] = identifier;\n    config['config'] = {};\n    return deserializeActivation(config);\n  } else if (identifier instanceof Activation) {\n    return identifier;\n  } else {\n    return deserializeActivation(identifier);\n  }\n}","map":{"version":3,"sources":["../src/activations.ts"],"names":[],"mappings":"AAAA;;;;;;;;AAQG;AAEH;AACA,OAAO,KAAK,GAAZ,MAAqB,uBAArB;AACA,SAAQ,aAAR,EAA+B,IAA/B,QAA0C,uBAA1C;AACA,OAAO,KAAK,CAAZ,MAAmB,wBAAnB;AAEA,SAAQ,sBAAR,QAAqC,uBAArC;AAEA;;;;;;AAMG;;AACH,OAAM,MAAgB,UAAhB,SAAmC,aAAa,CAAC,YAAjD,CAA6D;AAEjE,EAAA,SAAS,GAAA;AACP,WAAO,EAAP;AACD;;AAJgE;AAOnE;;;AAGG;;AACH,OAAM,MAAO,GAAP,SAAmB,UAAnB,CAA6B;AAGjC;;;;;;AAMG;AACH,EAAA,KAAK,CAAC,CAAD,EAAY,KAAK,GAAG,CAApB,EAAqB;AACxB,WAAO,CAAC,CAAC,GAAF,CAAM,CAAN,EAAS,KAAT,CAAP;AACD;;AAZgC;AACjC;;AACgB,GAAA,CAAA,SAAA,GAAY,KAAZ;AAYlB,aAAa,CAAC,aAAd,CAA4B,GAA5B;AAEA;;;;;;AAMG;;AACH,OAAM,MAAO,IAAP,SAAoB,UAApB,CAA8B;AAGlC,EAAA,KAAK,CAAC,CAAD,EAAU;AACb,WAAO,GAAG,CAAC,IAAJ,CAAS,CAAT,CAAP;AACD;;AALiC;AAClC;;AACgB,IAAA,CAAA,SAAA,GAAY,MAAZ;AAKlB,aAAa,CAAC,aAAd,CAA4B,IAA5B;AAEA;;AAEG;;AACH,OAAM,MAAO,IAAP,SAAoB,UAApB,CAA8B;AAGlC,EAAA,KAAK,CAAC,CAAD,EAAU;AACb,WAAO,GAAG,CAAC,IAAJ,CAAS,CAAT,CAAP;AACD;;AALiC;AAClC;;AACgB,IAAA,CAAA,SAAA,GAAY,MAAZ;AAKlB,aAAa,CAAC,aAAd,CAA4B,IAA5B;AAEA;;AAEG;;AACH,OAAM,MAAO,KAAP,SAAqB,UAArB,CAA+B;AAGnC,EAAA,KAAK,CAAC,CAAD,EAAU;AACb,WAAO,IAAI,CAAC,MAAM,GAAG,CAAC,OAAJ,CAAY,GAAZ,EAAiB,GAAG,CAAC,IAAJ,CAAS,CAAT,CAAjB,CAAP,CAAX;AACD;;AALkC;AACnC;;AACgB,KAAA,CAAA,SAAA,GAAY,OAAZ;AAKlB,aAAa,CAAC,aAAd,CAA4B,KAA5B,E,CAEA;;AACA,OAAM,MAAO,MAAP,SAAsB,UAAtB,CAAgC;AAGpC,EAAA,KAAK,CAAC,CAAD,EAAU;AACb,WAAO,CAAP;AACD;;AALmC;AACpC;;AACgB,MAAA,CAAA,SAAA,GAAY,QAAZ;AAKlB,aAAa,CAAC,aAAd,CAA4B,MAA5B;AAEA;;AAEG;;AACH,OAAM,MAAO,OAAP,SAAuB,UAAvB,CAAiC;AAGrC,EAAA,KAAK,CAAC,CAAD,EAAU;AACb,WAAO,GAAG,CAAC,OAAJ,CAAY,CAAZ,CAAP;AACD;;AALoC;AACrC;;AACgB,OAAA,CAAA,SAAA,GAAY,SAAZ;AAKlB,aAAa,CAAC,aAAd,CAA4B,OAA5B;AAEA;;AAEG;;AACH,OAAM,MAAO,WAAP,SAA2B,UAA3B,CAAqC;AAGzC,EAAA,KAAK,CAAC,CAAD,EAAU;AACb,WAAO,CAAC,CAAC,WAAF,CAAc,CAAd,CAAP;AACD;;AALwC;AACzC;;AACgB,WAAA,CAAA,SAAA,GAAY,aAAZ;AAKlB,aAAa,CAAC,aAAd,CAA4B,WAA5B;AAEA;;AAEG;;AACH,OAAM,MAAO,QAAP,SAAwB,UAAxB,CAAkC;AAGtC,EAAA,KAAK,CAAC,CAAD,EAAU;AACb,WAAO,GAAG,CAAC,QAAJ,CAAa,CAAb,CAAP;AACD;;AALqC;AACtC;;AACgB,QAAA,CAAA,SAAA,GAAY,UAAZ;AAKlB,aAAa,CAAC,aAAd,CAA4B,QAA5B;AAEA;;AAEG;;AACH,OAAM,MAAO,QAAP,SAAwB,UAAxB,CAAkC;AAGtC,EAAA,KAAK,CAAC,CAAD,EAAU;AACb,WAAO,CAAC,CAAC,QAAF,CAAW,CAAX,CAAP;AACD;;AALqC;AACtC;;AACgB,QAAA,CAAA,SAAA,GAAY,UAAZ;AAKlB,aAAa,CAAC,aAAd,CAA4B,QAA5B;AAEA;;AAEG;;AACH,OAAM,MAAO,IAAP,SAAoB,UAApB,CAA8B;AAGlC,EAAA,KAAK,CAAC,CAAD,EAAU;AACb,WAAO,GAAG,CAAC,IAAJ,CAAS,CAAT,CAAP;AACD;;AALiC;AAClC;;AACgB,IAAA,CAAA,SAAA,GAAY,MAAZ;AAKlB,aAAa,CAAC,aAAd,CAA4B,IAA5B;AAEA;;AAEG;;AACH,OAAM,MAAO,OAAP,SAAuB,UAAvB,CAAiC;AAGrC;;;;;;;;;;;AAWG;AACH,EAAA,KAAK,CAAC,CAAD,EAAY,IAAA,GAAgB,CAAC,CAA7B,EAA+B;AAClC,WAAO,GAAG,CAAC,OAAJ,CAAY,CAAZ,EAAe,IAAf,CAAP;AACD;;AAjBoC;AACrC;;AACgB,OAAA,CAAA,SAAA,GAAY,SAAZ;AAiBlB,aAAa,CAAC,aAAd,CAA4B,OAA5B;AAEA;;AAEG;;AACH,OAAM,MAAO,UAAP,SAA0B,UAA1B,CAAoC;AAGxC;;;;;;;;;;;;AAYG;AACH,EAAA,KAAK,CAAC,CAAD,EAAY,IAAA,GAAgB,CAAC,CAA7B,EAA+B;AAClC,WAAO,GAAG,CAAC,UAAJ,CAAe,CAAf,EAAkB,IAAlB,CAAP;AACD;;AAlBuC;AACxC;;AACgB,UAAA,CAAA,SAAA,GAAY,YAAZ;AAkBlB,aAAa,CAAC,aAAd,CAA4B,UAA5B;AAEA;;AAEG;;AACH,OAAM,MAAO,KAAP,SAAqB,UAArB,CAA+B;AAGnC;;;;;;AAMG;AACH,EAAA,KAAK,CAAC,CAAD,EAAY,KAAK,GAAG,CAApB,EAAqB;AACxB,WAAO,IAAI,CAAC,MAAM,GAAG,CAAC,OAAJ,CAAY,CAAC,CAAC,GAAF,CAAM,KAAN,CAAZ,EAA0B,GAA1B,CAA8B,CAA9B,CAAP,CAAX;AACD;;AAZkC;AACnC;;AACgB,KAAA,CAAA,SAAA,GAAY,OAAZ;AAYlB,aAAa,CAAC,aAAd,CAA4B,KAA5B;AAEA,OAAM,SAAU,mBAAV,CAA8B,UAA9B,EAAoD;AACxD,SAAO,UAAU,CAAC,YAAX,EAAP;AACD;AAED,OAAM,SAAU,qBAAV,CACH,MADG,EAEH,aAAA,GAA0C,EAFvC,EAEyC;AAC7C,SAAO,sBAAsB,CACzB,MADyB,EACjB,aAAa,CAAC,gBAAd,CAA+B,MAA/B,GAAwC,YADvB,EAEzB,aAFyB,EAEV,YAFU,CAA7B;AAGD;AAED,OAAM,SAAU,aAAV,CAAwB,UAAxB,EAC2D;AAC/D,MAAI,UAAU,IAAI,IAAlB,EAAwB;AACtB,UAAM,MAAM,GAA6B,EAAzC;AACA,IAAA,MAAM,CAAC,WAAD,CAAN,GAAsB,QAAtB;AACA,IAAA,MAAM,CAAC,QAAD,CAAN,GAAmB,EAAnB;AACA,WAAO,qBAAqB,CAAC,MAAD,CAA5B;AACD;;AACD,MAAI,OAAO,UAAP,KAAsB,QAA1B,EAAoC;AAClC,UAAM,MAAM,GAA6B,EAAzC;AACA,IAAA,MAAM,CAAC,WAAD,CAAN,GAAsB,UAAtB;AACA,IAAA,MAAM,CAAC,QAAD,CAAN,GAAmB,EAAnB;AACA,WAAO,qBAAqB,CAAC,MAAD,CAA5B;AACD,GALD,MAKO,IAAI,UAAU,YAAY,UAA1B,EAAsC;AAC3C,WAAO,UAAP;AACD,GAFM,MAEA;AACL,WAAO,qBAAqB,CAAC,UAAD,CAA5B;AACD;AACF","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n// Layer activation functions\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { serialization, tidy } from '@tensorflow/tfjs-core';\nimport * as K from './backend/tfjs_backend';\nimport { deserializeKerasObject } from './utils/generic_utils';\n/**\n * Base class for Activations.\n *\n * Special note: due to cross-language compatibility reasons, the\n * static readonly className field in this family of classes must be set to\n * the initialLowerCamelCase name of the activation.\n */\nexport class Activation extends serialization.Serializable {\n    getConfig() {\n        return {};\n    }\n}\n/**\n * Exponential linear unit (ELU).\n * Reference: https://arxiv.org/abs/1511.07289\n */\nexport class Elu extends Activation {\n    /**\n     * Calculate the activation function.\n     *\n     * @param x: Input.\n     * @param alpha: Scaling factor the negative section.\n     * @return Output of the ELU activation.\n     */\n    apply(x, alpha = 1) {\n        return K.elu(x, alpha);\n    }\n}\n/** @nocollapse */\nElu.className = 'elu';\nserialization.registerClass(Elu);\n/**\n * Scaled Exponential Linear Unit. (Klambauer et al., 2017).\n * Reference: Self-Normalizing Neural Networks, https://arxiv.org/abs/1706.02515\n * Notes:\n *   - To be used together with the initialization \"lecunNormal\".\n *   - To be used together with the dropout variant \"AlphaDropout\".\n */\nexport class Selu extends Activation {\n    apply(x) {\n        return tfc.selu(x);\n    }\n}\n/** @nocollapse */\nSelu.className = 'selu';\nserialization.registerClass(Selu);\n/**\n *  Rectified linear unit\n */\nexport class Relu extends Activation {\n    apply(x) {\n        return tfc.relu(x);\n    }\n}\n/** @nocollapse */\nRelu.className = 'relu';\nserialization.registerClass(Relu);\n/**\n * Rectified linear unit activation maxing out at 6.0.\n */\nexport class Relu6 extends Activation {\n    apply(x) {\n        return tidy(() => tfc.minimum(6.0, tfc.relu(x)));\n    }\n}\n/** @nocollapse */\nRelu6.className = 'relu6';\nserialization.registerClass(Relu6);\n//* Linear activation (no-op) */\nexport class Linear extends Activation {\n    apply(x) {\n        return x;\n    }\n}\n/** @nocollapse */\nLinear.className = 'linear';\nserialization.registerClass(Linear);\n/**\n * Sigmoid activation function.\n */\nexport class Sigmoid extends Activation {\n    apply(x) {\n        return tfc.sigmoid(x);\n    }\n}\n/** @nocollapse */\nSigmoid.className = 'sigmoid';\nserialization.registerClass(Sigmoid);\n/**\n * Segment-wise linear approximation of sigmoid.\n */\nexport class HardSigmoid extends Activation {\n    apply(x) {\n        return K.hardSigmoid(x);\n    }\n}\n/** @nocollapse */\nHardSigmoid.className = 'hardSigmoid';\nserialization.registerClass(HardSigmoid);\n/**\n * Softplus activation function.\n */\nexport class Softplus extends Activation {\n    apply(x) {\n        return tfc.softplus(x);\n    }\n}\n/** @nocollapse */\nSoftplus.className = 'softplus';\nserialization.registerClass(Softplus);\n/**\n * Softsign activation function.\n */\nexport class Softsign extends Activation {\n    apply(x) {\n        return K.softsign(x);\n    }\n}\n/** @nocollapse */\nSoftsign.className = 'softsign';\nserialization.registerClass(Softsign);\n/**\n * Hyperbolic tangent function.\n */\nexport class Tanh extends Activation {\n    apply(x) {\n        return tfc.tanh(x);\n    }\n}\n/** @nocollapse */\nTanh.className = 'tanh';\nserialization.registerClass(Tanh);\n/**\n * Softmax activation function\n */\nexport class Softmax extends Activation {\n    /**\n     * Calculate the activation function.\n     *\n     * @param x Tensor.\n     * @param axis Integer, axis along which the softmax normalization is applied.\n     * Invalid if < 2, as softmax across 1 (the batch dimension) is assumed to be\n     * an error.\n     *\n     * @returns a Tensor of the same shape as x\n     *\n     * @throws ValueError: In case `dim(x) < 2`.\n     */\n    apply(x, axis = (-1)) {\n        return tfc.softmax(x, axis);\n    }\n}\n/** @nocollapse */\nSoftmax.className = 'softmax';\nserialization.registerClass(Softmax);\n/**\n * Log softmax activation function\n */\nexport class LogSoftmax extends Activation {\n    /**\n     * Calculate the activation function of log softmax:\n     * log( exp(x_i) / sum(exp(x)) )\n     *\n     * @param x Tensor.\n     * @param axis Integer, axis along which the softmax normalization is applied.\n     * Invalid if < 2, as softmax across 1 (the batch dimension) is assumed to be\n     * an error.\n     *\n     * @returns a Tensor of the same shape as x\n     *\n     * @throws ValueError: In case `dim(x) < 2`.\n     */\n    apply(x, axis = (-1)) {\n        return tfc.logSoftmax(x, axis);\n    }\n}\n/** @nocollapse */\nLogSoftmax.className = 'logSoftmax';\nserialization.registerClass(LogSoftmax);\n/**\n * Swish activation function\n */\nexport class Swish extends Activation {\n    /**\n     * Calculate the activation function.\n     *\n     * @param x Tensor.\n     * @param alpha Scaling factor for the sigmoid function.\n     * @returns a Tensor of the same shape as x\n     */\n    apply(x, alpha = 1) {\n        return tidy(() => tfc.sigmoid(x.mul(alpha)).mul(x));\n    }\n}\n/** @nocollapse */\nSwish.className = 'swish';\nserialization.registerClass(Swish);\nexport function serializeActivation(activation) {\n    return activation.getClassName();\n}\nexport function deserializeActivation(config, customObjects = {}) {\n    return deserializeKerasObject(config, serialization.SerializationMap.getMap().classNameMap, customObjects, 'activation');\n}\nexport function getActivation(identifier) {\n    if (identifier == null) {\n        const config = {};\n        config['className'] = 'linear';\n        config['config'] = {};\n        return deserializeActivation(config);\n    }\n    if (typeof identifier === 'string') {\n        const config = {};\n        config['className'] = identifier;\n        config['config'] = {};\n        return deserializeActivation(config);\n    }\n    else if (identifier instanceof Activation) {\n        return identifier;\n    }\n    else {\n        return deserializeActivation(identifier);\n    }\n}\n//# sourceMappingURL=activations.js.map"]},"metadata":{},"sourceType":"module"}