{"ast":null,"code":"import _createForOfIteratorHelper from \"/Users/kimkiwoong/songil2/SonGil/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/createForOfIteratorHelper\";\nimport _classCallCheck from \"/Users/kimkiwoong/songil2/SonGil/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/classCallCheck\";\nimport _createClass from \"/Users/kimkiwoong/songil2/SonGil/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/createClass\";\n\n/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Executor: Evaluates SymbolicTensor based on feeds.\n */\nimport { cast, dispose, memory, util } from '@tensorflow/tfjs-core';\nimport { ValueError } from '../errors';\nimport { toList } from '../utils/generic_utils';\nimport { InputLayer } from './input_layer';\nimport { SymbolicTensor } from './topology';\n/**\n * Helper function to check the dtype and shape compatibility of a feed value.\n */\n\nfunction assertFeedCompatibility(key, val) {\n  // Check dtype compatibility.\n  if (key.dtype == null || key.dtype === val.dtype) {\n    //  a.  If types match, return val tensor as is.\n    return val;\n  }\n\n  try {\n    //  b. Attempt to convert to expected type.\n    return cast(val, key.dtype);\n  } catch (err) {\n    //  c. If conversion fails, return helpful error.\n    throw new ValueError(\"The dtype of the feed (\".concat(val.dtype, \") can not be cast to the dtype \") + \"of the key '\".concat(key.name, \"' (\").concat(key.dtype, \").\"));\n  }\n}\n/**\n * FeedDict: A mapping from unique SymbolicTensors to feed values for them.\n * A feed value is a concrete value represented as an `Tensor`.\n */\n\n\nexport var FeedDict = /*#__PURE__*/function () {\n  /**\n   * Constructor, optionally does copy-construction.\n   * @param feeds An Array of `Feed`s, or another `FeedDict`, in which case\n   *   copy-construction will be performed.\n   */\n  function FeedDict(feeds) {\n    _classCallCheck(this, FeedDict);\n\n    this.id2Value = {};\n    this.id2Mask = {};\n    this.name2Id = {};\n\n    if (feeds instanceof FeedDict) {\n      for (var id in feeds.id2Value) {\n        this.id2Value[id] = feeds.id2Value[id];\n\n        if (id in feeds.id2Mask) {\n          this.id2Mask[id] = feeds.id2Mask[id];\n        }\n      }\n    } else {\n      if (feeds == null) {\n        return;\n      }\n\n      var _iterator = _createForOfIteratorHelper(feeds),\n          _step;\n\n      try {\n        for (_iterator.s(); !(_step = _iterator.n()).done;) {\n          var feed = _step.value;\n          this.add(feed.key, feed.value);\n        }\n      } catch (err) {\n        _iterator.e(err);\n      } finally {\n        _iterator.f();\n      }\n    }\n  }\n  /**\n   * Add a key-value pair to the FeedDict.\n   *\n   * @param key The key of the feed.\n   * @param value The value of the tensor feed.\n   * @param mask The value of the mask feed (optional).\n   * @returns This `FeedDict`.\n   * @throws ValueError: If the key `SymbolicTensor` already exists in the\n   *   `FeedDict`.\n   */\n\n\n  _createClass(FeedDict, [{\n    key: \"add\",\n    value: function add(key, value, mask) {\n      if (this.id2Value[key.id] == null) {\n        this.id2Value[key.id] = assertFeedCompatibility(key, value);\n        this.name2Id[key.name] = key.id;\n\n        if (mask != null) {\n          this.id2Mask[key.id] = mask;\n        }\n      } else {\n        throw new ValueError(\"Duplicate key: name=\".concat(key.name, \", id=\").concat(key.id));\n      }\n\n      return this;\n    }\n    /**\n     * Add a Feed to the FeedDict.\n     * @param feed The new `Feed` to add.\n     * @returns This `FeedDict`.\n     */\n\n  }, {\n    key: \"addFeed\",\n    value: function addFeed(feed) {\n      this.add(feed.key, feed.value);\n    }\n    /**\n     * Probe whether a key already exists in the FeedDict.\n     * @param key\n     */\n\n  }, {\n    key: \"hasKey\",\n    value: function hasKey(key) {\n      return this.id2Value[key.id] != null;\n    }\n    /**\n     * Get all the SymbolicTensor available in this FeedDict.\n     */\n\n  }, {\n    key: \"names\",\n    value: function names() {\n      return Object.keys(this.name2Id);\n    }\n    /**\n     * Get the feed value for given key.\n     * @param key The SymbolicTensor, or its name (as a string), of which the\n     *     value is sought.\n     * @returns If `key` exists, the corresponding feed value.\n     * @throws ValueError: If `key` does not exist in this `FeedDict`.\n     */\n\n  }, {\n    key: \"getValue\",\n    value: function getValue(key) {\n      if (key instanceof SymbolicTensor) {\n        if (this.id2Value[key.id] == null) {\n          throw new ValueError(\"Nonexistent key: \".concat(key.name));\n        } else {\n          return this.id2Value[key.id];\n        }\n      } else {\n        var id = this.name2Id[key];\n\n        if (id == null) {\n          throw new ValueError(\"Feed dict has no SymbolicTensor name: \".concat(key));\n        }\n\n        return this.id2Value[id];\n      }\n    }\n    /**\n     * Get the feed mask for given key.\n     * @param key The SymbolicTensor, or its name (as a string), of which the\n     *     value is sought.\n     * @returns If `key` exists, the corresponding feed mask.\n     * @throws ValueError: If `key` does not exist in this `FeedDict`.\n     */\n\n  }, {\n    key: \"getMask\",\n    value: function getMask(key) {\n      if (key instanceof SymbolicTensor) {\n        if (this.id2Value[key.id] == null) {\n          throw new ValueError(\"Nonexistent key: \".concat(key.name));\n        } else {\n          return this.id2Mask[key.id];\n        }\n      } else {\n        var id = this.name2Id[key];\n\n        if (id == null) {\n          throw new ValueError(\"Feed dict has no SymbolicTensor name: \".concat(key));\n        }\n\n        return this.id2Mask[id];\n      }\n    }\n    /** Dispose all mask Tensors held by this object. */\n\n  }, {\n    key: \"disposeMasks\",\n    value: function disposeMasks() {\n      if (this.id2Mask != null) {\n        dispose(this.id2Mask);\n      }\n    }\n  }]);\n\n  return FeedDict;\n}(); // Cache for topologically sorted SymbolicTensors for given execution\n// targets (i.e., fetches).\n\nvar cachedSorted = {}; // Cache for recipient count maps for given execution targets (i.e., fetches).\n\nvar cachedRecipientCounts = {};\n/**\n * Execute a SymbolicTensor by using concrete feed values.\n *\n * A `SymbolicTensor` object is a node in a computation graph of TF.js\n * Layers. The object is backed by a source layer and input\n * `SymbolicTensor`s to the source layer. This method evaluates\n * the `call()` method of the source layer, using concrete values of the\n * inputs obtained from either\n * * `feedDict`, if the input key exists in `feedDict`, or else,\n * * a recursive call to `execute()` itself.\n *\n * @param x: The `SymbolicTensor` to execute.\n * @param feedDict: The feed values, as base condition of the recursion.\n *   execution.\n * @param kwargs: Optional keyword arguments.\n * @param probe: A probe object (of interface `ExecutionProbe`) used for\n *   testing memory footprint of `execute` calls.\n * @returns Result of the execution.\n * @throws ValueError: If any `SymbolicTensor`s from `InputLayer`s\n *   encountered during the execution lacks a feed value in `feedDict`.\n */\n\nexport function execute(fetches, feedDict, kwargs, probe) {\n  var training = kwargs == null ? false : kwargs['training'];\n  var arrayFetches = Array.isArray(fetches);\n  var fetchArray = arrayFetches ? fetches : [fetches];\n  var outputNames = fetchArray.map(function (t) {\n    return t.name;\n  });\n  var finalOutputs = [];\n  var feedNames = feedDict.names();\n\n  var _iterator2 = _createForOfIteratorHelper(outputNames),\n      _step2;\n\n  try {\n    for (_iterator2.s(); !(_step2 = _iterator2.n()).done;) {\n      var outputName = _step2.value;\n\n      if (feedNames.indexOf(outputName) !== -1) {\n        finalOutputs.push(feedDict.getValue(outputName));\n      } else {\n        finalOutputs.push(null);\n      }\n    }\n  } catch (err) {\n    _iterator2.e(err);\n  } finally {\n    _iterator2.f();\n  }\n\n  if (probe != null) {\n    // For optional probing of memory footprint during execution.\n    probe.maxNumTensors = -Infinity;\n    probe.minNumTensors = Infinity;\n  } // Check cache.\n\n\n  var fetchAndFeedKey = outputNames.join(',') + '|' + feedDict.names().join(',');\n  var sorted;\n  var recipientCounts;\n\n  if (cachedSorted[fetchAndFeedKey] == null) {\n    // Cache doesn't contain the desired combination of fetches. Compute\n    // topological sort for the combination for the first time.\n    var out = getTopologicalSortAndRecipientCounts(fetchArray, feedDict);\n    sorted = out.sorted;\n    recipientCounts = out.recipientCounts; // Store results in cache for future use.\n\n    cachedSorted[fetchAndFeedKey] = sorted;\n    cachedRecipientCounts[fetchAndFeedKey] = recipientCounts;\n  }\n\n  sorted = cachedSorted[fetchAndFeedKey];\n  recipientCounts = {};\n\n  if (!training) {\n    Object.assign(recipientCounts, cachedRecipientCounts[fetchAndFeedKey]);\n  }\n\n  var internalFeedDict = new FeedDict(feedDict); // Start iterative execution on the topologically-sorted SymbolicTensors.\n\n  for (var i = 0; i < sorted.length; ++i) {\n    if (probe != null) {\n      // For optional probing of memory usage during execution.\n      var numTensors = memory().numTensors;\n\n      if (numTensors > probe.maxNumTensors) {\n        probe.maxNumTensors = numTensors;\n      }\n\n      if (numTensors < probe.minNumTensors) {\n        probe.minNumTensors = numTensors;\n      }\n    }\n\n    var symbolic = sorted[i];\n    var srcLayer = symbolic.sourceLayer;\n\n    if (srcLayer instanceof InputLayer) {\n      continue;\n    }\n\n    var inputValues = [];\n    var inputMasks = [];\n    var tensorsToDispose = [];\n    var maskExists = false;\n\n    var _iterator3 = _createForOfIteratorHelper(symbolic.inputs),\n        _step3;\n\n    try {\n      for (_iterator3.s(); !(_step3 = _iterator3.n()).done;) {\n        var input = _step3.value;\n        var value = internalFeedDict.getValue(input);\n        var mask = internalFeedDict.getMask(input);\n        inputValues.push(value);\n        inputMasks.push(mask);\n\n        if (mask != null) {\n          maskExists = true;\n        }\n\n        if (!training) {\n          recipientCounts[input.name]--;\n\n          if (recipientCounts[input.name] === 0 && !feedDict.hasKey(input) && outputNames.indexOf(input.name) === -1 && !value.isDisposed && input.sourceLayer.stateful !== true) {\n            tensorsToDispose.push(value);\n          }\n        }\n      }\n    } catch (err) {\n      _iterator3.e(err);\n    } finally {\n      _iterator3.f();\n    }\n\n    if (maskExists) {\n      kwargs = kwargs || {};\n      kwargs['mask'] = inputMasks[0];\n    }\n\n    var outputTensors = toList(srcLayer.apply(inputValues, kwargs));\n    var outputMask = null;\n\n    if (srcLayer.supportsMasking) {\n      outputMask = srcLayer.computeMask(inputValues, inputMasks);\n    }\n\n    var layerOutputs = getNodeOutputs(symbolic);\n    var outputSymbolicTensors = Array.isArray(layerOutputs) ? layerOutputs : [layerOutputs];\n\n    for (var _i = 0; _i < outputSymbolicTensors.length; ++_i) {\n      if (!internalFeedDict.hasKey(outputSymbolicTensors[_i])) {\n        internalFeedDict.add(outputSymbolicTensors[_i], outputTensors[_i], Array.isArray(outputMask) ? outputMask[0] : outputMask);\n      }\n\n      var index = outputNames.indexOf(outputSymbolicTensors[_i].name);\n\n      if (index !== -1) {\n        finalOutputs[index] = outputTensors[_i];\n      }\n    }\n\n    if (!training) {\n      // Clean up Tensors that are no longer needed.\n      dispose(tensorsToDispose);\n    }\n  } // NOTE(cais): Unlike intermediate tensors, we don't discard mask\n  // tensors as we go, because these tensors are sometimes passed over a\n  // series of mutliple layers, i.e., not obeying the immediate input\n  // relations in the graph. If this becomes a memory-usage concern,\n  // we can improve this in the future.\n\n\n  internalFeedDict.disposeMasks();\n  return arrayFetches ? finalOutputs : finalOutputs[0];\n}\n/**\n * Sort the `SymbolicTensor`s topologically, for an array of fetches.\n *\n * This function calls getTopologicalSortAndRecipientCountsForOneFetch and\n * merges their results.\n *\n * @param fetch The array of fetches requested. Must be a non-empty array.\n * @param feedDict The dictionary of fed values.\n * @returns sorted: Topologically-sorted array of SymbolicTensors.\n *   recipientCounts: Recipient counts for all SymbolicTensors in `sorted`.\n */\n\nfunction getTopologicalSortAndRecipientCounts(fetches, feedDict) {\n  util.assert(fetches != null && fetches.length > 0, function () {\n    return \"Expected at least one fetch, got none\";\n  });\n  var finalSorted = [];\n  var finalRecipientMap = {};\n\n  if (fetches.length === 1) {\n    // Special-casing 1 fetch for efficiency.\n    var out = getTopologicalSortAndRecipientCountsForOneFetch(fetches[0], feedDict);\n    finalSorted = out.sorted;\n    finalRecipientMap = out.recipientMap;\n  } else {\n    var visited = new Set();\n\n    var _iterator4 = _createForOfIteratorHelper(fetches),\n        _step4;\n\n    try {\n      for (_iterator4.s(); !(_step4 = _iterator4.n()).done;) {\n        var fetch = _step4.value;\n\n        var _getTopologicalSortAn = getTopologicalSortAndRecipientCountsForOneFetch(fetch, feedDict),\n            sorted = _getTopologicalSortAn.sorted,\n            recipientMap = _getTopologicalSortAn.recipientMap; // Merge sorted SymbolicTensor Arrays.\n\n\n        var _iterator5 = _createForOfIteratorHelper(sorted),\n            _step5;\n\n        try {\n          for (_iterator5.s(); !(_step5 = _iterator5.n()).done;) {\n            var symbolicTensor = _step5.value;\n\n            if (!visited.has(symbolicTensor.name)) {\n              finalSorted.push(symbolicTensor);\n              visited.add(symbolicTensor.name);\n            }\n          } // Merge recipient maps.\n\n        } catch (err) {\n          _iterator5.e(err);\n        } finally {\n          _iterator5.f();\n        }\n\n        var _loop = function _loop(name) {\n          if (finalRecipientMap[name] == null) {\n            finalRecipientMap[name] = new Set();\n          }\n\n          recipientMap[name].forEach(function (recipient) {\n            return finalRecipientMap[name].add(recipient);\n          });\n        };\n\n        for (var name in recipientMap) {\n          _loop(name);\n        }\n      }\n    } catch (err) {\n      _iterator4.e(err);\n    } finally {\n      _iterator4.f();\n    }\n  }\n\n  return {\n    sorted: finalSorted,\n    recipientCounts: recipientMap2Counts(finalRecipientMap)\n  };\n}\n\nfunction recipientMap2Counts(recipientMap) {\n  var recipientCounts = {};\n\n  for (var name in recipientMap) {\n    recipientCounts[name] = recipientMap[name].size;\n  }\n\n  return recipientCounts;\n}\n/**\n * Sort the `SymbolicTensor`s topologically, for a single fetch.\n *\n * This helper function processes the upstream SymbolicTensors of a single\n * fetch.\n *\n * @param fetch The single fetch requested.\n * @param feedDict The dictionary of fed values.\n * @returns sorted: Topologically-sorted array of SymbolicTensors.\n *   recipientMap: Recipient names for all SymbolicTensors in `sorted`.\n */\n\n\nexport function getTopologicalSortAndRecipientCountsForOneFetch(fetch, feedDict) {\n  var visited = new Set();\n  var sorted = [];\n  var recipientMap = {}; // Put keys of the feedDict into visited first, so they don't have to be\n  // walked. This is needed in case where there are feeds for intermediate\n  // SymbolicTensors of the graph.\n\n  var _iterator6 = _createForOfIteratorHelper(feedDict.names()),\n      _step6;\n\n  try {\n    for (_iterator6.s(); !(_step6 = _iterator6.n()).done;) {\n      var key = _step6.value;\n      visited.add(key);\n    }\n  } catch (err) {\n    _iterator6.e(err);\n  } finally {\n    _iterator6.f();\n  }\n\n  var stack = [];\n  var marks = []; // Initial population of stack and marks.\n\n  stack.push(fetch);\n\n  while (stack.length > 0) {\n    var top = stack[stack.length - 1];\n\n    if (visited.has(top.name)) {\n      stack.pop();\n      continue;\n    }\n\n    var topIsMarked = marks[marks.length - 1] === stack.length - 1;\n\n    if (top.inputs.length === 0 || topIsMarked) {\n      // Input SymbolicTensor or all children have been visited.\n      stack.pop();\n      sorted.push(top);\n      visited.add(top.name);\n\n      if (topIsMarked) {\n        marks.pop();\n      }\n    } else {\n      // A non-input SymbolicTensor whose upstream SymbolicTensors haven't\n      // been visited yet. Push them onto the stack.\n      marks.push(stack.length - 1);\n\n      var _iterator7 = _createForOfIteratorHelper(top.inputs),\n          _step7;\n\n      try {\n        for (_iterator7.s(); !(_step7 = _iterator7.n()).done;) {\n          var input = _step7.value;\n\n          // Increment the recipient count. Note that this needs to happen\n          // regardless of whether the SymbolicTensor has been visited before.\n          if (recipientMap[input.name] == null) {\n            recipientMap[input.name] = new Set();\n          }\n\n          recipientMap[input.name].add(top.name);\n\n          if (visited.has(input.name)) {\n            continue; // Avoid repeated visits to the same SymbolicTensor.\n          }\n\n          stack.push(input);\n        }\n      } catch (err) {\n        _iterator7.e(err);\n      } finally {\n        _iterator7.f();\n      }\n    }\n  }\n\n  return {\n    sorted: sorted,\n    recipientMap: recipientMap\n  };\n}\n/**\n * Get the symbolic output tensors of the node to which a given fetch belongs.\n * @param fetch The fetched symbolic tensor.\n * @returns The Array of symbolic tensors output by the node to which `fetch`\n *   belongs.\n */\n\nfunction getNodeOutputs(fetch) {\n  var layerOutputs;\n\n  if (fetch.sourceLayer.inboundNodes.length === 1) {\n    layerOutputs = fetch.sourceLayer.output;\n  } else {\n    var nodeIndex = null;\n\n    for (var i = 0; i < fetch.sourceLayer.inboundNodes.length; ++i) {\n      var _iterator8 = _createForOfIteratorHelper(fetch.sourceLayer.inboundNodes[i].outputTensors),\n          _step8;\n\n      try {\n        for (_iterator8.s(); !(_step8 = _iterator8.n()).done;) {\n          var outputTensor = _step8.value;\n\n          if (outputTensor.id === fetch.id) {\n            nodeIndex = i;\n            break;\n          }\n        }\n      } catch (err) {\n        _iterator8.e(err);\n      } finally {\n        _iterator8.f();\n      }\n    }\n\n    layerOutputs = fetch.sourceLayer.getOutputAt(nodeIndex);\n  }\n\n  return layerOutputs;\n}","map":{"version":3,"sources":["../../src/engine/executor.ts"],"names":[],"mappings":";;;;AAAA;;;;;;;;AAQG;;AAEH;;AAEG;AAEH,SAAQ,IAAR,EAAc,OAAd,EAAuB,MAAvB,EAAuC,IAAvC,QAAkD,uBAAlD;AAEA,SAAQ,UAAR,QAAyB,WAAzB;AAEA,SAAQ,MAAR,QAAqB,wBAArB;AAEA,SAAQ,UAAR,QAAyB,eAAzB;AACA,SAAQ,cAAR,QAA6B,YAA7B;AAEA;;AAEG;;AACH,SAAS,uBAAT,CAAiC,GAAjC,EAAsD,GAAtD,EAAiE;AAC/D;AACA,MAAI,GAAG,CAAC,KAAJ,IAAa,IAAb,IAAqB,GAAG,CAAC,KAAJ,KAAc,GAAG,CAAC,KAA3C,EAAkD;AAChD;AACA,WAAO,GAAP;AACD;;AACD,MAAI;AACF;AACA,WAAO,IAAI,CAAC,GAAD,EAAM,GAAG,CAAC,KAAV,CAAX;AACD,GAHD,CAGE,OAAO,GAAP,EAAY;AACZ;AACA,UAAM,IAAI,UAAJ,CACF,iCAA0B,GAAG,CAAC,KAA9B,6DACe,GAAG,CAAC,IADnB,gBAC6B,GAAG,CAAC,KADjC,OADE,CAAN;AAGD;AACF;AAUD;;;AAGG;;;AACH,WAAa,QAAb;AAKE;;;;AAIG;AACH,oBAAY,KAAZ,EAAmC;AAAA;;AAT3B,SAAA,QAAA,GAAmC,EAAnC;AACA,SAAA,OAAA,GAAkC,EAAlC;AACA,SAAA,OAAA,GAAoC,EAApC;;AAQN,QAAI,KAAK,YAAY,QAArB,EAA+B;AAC7B,WAAK,IAAM,EAAX,IAAiB,KAAK,CAAC,QAAvB,EAAiC;AAC/B,aAAK,QAAL,CAAc,EAAd,IAAoB,KAAK,CAAC,QAAN,CAAe,EAAf,CAApB;;AACA,YAAI,EAAE,IAAI,KAAK,CAAC,OAAhB,EAAyB;AACvB,eAAK,OAAL,CAAa,EAAb,IAAmB,KAAK,CAAC,OAAN,CAAc,EAAd,CAAnB;AACD;AACF;AACF,KAPD,MAOO;AACL,UAAI,KAAK,IAAI,IAAb,EAAmB;AACjB;AACD;;AAHI,iDAIc,KAJd;AAAA;;AAAA;AAIL,4DAA0B;AAAA,cAAf,IAAe;AACxB,eAAK,GAAL,CAAS,IAAI,CAAC,GAAd,EAAmB,IAAI,CAAC,KAAxB;AACD;AANI;AAAA;AAAA;AAAA;AAAA;AAON;AACF;AAED;;;;;;;;;AASG;;;AArCL;AAAA;AAAA,WAsCE,aAAI,GAAJ,EAAyB,KAAzB,EAAwC,IAAxC,EAAqD;AACnD,UAAI,KAAK,QAAL,CAAc,GAAG,CAAC,EAAlB,KAAyB,IAA7B,EAAmC;AACjC,aAAK,QAAL,CAAc,GAAG,CAAC,EAAlB,IAAwB,uBAAuB,CAAC,GAAD,EAAM,KAAN,CAA/C;AACA,aAAK,OAAL,CAAa,GAAG,CAAC,IAAjB,IAAyB,GAAG,CAAC,EAA7B;;AACA,YAAI,IAAI,IAAI,IAAZ,EAAkB;AAChB,eAAK,OAAL,CAAa,GAAG,CAAC,EAAjB,IAAuB,IAAvB;AACD;AACF,OAND,MAMO;AACL,cAAM,IAAI,UAAJ,+BAAsC,GAAG,CAAC,IAA1C,kBAAsD,GAAG,CAAC,EAA1D,EAAN;AACD;;AACD,aAAO,IAAP;AACD;AAED;;;;AAIG;;AAvDL;AAAA;AAAA,WAwDE,iBAAQ,IAAR,EAAkB;AAChB,WAAK,GAAL,CAAS,IAAI,CAAC,GAAd,EAAmB,IAAI,CAAC,KAAxB;AACD;AAED;;;AAGG;;AA/DL;AAAA;AAAA,WAgEE,gBAAO,GAAP,EAA0B;AACxB,aAAO,KAAK,QAAL,CAAc,GAAG,CAAC,EAAlB,KAAyB,IAAhC;AACD;AAED;;AAEG;;AAtEL;AAAA;AAAA,WAuEE,iBAAK;AACH,aAAO,MAAM,CAAC,IAAP,CAAY,KAAK,OAAjB,CAAP;AACD;AAED;;;;;;AAMG;;AAjFL;AAAA;AAAA,WAkFE,kBAAS,GAAT,EAAmC;AACjC,UAAI,GAAG,YAAY,cAAnB,EAAmC;AACjC,YAAI,KAAK,QAAL,CAAc,GAAG,CAAC,EAAlB,KAAyB,IAA7B,EAAmC;AACjC,gBAAM,IAAI,UAAJ,4BAAmC,GAAG,CAAC,IAAvC,EAAN;AACD,SAFD,MAEO;AACL,iBAAO,KAAK,QAAL,CAAc,GAAG,CAAC,EAAlB,CAAP;AACD;AACF,OAND,MAMO;AACL,YAAM,EAAE,GAAG,KAAK,OAAL,CAAa,GAAb,CAAX;;AACA,YAAI,EAAE,IAAI,IAAV,EAAgB;AACd,gBAAM,IAAI,UAAJ,iDAAwD,GAAxD,EAAN;AACD;;AACD,eAAO,KAAK,QAAL,CAAc,EAAd,CAAP;AACD;AACF;AAED;;;;;;AAMG;;AAxGL;AAAA;AAAA,WAyGE,iBAAQ,GAAR,EAAkC;AAChC,UAAI,GAAG,YAAY,cAAnB,EAAmC;AACjC,YAAI,KAAK,QAAL,CAAc,GAAG,CAAC,EAAlB,KAAyB,IAA7B,EAAmC;AACjC,gBAAM,IAAI,UAAJ,4BAAmC,GAAG,CAAC,IAAvC,EAAN;AACD,SAFD,MAEO;AACL,iBAAO,KAAK,OAAL,CAAa,GAAG,CAAC,EAAjB,CAAP;AACD;AACF,OAND,MAMO;AACL,YAAM,EAAE,GAAG,KAAK,OAAL,CAAa,GAAb,CAAX;;AACA,YAAI,EAAE,IAAI,IAAV,EAAgB;AACd,gBAAM,IAAI,UAAJ,iDAAwD,GAAxD,EAAN;AACD;;AACD,eAAO,KAAK,OAAL,CAAa,EAAb,CAAP;AACD;AACF;AAED;;AAzHF;AAAA;AAAA,WA0HE,wBAAY;AACV,UAAI,KAAK,OAAL,IAAgB,IAApB,EAA0B;AACxB,QAAA,OAAO,CAAC,KAAK,OAAN,CAAP;AACD;AACF;AA9HH;;AAAA;AAAA,I,CAiIA;AACA;;AACA,IAAM,YAAY,GAAmD,EAArE,C,CAEA;;AACA,IAAM,qBAAqB,GACuC,EADlE;AAuBA;;;;;;;;;;;;;;;;;;;;AAoBG;;AACH,OAAM,SAAU,OAAV,CACF,OADE,EACwC,QADxC,EAEF,MAFE,EAEe,KAFf,EAEqC;AAEzC,MAAM,QAAQ,GAAY,MAAM,IAAI,IAAV,GAAiB,KAAjB,GAAyB,MAAM,CAAC,UAAD,CAAzD;AAEA,MAAM,YAAY,GAAG,KAAK,CAAC,OAAN,CAAc,OAAd,CAArB;AACA,MAAM,UAAU,GACZ,YAAY,GAAG,OAAH,GAAiC,CAAC,OAAD,CADjD;AAGA,MAAM,WAAW,GAAG,UAAU,CAAC,GAAX,CAAe,UAAA,CAAC;AAAA,WAAI,CAAC,CAAC,IAAN;AAAA,GAAhB,CAApB;AACA,MAAM,YAAY,GAAa,EAA/B;AACA,MAAM,SAAS,GAAG,QAAQ,CAAC,KAAT,EAAlB;;AAVyC,8CAWhB,WAXgB;AAAA;;AAAA;AAWzC,2DAAsC;AAAA,UAA3B,UAA2B;;AACpC,UAAI,SAAS,CAAC,OAAV,CAAkB,UAAlB,MAAkC,CAAC,CAAvC,EAA0C;AACxC,QAAA,YAAY,CAAC,IAAb,CAAkB,QAAQ,CAAC,QAAT,CAAkB,UAAlB,CAAlB;AACD,OAFD,MAEO;AACL,QAAA,YAAY,CAAC,IAAb,CAAkB,IAAlB;AACD;AACF;AAjBwC;AAAA;AAAA;AAAA;AAAA;;AAmBzC,MAAI,KAAK,IAAI,IAAb,EAAmB;AACjB;AACA,IAAA,KAAK,CAAC,aAAN,GAAsB,CAAC,QAAvB;AACA,IAAA,KAAK,CAAC,aAAN,GAAsB,QAAtB;AACD,GAvBwC,CAyBzC;;;AACA,MAAM,eAAe,GACjB,WAAW,CAAC,IAAZ,CAAiB,GAAjB,IAAwB,GAAxB,GAA8B,QAAQ,CAAC,KAAT,GAAiB,IAAjB,CAAsB,GAAtB,CADlC;AAEA,MAAI,MAAJ;AACA,MAAI,eAAJ;;AACA,MAAI,YAAY,CAAC,eAAD,CAAZ,IAAiC,IAArC,EAA2C;AACzC;AACA;AACA,QAAM,GAAG,GAAG,oCAAoC,CAAC,UAAD,EAAa,QAAb,CAAhD;AACA,IAAA,MAAM,GAAG,GAAG,CAAC,MAAb;AACA,IAAA,eAAe,GAAG,GAAG,CAAC,eAAtB,CALyC,CAOzC;;AACA,IAAA,YAAY,CAAC,eAAD,CAAZ,GAAgC,MAAhC;AACA,IAAA,qBAAqB,CAAC,eAAD,CAArB,GAAyC,eAAzC;AACD;;AACD,EAAA,MAAM,GAAG,YAAY,CAAC,eAAD,CAArB;AACA,EAAA,eAAe,GAAG,EAAlB;;AACA,MAAI,CAAC,QAAL,EAAe;AACb,IAAA,MAAM,CAAC,MAAP,CAAc,eAAd,EAA+B,qBAAqB,CAAC,eAAD,CAApD;AACD;;AAED,MAAM,gBAAgB,GAAG,IAAI,QAAJ,CAAa,QAAb,CAAzB,CA/CyC,CAiDzC;;AACA,OAAK,IAAI,CAAC,GAAG,CAAb,EAAgB,CAAC,GAAG,MAAM,CAAC,MAA3B,EAAmC,EAAE,CAArC,EAAwC;AACtC,QAAI,KAAK,IAAI,IAAb,EAAmB;AACjB;AACA,UAAM,UAAU,GAAG,MAAM,GAAG,UAA5B;;AACA,UAAI,UAAU,GAAG,KAAK,CAAC,aAAvB,EAAsC;AACpC,QAAA,KAAK,CAAC,aAAN,GAAsB,UAAtB;AACD;;AACD,UAAI,UAAU,GAAG,KAAK,CAAC,aAAvB,EAAsC;AACpC,QAAA,KAAK,CAAC,aAAN,GAAsB,UAAtB;AACD;AACF;;AAED,QAAM,QAAQ,GAAG,MAAM,CAAC,CAAD,CAAvB;AACA,QAAM,QAAQ,GAAG,QAAQ,CAAC,WAA1B;;AACA,QAAI,QAAQ,YAAY,UAAxB,EAAoC;AAClC;AACD;;AACD,QAAM,WAAW,GAAa,EAA9B;AACA,QAAM,UAAU,GAAa,EAA7B;AACA,QAAM,gBAAgB,GAAa,EAAnC;AAEA,QAAI,UAAU,GAAG,KAAjB;;AArBsC,gDAsBlB,QAAQ,CAAC,MAtBS;AAAA;;AAAA;AAsBtC,6DAAqC;AAAA,YAA1B,KAA0B;AACnC,YAAM,KAAK,GAAG,gBAAgB,CAAC,QAAjB,CAA0B,KAA1B,CAAd;AACA,YAAM,IAAI,GAAG,gBAAgB,CAAC,OAAjB,CAAyB,KAAzB,CAAb;AACA,QAAA,WAAW,CAAC,IAAZ,CAAiB,KAAjB;AACA,QAAA,UAAU,CAAC,IAAX,CAAgB,IAAhB;;AACA,YAAI,IAAI,IAAI,IAAZ,EAAkB;AAChB,UAAA,UAAU,GAAG,IAAb;AACD;;AACD,YAAI,CAAC,QAAL,EAAe;AACb,UAAA,eAAe,CAAC,KAAK,CAAC,IAAP,CAAf;;AACA,cAAI,eAAe,CAAC,KAAK,CAAC,IAAP,CAAf,KAAgC,CAAhC,IAAqC,CAAC,QAAQ,CAAC,MAAT,CAAgB,KAAhB,CAAtC,IACA,WAAW,CAAC,OAAZ,CAAoB,KAAK,CAAC,IAA1B,MAAoC,CAAC,CADrC,IAC0C,CAAC,KAAK,CAAC,UADjD,IAEA,KAAK,CAAC,WAAN,CAAkB,QAAlB,KAA+B,IAFnC,EAEyC;AACvC,YAAA,gBAAgB,CAAC,IAAjB,CAAsB,KAAtB;AACD;AACF;AACF;AAtCqC;AAAA;AAAA;AAAA;AAAA;;AAwCtC,QAAI,UAAJ,EAAgB;AACd,MAAA,MAAM,GAAG,MAAM,IAAI,EAAnB;AACA,MAAA,MAAM,CAAC,MAAD,CAAN,GAAiB,UAAU,CAAC,CAAD,CAA3B;AACD;;AACD,QAAM,aAAa,GACf,MAAM,CAAC,QAAQ,CAAC,KAAT,CAAe,WAAf,EAA4B,MAA5B,CAAD,CADV;AAEA,QAAI,UAAU,GAAoB,IAAlC;;AACA,QAAI,QAAQ,CAAC,eAAb,EAA8B;AAC5B,MAAA,UAAU,GAAG,QAAQ,CAAC,WAAT,CAAqB,WAArB,EAAkC,UAAlC,CAAb;AACD;;AACD,QAAM,YAAY,GAAG,cAAc,CAAC,QAAD,CAAnC;AACA,QAAM,qBAAqB,GACvB,KAAK,CAAC,OAAN,CAAc,YAAd,IAA8B,YAA9B,GAA6C,CAAC,YAAD,CADjD;;AAEA,SAAK,IAAI,EAAC,GAAG,CAAb,EAAgB,EAAC,GAAG,qBAAqB,CAAC,MAA1C,EAAkD,EAAE,EAApD,EAAuD;AACrD,UAAI,CAAC,gBAAgB,CAAC,MAAjB,CAAwB,qBAAqB,CAAC,EAAD,CAA7C,CAAL,EAAwD;AACtD,QAAA,gBAAgB,CAAC,GAAjB,CACI,qBAAqB,CAAC,EAAD,CADzB,EAC8B,aAAa,CAAC,EAAD,CAD3C,EAEI,KAAK,CAAC,OAAN,CAAc,UAAd,IAA4B,UAAU,CAAC,CAAD,CAAtC,GAA4C,UAFhD;AAGD;;AACD,UAAM,KAAK,GAAG,WAAW,CAAC,OAAZ,CAAoB,qBAAqB,CAAC,EAAD,CAArB,CAAyB,IAA7C,CAAd;;AACA,UAAI,KAAK,KAAK,CAAC,CAAf,EAAkB;AAChB,QAAA,YAAY,CAAC,KAAD,CAAZ,GAAsB,aAAa,CAAC,EAAD,CAAnC;AACD;AACF;;AAED,QAAI,CAAC,QAAL,EAAe;AACb;AACA,MAAA,OAAO,CAAC,gBAAD,CAAP;AACD;AACF,GAvHwC,CAwHzC;AACA;AACA;AACA;AACA;;;AACA,EAAA,gBAAgB,CAAC,YAAjB;AAEA,SAAO,YAAY,GAAG,YAAH,GAAkB,YAAY,CAAC,CAAD,CAAjD;AACD;AAUD;;;;;;;;;;AAUG;;AACH,SAAS,oCAAT,CACI,OADJ,EAC+B,QAD/B,EACiD;AAE/C,EAAA,IAAI,CAAC,MAAL,CACI,OAAO,IAAI,IAAX,IAAmB,OAAO,CAAC,MAAR,GAAiB,CADxC,EAEI;AAAA;AAAA,GAFJ;AAIA,MAAI,WAAW,GAAqB,EAApC;AACA,MAAI,iBAAiB,GAAiB,EAAtC;;AACA,MAAI,OAAO,CAAC,MAAR,KAAmB,CAAvB,EAA0B;AACxB;AACA,QAAM,GAAG,GACL,+CAA+C,CAAC,OAAO,CAAC,CAAD,CAAR,EAAa,QAAb,CADnD;AAEA,IAAA,WAAW,GAAG,GAAG,CAAC,MAAlB;AACA,IAAA,iBAAiB,GAAG,GAAG,CAAC,YAAxB;AACD,GAND,MAMO;AACL,QAAM,OAAO,GAAG,IAAI,GAAJ,EAAhB;;AADK,gDAEe,OAFf;AAAA;;AAAA;AAEL,6DAA6B;AAAA,YAAlB,KAAkB;;AAAA,oCAEvB,+CAA+C,CAAC,KAAD,EAAQ,QAAR,CAFxB;AAAA,YACpB,MADoB,yBACpB,MADoB;AAAA,YACZ,YADY,yBACZ,YADY,EAI3B;;;AAJ2B,oDAKE,MALF;AAAA;;AAAA;AAK3B,iEAAqC;AAAA,gBAA1B,cAA0B;;AACnC,gBAAI,CAAC,OAAO,CAAC,GAAR,CAAY,cAAc,CAAC,IAA3B,CAAL,EAAuC;AACrC,cAAA,WAAW,CAAC,IAAZ,CAAiB,cAAjB;AACA,cAAA,OAAO,CAAC,GAAR,CAAY,cAAc,CAAC,IAA3B;AACD;AACF,WAV0B,CAY3B;;AAZ2B;AAAA;AAAA;AAAA;AAAA;;AAAA,mCAahB,IAbgB;AAczB,cAAI,iBAAiB,CAAC,IAAD,CAAjB,IAA2B,IAA/B,EAAqC;AACnC,YAAA,iBAAiB,CAAC,IAAD,CAAjB,GAA0B,IAAI,GAAJ,EAA1B;AACD;;AACD,UAAA,YAAY,CAAC,IAAD,CAAZ,CAAmB,OAAnB,CACI,UAAA,SAAS;AAAA,mBAAI,iBAAiB,CAAC,IAAD,CAAjB,CAAwB,GAAxB,CAA4B,SAA5B,CAAJ;AAAA,WADb;AAjByB;;AAa3B,aAAK,IAAM,IAAX,IAAmB,YAAnB,EAAiC;AAAA,gBAAtB,IAAsB;AAMhC;AACF;AAtBI;AAAA;AAAA;AAAA;AAAA;AAuBN;;AACD,SAAO;AACL,IAAA,MAAM,EAAE,WADH;AAEL,IAAA,eAAe,EAAE,mBAAmB,CAAC,iBAAD;AAF/B,GAAP;AAID;;AAED,SAAS,mBAAT,CAA6B,YAA7B,EAAuD;AACrD,MAAM,eAAe,GAAoB,EAAzC;;AACA,OAAK,IAAM,IAAX,IAAmB,YAAnB,EAAiC;AAC/B,IAAA,eAAe,CAAC,IAAD,CAAf,GAAwB,YAAY,CAAC,IAAD,CAAZ,CAAmB,IAA3C;AACD;;AACD,SAAO,eAAP;AACD;AAED;;;;;;;;;;AAUG;;;AACH,OAAM,SAAU,+CAAV,CACF,KADE,EACqB,QADrB,EACuC;AAE3C,MAAM,OAAO,GAAG,IAAI,GAAJ,EAAhB;AACA,MAAM,MAAM,GAAqB,EAAjC;AACA,MAAM,YAAY,GAAiB,EAAnC,CAJ2C,CAM3C;AACA;AACA;;AAR2C,8CASzB,QAAQ,CAAC,KAAT,EATyB;AAAA;;AAAA;AAS3C,2DAAoC;AAAA,UAAzB,GAAyB;AAClC,MAAA,OAAO,CAAC,GAAR,CAAY,GAAZ;AACD;AAX0C;AAAA;AAAA;AAAA;AAAA;;AAa3C,MAAM,KAAK,GAAqB,EAAhC;AACA,MAAM,KAAK,GAAa,EAAxB,CAd2C,CAgB3C;;AACA,EAAA,KAAK,CAAC,IAAN,CAAW,KAAX;;AAEA,SAAO,KAAK,CAAC,MAAN,GAAe,CAAtB,EAAyB;AACvB,QAAM,GAAG,GAAG,KAAK,CAAC,KAAK,CAAC,MAAN,GAAe,CAAhB,CAAjB;;AACA,QAAI,OAAO,CAAC,GAAR,CAAY,GAAG,CAAC,IAAhB,CAAJ,EAA2B;AACzB,MAAA,KAAK,CAAC,GAAN;AACA;AACD;;AACD,QAAM,WAAW,GAAG,KAAK,CAAC,KAAK,CAAC,MAAN,GAAe,CAAhB,CAAL,KAA4B,KAAK,CAAC,MAAN,GAAe,CAA/D;;AACA,QAAI,GAAG,CAAC,MAAJ,CAAW,MAAX,KAAsB,CAAtB,IAA2B,WAA/B,EAA4C;AAC1C;AACA,MAAA,KAAK,CAAC,GAAN;AACA,MAAA,MAAM,CAAC,IAAP,CAAY,GAAZ;AACA,MAAA,OAAO,CAAC,GAAR,CAAY,GAAG,CAAC,IAAhB;;AACA,UAAI,WAAJ,EAAiB;AACf,QAAA,KAAK,CAAC,GAAN;AACD;AACF,KARD,MAQO;AACL;AACA;AACA,MAAA,KAAK,CAAC,IAAN,CAAW,KAAK,CAAC,MAAN,GAAe,CAA1B;;AAHK,kDAIe,GAAG,CAAC,MAJnB;AAAA;;AAAA;AAIL,+DAAgC;AAAA,cAArB,KAAqB;;AAC9B;AACA;AACA,cAAI,YAAY,CAAC,KAAK,CAAC,IAAP,CAAZ,IAA4B,IAAhC,EAAsC;AACpC,YAAA,YAAY,CAAC,KAAK,CAAC,IAAP,CAAZ,GAA2B,IAAI,GAAJ,EAA3B;AACD;;AACD,UAAA,YAAY,CAAC,KAAK,CAAC,IAAP,CAAZ,CAAyB,GAAzB,CAA6B,GAAG,CAAC,IAAjC;;AAEA,cAAI,OAAO,CAAC,GAAR,CAAY,KAAK,CAAC,IAAlB,CAAJ,EAA6B;AAC3B,qBAD2B,CAChB;AACZ;;AACD,UAAA,KAAK,CAAC,IAAN,CAAW,KAAX;AACD;AAhBI;AAAA;AAAA;AAAA;AAAA;AAiBN;AACF;;AACD,SAAO;AAAC,IAAA,MAAM,EAAN,MAAD;AAAS,IAAA,YAAY,EAAZ;AAAT,GAAP;AACD;AAED;;;;;AAKG;;AACH,SAAS,cAAT,CAAwB,KAAxB,EAA6C;AAE3C,MAAI,YAAJ;;AACA,MAAI,KAAK,CAAC,WAAN,CAAkB,YAAlB,CAA+B,MAA/B,KAA0C,CAA9C,EAAiD;AAC/C,IAAA,YAAY,GAAG,KAAK,CAAC,WAAN,CAAkB,MAAjC;AACD,GAFD,MAEO;AACL,QAAI,SAAS,GAAW,IAAxB;;AACA,SAAK,IAAI,CAAC,GAAG,CAAb,EAAgB,CAAC,GAAG,KAAK,CAAC,WAAN,CAAkB,YAAlB,CAA+B,MAAnD,EAA2D,EAAE,CAA7D,EAAgE;AAAA,kDACnC,KAAK,CAAC,WAAN,CAAkB,YAAlB,CAA+B,CAA/B,EACjB,aAFoD;AAAA;;AAAA;AAC9D,+DACyB;AAAA,cADd,YACc;;AACvB,cAAI,YAAY,CAAC,EAAb,KAAoB,KAAK,CAAC,EAA9B,EAAkC;AAChC,YAAA,SAAS,GAAG,CAAZ;AACA;AACD;AACF;AAP6D;AAAA;AAAA;AAAA;AAAA;AAQ/D;;AACD,IAAA,YAAY,GAAG,KAAK,CAAC,WAAN,CAAkB,WAAlB,CAA8B,SAA9B,CAAf;AACD;;AACD,SAAO,YAAP;AACD","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * Executor: Evaluates SymbolicTensor based on feeds.\n */\nimport { cast, dispose, memory, util } from '@tensorflow/tfjs-core';\nimport { ValueError } from '../errors';\nimport { toList } from '../utils/generic_utils';\nimport { InputLayer } from './input_layer';\nimport { SymbolicTensor } from './topology';\n/**\n * Helper function to check the dtype and shape compatibility of a feed value.\n */\nfunction assertFeedCompatibility(key, val) {\n    // Check dtype compatibility.\n    if (key.dtype == null || key.dtype === val.dtype) {\n        //  a.  If types match, return val tensor as is.\n        return val;\n    }\n    try {\n        //  b. Attempt to convert to expected type.\n        return cast(val, key.dtype);\n    }\n    catch (err) {\n        //  c. If conversion fails, return helpful error.\n        throw new ValueError(`The dtype of the feed (${val.dtype}) can not be cast to the dtype ` +\n            `of the key '${key.name}' (${key.dtype}).`);\n    }\n}\n/**\n * FeedDict: A mapping from unique SymbolicTensors to feed values for them.\n * A feed value is a concrete value represented as an `Tensor`.\n */\nexport class FeedDict {\n    /**\n     * Constructor, optionally does copy-construction.\n     * @param feeds An Array of `Feed`s, or another `FeedDict`, in which case\n     *   copy-construction will be performed.\n     */\n    constructor(feeds) {\n        this.id2Value = {};\n        this.id2Mask = {};\n        this.name2Id = {};\n        if (feeds instanceof FeedDict) {\n            for (const id in feeds.id2Value) {\n                this.id2Value[id] = feeds.id2Value[id];\n                if (id in feeds.id2Mask) {\n                    this.id2Mask[id] = feeds.id2Mask[id];\n                }\n            }\n        }\n        else {\n            if (feeds == null) {\n                return;\n            }\n            for (const feed of feeds) {\n                this.add(feed.key, feed.value);\n            }\n        }\n    }\n    /**\n     * Add a key-value pair to the FeedDict.\n     *\n     * @param key The key of the feed.\n     * @param value The value of the tensor feed.\n     * @param mask The value of the mask feed (optional).\n     * @returns This `FeedDict`.\n     * @throws ValueError: If the key `SymbolicTensor` already exists in the\n     *   `FeedDict`.\n     */\n    add(key, value, mask) {\n        if (this.id2Value[key.id] == null) {\n            this.id2Value[key.id] = assertFeedCompatibility(key, value);\n            this.name2Id[key.name] = key.id;\n            if (mask != null) {\n                this.id2Mask[key.id] = mask;\n            }\n        }\n        else {\n            throw new ValueError(`Duplicate key: name=${key.name}, id=${key.id}`);\n        }\n        return this;\n    }\n    /**\n     * Add a Feed to the FeedDict.\n     * @param feed The new `Feed` to add.\n     * @returns This `FeedDict`.\n     */\n    addFeed(feed) {\n        this.add(feed.key, feed.value);\n    }\n    /**\n     * Probe whether a key already exists in the FeedDict.\n     * @param key\n     */\n    hasKey(key) {\n        return this.id2Value[key.id] != null;\n    }\n    /**\n     * Get all the SymbolicTensor available in this FeedDict.\n     */\n    names() {\n        return Object.keys(this.name2Id);\n    }\n    /**\n     * Get the feed value for given key.\n     * @param key The SymbolicTensor, or its name (as a string), of which the\n     *     value is sought.\n     * @returns If `key` exists, the corresponding feed value.\n     * @throws ValueError: If `key` does not exist in this `FeedDict`.\n     */\n    getValue(key) {\n        if (key instanceof SymbolicTensor) {\n            if (this.id2Value[key.id] == null) {\n                throw new ValueError(`Nonexistent key: ${key.name}`);\n            }\n            else {\n                return this.id2Value[key.id];\n            }\n        }\n        else {\n            const id = this.name2Id[key];\n            if (id == null) {\n                throw new ValueError(`Feed dict has no SymbolicTensor name: ${key}`);\n            }\n            return this.id2Value[id];\n        }\n    }\n    /**\n     * Get the feed mask for given key.\n     * @param key The SymbolicTensor, or its name (as a string), of which the\n     *     value is sought.\n     * @returns If `key` exists, the corresponding feed mask.\n     * @throws ValueError: If `key` does not exist in this `FeedDict`.\n     */\n    getMask(key) {\n        if (key instanceof SymbolicTensor) {\n            if (this.id2Value[key.id] == null) {\n                throw new ValueError(`Nonexistent key: ${key.name}`);\n            }\n            else {\n                return this.id2Mask[key.id];\n            }\n        }\n        else {\n            const id = this.name2Id[key];\n            if (id == null) {\n                throw new ValueError(`Feed dict has no SymbolicTensor name: ${key}`);\n            }\n            return this.id2Mask[id];\n        }\n    }\n    /** Dispose all mask Tensors held by this object. */\n    disposeMasks() {\n        if (this.id2Mask != null) {\n            dispose(this.id2Mask);\n        }\n    }\n}\n// Cache for topologically sorted SymbolicTensors for given execution\n// targets (i.e., fetches).\nconst cachedSorted = {};\n// Cache for recipient count maps for given execution targets (i.e., fetches).\nconst cachedRecipientCounts = {};\n/**\n * Execute a SymbolicTensor by using concrete feed values.\n *\n * A `SymbolicTensor` object is a node in a computation graph of TF.js\n * Layers. The object is backed by a source layer and input\n * `SymbolicTensor`s to the source layer. This method evaluates\n * the `call()` method of the source layer, using concrete values of the\n * inputs obtained from either\n * * `feedDict`, if the input key exists in `feedDict`, or else,\n * * a recursive call to `execute()` itself.\n *\n * @param x: The `SymbolicTensor` to execute.\n * @param feedDict: The feed values, as base condition of the recursion.\n *   execution.\n * @param kwargs: Optional keyword arguments.\n * @param probe: A probe object (of interface `ExecutionProbe`) used for\n *   testing memory footprint of `execute` calls.\n * @returns Result of the execution.\n * @throws ValueError: If any `SymbolicTensor`s from `InputLayer`s\n *   encountered during the execution lacks a feed value in `feedDict`.\n */\nexport function execute(fetches, feedDict, kwargs, probe) {\n    const training = kwargs == null ? false : kwargs['training'];\n    const arrayFetches = Array.isArray(fetches);\n    const fetchArray = arrayFetches ? fetches : [fetches];\n    const outputNames = fetchArray.map(t => t.name);\n    const finalOutputs = [];\n    const feedNames = feedDict.names();\n    for (const outputName of outputNames) {\n        if (feedNames.indexOf(outputName) !== -1) {\n            finalOutputs.push(feedDict.getValue(outputName));\n        }\n        else {\n            finalOutputs.push(null);\n        }\n    }\n    if (probe != null) {\n        // For optional probing of memory footprint during execution.\n        probe.maxNumTensors = -Infinity;\n        probe.minNumTensors = Infinity;\n    }\n    // Check cache.\n    const fetchAndFeedKey = outputNames.join(',') + '|' + feedDict.names().join(',');\n    let sorted;\n    let recipientCounts;\n    if (cachedSorted[fetchAndFeedKey] == null) {\n        // Cache doesn't contain the desired combination of fetches. Compute\n        // topological sort for the combination for the first time.\n        const out = getTopologicalSortAndRecipientCounts(fetchArray, feedDict);\n        sorted = out.sorted;\n        recipientCounts = out.recipientCounts;\n        // Store results in cache for future use.\n        cachedSorted[fetchAndFeedKey] = sorted;\n        cachedRecipientCounts[fetchAndFeedKey] = recipientCounts;\n    }\n    sorted = cachedSorted[fetchAndFeedKey];\n    recipientCounts = {};\n    if (!training) {\n        Object.assign(recipientCounts, cachedRecipientCounts[fetchAndFeedKey]);\n    }\n    const internalFeedDict = new FeedDict(feedDict);\n    // Start iterative execution on the topologically-sorted SymbolicTensors.\n    for (let i = 0; i < sorted.length; ++i) {\n        if (probe != null) {\n            // For optional probing of memory usage during execution.\n            const numTensors = memory().numTensors;\n            if (numTensors > probe.maxNumTensors) {\n                probe.maxNumTensors = numTensors;\n            }\n            if (numTensors < probe.minNumTensors) {\n                probe.minNumTensors = numTensors;\n            }\n        }\n        const symbolic = sorted[i];\n        const srcLayer = symbolic.sourceLayer;\n        if (srcLayer instanceof InputLayer) {\n            continue;\n        }\n        const inputValues = [];\n        const inputMasks = [];\n        const tensorsToDispose = [];\n        let maskExists = false;\n        for (const input of symbolic.inputs) {\n            const value = internalFeedDict.getValue(input);\n            const mask = internalFeedDict.getMask(input);\n            inputValues.push(value);\n            inputMasks.push(mask);\n            if (mask != null) {\n                maskExists = true;\n            }\n            if (!training) {\n                recipientCounts[input.name]--;\n                if (recipientCounts[input.name] === 0 && !feedDict.hasKey(input) &&\n                    outputNames.indexOf(input.name) === -1 && !value.isDisposed &&\n                    input.sourceLayer.stateful !== true) {\n                    tensorsToDispose.push(value);\n                }\n            }\n        }\n        if (maskExists) {\n            kwargs = kwargs || {};\n            kwargs['mask'] = inputMasks[0];\n        }\n        const outputTensors = toList(srcLayer.apply(inputValues, kwargs));\n        let outputMask = null;\n        if (srcLayer.supportsMasking) {\n            outputMask = srcLayer.computeMask(inputValues, inputMasks);\n        }\n        const layerOutputs = getNodeOutputs(symbolic);\n        const outputSymbolicTensors = Array.isArray(layerOutputs) ? layerOutputs : [layerOutputs];\n        for (let i = 0; i < outputSymbolicTensors.length; ++i) {\n            if (!internalFeedDict.hasKey(outputSymbolicTensors[i])) {\n                internalFeedDict.add(outputSymbolicTensors[i], outputTensors[i], Array.isArray(outputMask) ? outputMask[0] : outputMask);\n            }\n            const index = outputNames.indexOf(outputSymbolicTensors[i].name);\n            if (index !== -1) {\n                finalOutputs[index] = outputTensors[i];\n            }\n        }\n        if (!training) {\n            // Clean up Tensors that are no longer needed.\n            dispose(tensorsToDispose);\n        }\n    }\n    // NOTE(cais): Unlike intermediate tensors, we don't discard mask\n    // tensors as we go, because these tensors are sometimes passed over a\n    // series of mutliple layers, i.e., not obeying the immediate input\n    // relations in the graph. If this becomes a memory-usage concern,\n    // we can improve this in the future.\n    internalFeedDict.disposeMasks();\n    return arrayFetches ? finalOutputs : finalOutputs[0];\n}\n/**\n * Sort the `SymbolicTensor`s topologically, for an array of fetches.\n *\n * This function calls getTopologicalSortAndRecipientCountsForOneFetch and\n * merges their results.\n *\n * @param fetch The array of fetches requested. Must be a non-empty array.\n * @param feedDict The dictionary of fed values.\n * @returns sorted: Topologically-sorted array of SymbolicTensors.\n *   recipientCounts: Recipient counts for all SymbolicTensors in `sorted`.\n */\nfunction getTopologicalSortAndRecipientCounts(fetches, feedDict) {\n    util.assert(fetches != null && fetches.length > 0, () => `Expected at least one fetch, got none`);\n    let finalSorted = [];\n    let finalRecipientMap = {};\n    if (fetches.length === 1) {\n        // Special-casing 1 fetch for efficiency.\n        const out = getTopologicalSortAndRecipientCountsForOneFetch(fetches[0], feedDict);\n        finalSorted = out.sorted;\n        finalRecipientMap = out.recipientMap;\n    }\n    else {\n        const visited = new Set();\n        for (const fetch of fetches) {\n            const { sorted, recipientMap } = getTopologicalSortAndRecipientCountsForOneFetch(fetch, feedDict);\n            // Merge sorted SymbolicTensor Arrays.\n            for (const symbolicTensor of sorted) {\n                if (!visited.has(symbolicTensor.name)) {\n                    finalSorted.push(symbolicTensor);\n                    visited.add(symbolicTensor.name);\n                }\n            }\n            // Merge recipient maps.\n            for (const name in recipientMap) {\n                if (finalRecipientMap[name] == null) {\n                    finalRecipientMap[name] = new Set();\n                }\n                recipientMap[name].forEach(recipient => finalRecipientMap[name].add(recipient));\n            }\n        }\n    }\n    return {\n        sorted: finalSorted,\n        recipientCounts: recipientMap2Counts(finalRecipientMap)\n    };\n}\nfunction recipientMap2Counts(recipientMap) {\n    const recipientCounts = {};\n    for (const name in recipientMap) {\n        recipientCounts[name] = recipientMap[name].size;\n    }\n    return recipientCounts;\n}\n/**\n * Sort the `SymbolicTensor`s topologically, for a single fetch.\n *\n * This helper function processes the upstream SymbolicTensors of a single\n * fetch.\n *\n * @param fetch The single fetch requested.\n * @param feedDict The dictionary of fed values.\n * @returns sorted: Topologically-sorted array of SymbolicTensors.\n *   recipientMap: Recipient names for all SymbolicTensors in `sorted`.\n */\nexport function getTopologicalSortAndRecipientCountsForOneFetch(fetch, feedDict) {\n    const visited = new Set();\n    const sorted = [];\n    const recipientMap = {};\n    // Put keys of the feedDict into visited first, so they don't have to be\n    // walked. This is needed in case where there are feeds for intermediate\n    // SymbolicTensors of the graph.\n    for (const key of feedDict.names()) {\n        visited.add(key);\n    }\n    const stack = [];\n    const marks = [];\n    // Initial population of stack and marks.\n    stack.push(fetch);\n    while (stack.length > 0) {\n        const top = stack[stack.length - 1];\n        if (visited.has(top.name)) {\n            stack.pop();\n            continue;\n        }\n        const topIsMarked = marks[marks.length - 1] === stack.length - 1;\n        if (top.inputs.length === 0 || topIsMarked) {\n            // Input SymbolicTensor or all children have been visited.\n            stack.pop();\n            sorted.push(top);\n            visited.add(top.name);\n            if (topIsMarked) {\n                marks.pop();\n            }\n        }\n        else {\n            // A non-input SymbolicTensor whose upstream SymbolicTensors haven't\n            // been visited yet. Push them onto the stack.\n            marks.push(stack.length - 1);\n            for (const input of top.inputs) {\n                // Increment the recipient count. Note that this needs to happen\n                // regardless of whether the SymbolicTensor has been visited before.\n                if (recipientMap[input.name] == null) {\n                    recipientMap[input.name] = new Set();\n                }\n                recipientMap[input.name].add(top.name);\n                if (visited.has(input.name)) {\n                    continue; // Avoid repeated visits to the same SymbolicTensor.\n                }\n                stack.push(input);\n            }\n        }\n    }\n    return { sorted, recipientMap };\n}\n/**\n * Get the symbolic output tensors of the node to which a given fetch belongs.\n * @param fetch The fetched symbolic tensor.\n * @returns The Array of symbolic tensors output by the node to which `fetch`\n *   belongs.\n */\nfunction getNodeOutputs(fetch) {\n    let layerOutputs;\n    if (fetch.sourceLayer.inboundNodes.length === 1) {\n        layerOutputs = fetch.sourceLayer.output;\n    }\n    else {\n        let nodeIndex = null;\n        for (let i = 0; i < fetch.sourceLayer.inboundNodes.length; ++i) {\n            for (const outputTensor of fetch.sourceLayer.inboundNodes[i]\n                .outputTensors) {\n                if (outputTensor.id === fetch.id) {\n                    nodeIndex = i;\n                    break;\n                }\n            }\n        }\n        layerOutputs = fetch.sourceLayer.getOutputAt(nodeIndex);\n    }\n    return layerOutputs;\n}\n//# sourceMappingURL=executor.js.map"]},"metadata":{},"sourceType":"module"}