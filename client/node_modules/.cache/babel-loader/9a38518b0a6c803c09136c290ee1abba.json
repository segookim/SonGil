{"ast":null,"code":"/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { env, util } from '@tensorflow/tfjs-core';\nimport { Im2ColPackedProgram } from '../im2col_packed_gpu';\nimport { mapActivationToShaderProgram } from '../kernel_utils/kernel_funcs_utils';\nimport { MatMulPackedProgram } from '../mulmat_packed_gpu';\nimport * as webgl_util from '../webgl_util';\nimport { batchMatMulImpl, MATMUL_SHARED_DIM_THRESHOLD } from './BatchMatMul_impl';\nimport { identity } from './Identity';\nimport { reshape } from './Reshape'; // For 1x1 kernels that iterate through every point in the input, convolution\n// can be expressed as matrix multiplication (without need for memory\n// remapping).\n\nexport function conv2dByMatMul({\n  x,\n  filter,\n  convInfo,\n  backend,\n  bias = null,\n  preluActivationWeights = null,\n  leakyreluAlpha = 0,\n  activation = null\n}) {\n  // Reshapes conv2D input to 2D tensors, uses matMul and then reshape the\n  // result from 2D to 4D.\n  const xShape = x.shape;\n  const xTexData = backend.texData.get(x.dataId);\n  const sharedMatMulDim = convInfo.inChannels;\n  const outerShapeX = xShape[0] * xShape[1] * xShape[2];\n  const outerShapeFilter = convInfo.outChannels;\n  const isChannelsLast = convInfo.dataFormat === 'channelsLast';\n  const transposeA = false;\n  const transposeB = false;\n  let out;\n  const intermediates = []; // TODO: Once reduction ops are packed, batchMatMul will always be packed\n  // and we can remove this condition.\n\n  const batchMatMulWillBeUnpacked = (outerShapeX === 1 || outerShapeFilter === 1) && sharedMatMulDim > MATMUL_SHARED_DIM_THRESHOLD;\n  const reshapeWillBeExpensive = xShape[2] % 2 !== 0 && !!xTexData.isPacked;\n\n  if (batchMatMulWillBeUnpacked || !env().getBool('WEBGL_LAZILY_UNPACK') || !env().getBool('WEBGL_PACK_BINARY_OPERATIONS') || !reshapeWillBeExpensive) {\n    const targetShape = isChannelsLast ? xShape[0] * xShape[1] * xShape[2] : xShape[0] * xShape[2] * xShape[3];\n    const xReshaped = reshape({\n      inputs: {\n        x\n      },\n      backend,\n      attrs: {\n        shape: [1, targetShape, convInfo.inChannels]\n      }\n    });\n    const filterReshaped = reshape({\n      inputs: {\n        x: filter\n      },\n      backend,\n      attrs: {\n        shape: [1, convInfo.inChannels, convInfo.outChannels]\n      }\n    });\n    const result = batchMatMulImpl({\n      a: xReshaped,\n      b: filterReshaped,\n      transposeA,\n      transposeB,\n      backend,\n      bias,\n      activation,\n      preluActivationWeights,\n      leakyreluAlpha\n    });\n    out = reshape({\n      inputs: {\n        x: result\n      },\n      backend,\n      attrs: {\n        shape: convInfo.outShape\n      }\n    });\n    intermediates.push(xReshaped);\n    intermediates.push(filterReshaped);\n    intermediates.push(result);\n  } else {\n    // Following optimization is specific to packed |x| with odd row count\n    // (For example, in channelLast mode, 'row count' refers to x.shape[2]):\n    // we avoid expensive packed 2x2 reshape by padding row count to next,\n    // even number. When x.shape[2] is odd, the result of packed batchMatMul is\n    // the same (has the same texture layout and and values in the texture) as\n    // it is for even x.shape[2] + 1. We make the odd-rows tensor to look like\n    // even-rows tensor before the operation and, after the batchMatMul,\n    // fix the even-rows result to have odd number of rows.\n    const targetShape = isChannelsLast ? xShape[0] * xShape[1] * (xShape[2] + 1) : xShape[0] * xShape[2] * (xShape[3] + 1);\n    const xReshaped = {\n      dataId: x.dataId,\n      shape: [1, targetShape, convInfo.inChannels],\n      dtype: x.dtype\n    }; // xTexData.shape gets referenced from GPGPUBinary.inShapeInfos.\n    // Decrementing row count, after batchMatMul->...->compileProgram leads to\n    // invalid row count within the reference in GPGPUBinary.inShapeInfos.\n    // Alternative fix would be to provide a copy to GPGPUBinary.inShapeInfos\n    // in compileProgram method, but that would affect compilation of all\n    // programs - instead, provide a copy here, with even row count, before\n    // calling batchMatMul->...->compileProgram and after that, the original\n    // xTexData.shape is restored.\n\n    const originalXTexDataShape = xTexData.shape;\n    xTexData.shape = xTexData.shape.slice();\n    xTexData.shape[xTexData.shape.length - 2]++;\n    util.assert(webgl_util.isReshapeFree(xTexData.shape, xReshaped.shape), () => `packed reshape ${xTexData.shape} to ${xReshaped.shape} isn't free`);\n    const filterReshaped = reshape({\n      inputs: {\n        x: filter\n      },\n      backend,\n      attrs: {\n        shape: [1, convInfo.inChannels, convInfo.outChannels]\n      }\n    });\n    intermediates.push(filterReshaped);\n    const pointwiseConv = batchMatMulImpl({\n      a: xReshaped,\n      b: filterReshaped,\n      backend,\n      transposeA,\n      transposeB,\n      bias,\n      activation,\n      preluActivationWeights,\n      leakyreluAlpha\n    });\n    const pointwiseConvTexData = backend.texData.get(pointwiseConv.dataId);\n    util.assert(pointwiseConvTexData.isPacked, () => 'batchMatMul result is expected to be packed'); // Restore the input shape to original.\n\n    xTexData.shape = originalXTexDataShape; // Set the output shape - there is no need for expensive reshape as data\n    // layout is already correct.\n\n    pointwiseConvTexData.shape = convInfo.outShape;\n    out = identity({\n      inputs: {\n        x: pointwiseConv\n      },\n      backend\n    });\n    out.shape = convInfo.outShape;\n    intermediates.push(pointwiseConv);\n  }\n\n  for (const i of intermediates) {\n    backend.disposeIntermediateTensorInfo(i);\n  }\n\n  return out;\n} // Implements the im2row algorithm as outlined in \"High Performance\n// Convolutional Neural Networks for Document Processing\" (Suvisoft, 2006)\n\nexport function conv2dWithIm2Row({\n  x,\n  filter,\n  convInfo,\n  backend,\n  bias = null,\n  preluActivationWeights = null,\n  leakyreluAlpha = 0,\n  activation = null\n}) {\n  // Rearranges conv2d input so each block to be convolved over forms the\n  // column of a new matrix with shape [filterWidth * filterHeight *\n  // inChannels, outHeight * outWidth]. The filter is also rearranged so each\n  // output channel forms a row of a new matrix with shape [outChannels,\n  // filterWidth * filterHeight * inChannels]. The convolution is then\n  // computed by multiplying these matrices and reshaping the result.\n  const {\n    filterWidth,\n    filterHeight,\n    inChannels,\n    outWidth,\n    outHeight,\n    dataFormat\n  } = convInfo;\n  const isChannelsLast = dataFormat === 'channelsLast';\n  const sharedDim = filterWidth * filterHeight * inChannels;\n  const numCols = outHeight * outWidth;\n  const x2ColShape = [sharedDim, numCols];\n  const transposeA = true;\n  const transposeB = false;\n  const intermediates = [];\n  const xSqueezed = reshape({\n    inputs: {\n      x\n    },\n    backend,\n    attrs: {\n      shape: x.shape.slice(1)\n    }\n  });\n  const w2Row = reshape({\n    inputs: {\n      x: filter\n    },\n    backend,\n    attrs: {\n      shape: [1, sharedDim, util.sizeFromShape(filter.shape) / sharedDim]\n    }\n  });\n  intermediates.push(xSqueezed);\n  intermediates.push(w2Row);\n  const im2ColProgram = new Im2ColPackedProgram(x2ColShape, xSqueezed.shape, convInfo);\n  const im2Col = backend.runWebGLProgram(im2ColProgram, [xSqueezed], 'float32');\n  const im2ColReshaped = reshape({\n    inputs: {\n      x: im2Col\n    },\n    backend,\n    attrs: {\n      shape: [1, x2ColShape[0], x2ColShape[1]]\n    }\n  });\n  intermediates.push(im2Col);\n  intermediates.push(im2ColReshaped);\n  const hasBias = bias != null;\n  const hasPreluActivationWeights = preluActivationWeights != null;\n  const hasLeakyreluAlpha = activation === 'leakyrelu';\n  const fusedActivation = activation ? mapActivationToShaderProgram(activation, true) : null;\n  const matmulProgram = new MatMulPackedProgram(im2ColReshaped.shape, w2Row.shape, [1, numCols, convInfo.outChannels], transposeA, transposeB, hasBias, fusedActivation, hasPreluActivationWeights, hasLeakyreluAlpha);\n  const inputs = [im2ColReshaped, w2Row];\n\n  if (bias) {\n    inputs.push(bias);\n  }\n\n  if (hasPreluActivationWeights) {\n    inputs.push(preluActivationWeights);\n  }\n\n  if (hasLeakyreluAlpha) {\n    const $leakyreluAlpha = backend.makeTensorInfo([], 'float32', util.createScalarValue(leakyreluAlpha, 'float32'));\n    inputs.push($leakyreluAlpha);\n    intermediates.push($leakyreluAlpha);\n  }\n\n  const product = backend.runWebGLProgram(matmulProgram, inputs, 'float32');\n  const outShape = isChannelsLast ? [1, outHeight, outWidth, convInfo.outChannels] : [1, convInfo.outChannels, outHeight, outWidth];\n  const out = reshape({\n    inputs: {\n      x: product\n    },\n    backend,\n    attrs: {\n      shape: outShape\n    }\n  });\n  intermediates.push(product);\n\n  for (const i of intermediates) {\n    backend.disposeIntermediateTensorInfo(i);\n  }\n\n  return out;\n}","map":{"version":3,"sources":["../../src/kernels/Conv2D_impl.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;AAeG;AAEH,SAAsB,GAAtB,EAAuC,IAAvC,QAAkD,uBAAlD;AAGA,SAAQ,mBAAR,QAAkC,sBAAlC;AACA,SAAQ,4BAAR,QAA2C,oCAA3C;AACA,SAAQ,mBAAR,QAAkC,sBAAlC;AACA,OAAO,KAAK,UAAZ,MAA4B,eAA5B;AAEA,SAAQ,eAAR,EAAyB,2BAAzB,QAA2D,oBAA3D;AACA,SAAQ,QAAR,QAAuB,YAAvB;AACA,SAAQ,OAAR,QAAsB,WAAtB,C,CAaA;AACA;AACA;;AACA,OAAM,SAAU,cAAV,CAAyB;AAC7B,EAAA,CAD6B;AAE7B,EAAA,MAF6B;AAG7B,EAAA,QAH6B;AAI7B,EAAA,OAJ6B;AAK7B,EAAA,IAAI,GAAG,IALsB;AAM7B,EAAA,sBAAsB,GAAG,IANI;AAO7B,EAAA,cAAc,GAAG,CAPY;AAQ7B,EAAA,UAAU,GAAG;AARgB,CAAzB,EASS;AACb;AACA;AACA,QAAM,MAAM,GAAG,CAAC,CAAC,KAAjB;AACA,QAAM,QAAQ,GAAG,OAAO,CAAC,OAAR,CAAgB,GAAhB,CAAoB,CAAC,CAAC,MAAtB,CAAjB;AACA,QAAM,eAAe,GAAG,QAAQ,CAAC,UAAjC;AACA,QAAM,WAAW,GAAG,MAAM,CAAC,CAAD,CAAN,GAAY,MAAM,CAAC,CAAD,CAAlB,GAAwB,MAAM,CAAC,CAAD,CAAlD;AACA,QAAM,gBAAgB,GAAG,QAAQ,CAAC,WAAlC;AACA,QAAM,cAAc,GAAG,QAAQ,CAAC,UAAT,KAAwB,cAA/C;AACA,QAAM,UAAU,GAAG,KAAnB;AACA,QAAM,UAAU,GAAG,KAAnB;AAEA,MAAI,GAAJ;AACA,QAAM,aAAa,GAAiB,EAApC,CAba,CAeb;AACA;;AACA,QAAM,yBAAyB,GAC3B,CAAC,WAAW,KAAK,CAAhB,IAAqB,gBAAgB,KAAK,CAA3C,KACA,eAAe,GAAG,2BAFtB;AAGA,QAAM,sBAAsB,GAAG,MAAM,CAAC,CAAD,CAAN,GAAY,CAAZ,KAAkB,CAAlB,IAAuB,CAAC,CAAC,QAAQ,CAAC,QAAjE;;AAEA,MAAI,yBAAyB,IAAI,CAAC,GAAG,GAAG,OAAN,CAAc,qBAAd,CAA9B,IACA,CAAC,GAAG,GAAG,OAAN,CAAc,8BAAd,CADD,IAEA,CAAC,sBAFL,EAE6B;AAC3B,UAAM,WAAW,GAAG,cAAc,GAAG,MAAM,CAAC,CAAD,CAAN,GAAY,MAAM,CAAC,CAAD,CAAlB,GAAwB,MAAM,CAAC,CAAD,CAAjC,GACG,MAAM,CAAC,CAAD,CAAN,GAAY,MAAM,CAAC,CAAD,CAAlB,GAAwB,MAAM,CAAC,CAAD,CADnE;AAEA,UAAM,SAAS,GAAG,OAAO,CAAC;AACxB,MAAA,MAAM,EAAE;AAAC,QAAA;AAAD,OADgB;AAExB,MAAA,OAFwB;AAGxB,MAAA,KAAK,EAAE;AAAC,QAAA,KAAK,EAAE,CAAC,CAAD,EAAI,WAAJ,EAAiB,QAAQ,CAAC,UAA1B;AAAR;AAHiB,KAAD,CAAzB;AAKA,UAAM,cAAc,GAAG,OAAO,CAAC;AAC7B,MAAA,MAAM,EAAE;AAAC,QAAA,CAAC,EAAE;AAAJ,OADqB;AAE7B,MAAA,OAF6B;AAG7B,MAAA,KAAK,EAAE;AAAC,QAAA,KAAK,EAAE,CAAC,CAAD,EAAI,QAAQ,CAAC,UAAb,EAAyB,QAAQ,CAAC,WAAlC;AAAR;AAHsB,KAAD,CAA9B;AAKA,UAAM,MAAM,GAAG,eAAe,CAAC;AAC7B,MAAA,CAAC,EAAE,SAD0B;AAE7B,MAAA,CAAC,EAAE,cAF0B;AAG7B,MAAA,UAH6B;AAI7B,MAAA,UAJ6B;AAK7B,MAAA,OAL6B;AAM7B,MAAA,IAN6B;AAO7B,MAAA,UAP6B;AAQ7B,MAAA,sBAR6B;AAS7B,MAAA;AAT6B,KAAD,CAA9B;AAYA,IAAA,GAAG,GAAG,OAAO,CACT;AAAC,MAAA,MAAM,EAAE;AAAC,QAAA,CAAC,EAAE;AAAJ,OAAT;AAAsB,MAAA,OAAtB;AAA+B,MAAA,KAAK,EAAE;AAAC,QAAA,KAAK,EAAE,QAAQ,CAAC;AAAjB;AAAtC,KADS,CAAb;AAGA,IAAA,aAAa,CAAC,IAAd,CAAmB,SAAnB;AACA,IAAA,aAAa,CAAC,IAAd,CAAmB,cAAnB;AACA,IAAA,aAAa,CAAC,IAAd,CAAmB,MAAnB;AACD,GAjCD,MAiCO;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,UAAM,WAAW,GAAG,cAAc,GAC9B,MAAM,CAAC,CAAD,CAAN,GAAY,MAAM,CAAC,CAAD,CAAlB,IAAyB,MAAM,CAAC,CAAD,CAAN,GAAY,CAArC,CAD8B,GAE9B,MAAM,CAAC,CAAD,CAAN,GAAY,MAAM,CAAC,CAAD,CAAlB,IAAyB,MAAM,CAAC,CAAD,CAAN,GAAY,CAArC,CAFJ;AAGA,UAAM,SAAS,GAAe;AAC5B,MAAA,MAAM,EAAE,CAAC,CAAC,MADkB;AAE5B,MAAA,KAAK,EAAE,CAAC,CAAD,EAAI,WAAJ,EAAiB,QAAQ,CAAC,UAA1B,CAFqB;AAG5B,MAAA,KAAK,EAAE,CAAC,CAAC;AAHmB,KAA9B,CAZK,CAiBL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA,UAAM,qBAAqB,GAAG,QAAQ,CAAC,KAAvC;AACA,IAAA,QAAQ,CAAC,KAAT,GAAiB,QAAQ,CAAC,KAAT,CAAe,KAAf,EAAjB;AACA,IAAA,QAAQ,CAAC,KAAT,CAAe,QAAQ,CAAC,KAAT,CAAe,MAAf,GAAwB,CAAvC;AACA,IAAA,IAAI,CAAC,MAAL,CACI,UAAU,CAAC,aAAX,CAAyB,QAAQ,CAAC,KAAlC,EAAyC,SAAS,CAAC,KAAnD,CADJ,EAEI,MAAM,kBAAkB,QAAQ,CAAC,KAAK,OAClC,SAAS,CAAC,KAAK,aAHvB;AAIA,UAAM,cAAc,GAAG,OAAO,CAAC;AAC7B,MAAA,MAAM,EAAE;AAAC,QAAA,CAAC,EAAE;AAAJ,OADqB;AAE7B,MAAA,OAF6B;AAG7B,MAAA,KAAK,EAAE;AAAC,QAAA,KAAK,EAAE,CAAC,CAAD,EAAI,QAAQ,CAAC,UAAb,EAAyB,QAAQ,CAAC,WAAlC;AAAR;AAHsB,KAAD,CAA9B;AAKA,IAAA,aAAa,CAAC,IAAd,CAAmB,cAAnB;AACA,UAAM,aAAa,GAAG,eAAe,CAAC;AACpC,MAAA,CAAC,EAAE,SADiC;AAEpC,MAAA,CAAC,EAAE,cAFiC;AAGpC,MAAA,OAHoC;AAIpC,MAAA,UAJoC;AAKpC,MAAA,UALoC;AAMpC,MAAA,IANoC;AAOpC,MAAA,UAPoC;AAQpC,MAAA,sBARoC;AASpC,MAAA;AAToC,KAAD,CAArC;AAYA,UAAM,oBAAoB,GAAG,OAAO,CAAC,OAAR,CAAgB,GAAhB,CAAoB,aAAa,CAAC,MAAlC,CAA7B;AACA,IAAA,IAAI,CAAC,MAAL,CACI,oBAAoB,CAAC,QADzB,EAEI,MAAM,6CAFV,EAnDK,CAsDL;;AACA,IAAA,QAAQ,CAAC,KAAT,GAAiB,qBAAjB,CAvDK,CAwDL;AACA;;AACA,IAAA,oBAAoB,CAAC,KAArB,GAA6B,QAAQ,CAAC,QAAtC;AAEA,IAAA,GAAG,GAAG,QAAQ,CAAC;AAAC,MAAA,MAAM,EAAE;AAAC,QAAA,CAAC,EAAE;AAAJ,OAAT;AAA6B,MAAA;AAA7B,KAAD,CAAd;AACA,IAAA,GAAG,CAAC,KAAJ,GAAY,QAAQ,CAAC,QAArB;AAEA,IAAA,aAAa,CAAC,IAAd,CAAmB,aAAnB;AACD;;AAED,OAAK,MAAM,CAAX,IAAgB,aAAhB,EAA+B;AAC7B,IAAA,OAAO,CAAC,6BAAR,CAAsC,CAAtC;AACD;;AAED,SAAO,GAAP;AACD,C,CAED;AACA;;AACA,OAAM,SAAU,gBAAV,CAA2B;AAC/B,EAAA,CAD+B;AAE/B,EAAA,MAF+B;AAG/B,EAAA,QAH+B;AAI/B,EAAA,OAJ+B;AAK/B,EAAA,IAAI,GAAG,IALwB;AAM/B,EAAA,sBAAsB,GAAG,IANM;AAO/B,EAAA,cAAc,GAAG,CAPc;AAQ/B,EAAA,UAAU,GAAG;AARkB,CAA3B,EASS;AACb;AACA;AACA;AACA;AACA;AACA;AACA,QAAM;AACJ,IAAA,WADI;AAEJ,IAAA,YAFI;AAGJ,IAAA,UAHI;AAIJ,IAAA,QAJI;AAKJ,IAAA,SALI;AAMJ,IAAA;AANI,MAOF,QAPJ;AASA,QAAM,cAAc,GAAG,UAAU,KAAK,cAAtC;AAEA,QAAM,SAAS,GAAG,WAAW,GAAG,YAAd,GAA6B,UAA/C;AACA,QAAM,OAAO,GAAG,SAAS,GAAG,QAA5B;AACA,QAAM,UAAU,GAAG,CAAC,SAAD,EAAY,OAAZ,CAAnB;AACA,QAAM,UAAU,GAAG,IAAnB;AACA,QAAM,UAAU,GAAG,KAAnB;AAEA,QAAM,aAAa,GAAiB,EAApC;AAEA,QAAM,SAAS,GACX,OAAO,CAAC;AAAC,IAAA,MAAM,EAAE;AAAC,MAAA;AAAD,KAAT;AAAc,IAAA,OAAd;AAAuB,IAAA,KAAK,EAAE;AAAC,MAAA,KAAK,EAAE,CAAC,CAAC,KAAF,CAAQ,KAAR,CAAc,CAAd;AAAR;AAA9B,GAAD,CADX;AAEA,QAAM,KAAK,GAAG,OAAO,CAAC;AACpB,IAAA,MAAM,EAAE;AAAC,MAAA,CAAC,EAAE;AAAJ,KADY;AAEpB,IAAA,OAFoB;AAGpB,IAAA,KAAK,EAAE;AAAC,MAAA,KAAK,EAAE,CAAC,CAAD,EAAI,SAAJ,EAAe,IAAI,CAAC,aAAL,CAAmB,MAAM,CAAC,KAA1B,IAAmC,SAAlD;AAAR;AAHa,GAAD,CAArB;AAMA,EAAA,aAAa,CAAC,IAAd,CAAmB,SAAnB;AACA,EAAA,aAAa,CAAC,IAAd,CAAmB,KAAnB;AAEA,QAAM,aAAa,GACf,IAAI,mBAAJ,CAAwB,UAAxB,EAAoC,SAAS,CAAC,KAA9C,EAAqD,QAArD,CADJ;AAEA,QAAM,MAAM,GAAG,OAAO,CAAC,eAAR,CAAwB,aAAxB,EAAuC,CAAC,SAAD,CAAvC,EAAoD,SAApD,CAAf;AACA,QAAM,cAAc,GAAG,OAAO,CAAC;AAC7B,IAAA,MAAM,EAAE;AAAC,MAAA,CAAC,EAAE;AAAJ,KADqB;AAE7B,IAAA,OAF6B;AAG7B,IAAA,KAAK,EAAE;AAAC,MAAA,KAAK,EAAE,CAAC,CAAD,EAAI,UAAU,CAAC,CAAD,CAAd,EAAmB,UAAU,CAAC,CAAD,CAA7B;AAAR;AAHsB,GAAD,CAA9B;AAMA,EAAA,aAAa,CAAC,IAAd,CAAmB,MAAnB;AACA,EAAA,aAAa,CAAC,IAAd,CAAmB,cAAnB;AAEA,QAAM,OAAO,GAAG,IAAI,IAAI,IAAxB;AACA,QAAM,yBAAyB,GAAG,sBAAsB,IAAI,IAA5D;AACA,QAAM,iBAAiB,GAAG,UAAU,KAAK,WAAzC;AACA,QAAM,eAAe,GACjB,UAAU,GAAG,4BAA4B,CAAC,UAAD,EAAa,IAAb,CAA/B,GAAoD,IADlE;AAEA,QAAM,aAAa,GAAG,IAAI,mBAAJ,CAClB,cAAc,CAAC,KADG,EAElB,KAAK,CAAC,KAFY,EAGlB,CAAC,CAAD,EAAI,OAAJ,EAAa,QAAQ,CAAC,WAAtB,CAHkB,EAGkB,UAHlB,EAG8B,UAH9B,EAG0C,OAH1C,EAIlB,eAJkB,EAID,yBAJC,EAI0B,iBAJ1B,CAAtB;AAKA,QAAM,MAAM,GAAiB,CAAC,cAAD,EAAiB,KAAjB,CAA7B;;AACA,MAAI,IAAJ,EAAU;AACR,IAAA,MAAM,CAAC,IAAP,CAAY,IAAZ;AACD;;AACD,MAAI,yBAAJ,EAA+B;AAC7B,IAAA,MAAM,CAAC,IAAP,CAAY,sBAAZ;AACD;;AACD,MAAI,iBAAJ,EAAuB;AACrB,UAAM,eAAe,GAAG,OAAO,CAAC,cAAR,CACpB,EADoB,EAChB,SADgB,EAEpB,IAAI,CAAC,iBAAL,CAAuB,cAAvB,EAA0D,SAA1D,CAFoB,CAAxB;AAGA,IAAA,MAAM,CAAC,IAAP,CAAY,eAAZ;AACA,IAAA,aAAa,CAAC,IAAd,CAAmB,eAAnB;AACD;;AACD,QAAM,OAAO,GAAG,OAAO,CAAC,eAAR,CAAwB,aAAxB,EAAuC,MAAvC,EAA+C,SAA/C,CAAhB;AAEA,QAAM,QAAQ,GAAG,cAAc,GAC3B,CAAC,CAAD,EAAI,SAAJ,EAAe,QAAf,EAAyB,QAAQ,CAAC,WAAlC,CAD2B,GAE3B,CAAC,CAAD,EAAI,QAAQ,CAAC,WAAb,EAA0B,SAA1B,EAAqC,QAArC,CAFJ;AAGA,QAAM,GAAG,GACL,OAAO,CAAC;AAAC,IAAA,MAAM,EAAE;AAAC,MAAA,CAAC,EAAE;AAAJ,KAAT;AAAuB,IAAA,OAAvB;AAAgC,IAAA,KAAK,EAAE;AAAC,MAAA,KAAK,EAAE;AAAR;AAAvC,GAAD,CADX;AAGA,EAAA,aAAa,CAAC,IAAd,CAAmB,OAAnB;;AACA,OAAK,MAAM,CAAX,IAAgB,aAAhB,EAA+B;AAC7B,IAAA,OAAO,CAAC,6BAAR,CAAsC,CAAtC;AACD;;AAED,SAAO,GAAP;AACD","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { env, util } from '@tensorflow/tfjs-core';\nimport { Im2ColPackedProgram } from '../im2col_packed_gpu';\nimport { mapActivationToShaderProgram } from '../kernel_utils/kernel_funcs_utils';\nimport { MatMulPackedProgram } from '../mulmat_packed_gpu';\nimport * as webgl_util from '../webgl_util';\nimport { batchMatMulImpl, MATMUL_SHARED_DIM_THRESHOLD } from './BatchMatMul_impl';\nimport { identity } from './Identity';\nimport { reshape } from './Reshape';\n// For 1x1 kernels that iterate through every point in the input, convolution\n// can be expressed as matrix multiplication (without need for memory\n// remapping).\nexport function conv2dByMatMul({ x, filter, convInfo, backend, bias = null, preluActivationWeights = null, leakyreluAlpha = 0, activation = null }) {\n    // Reshapes conv2D input to 2D tensors, uses matMul and then reshape the\n    // result from 2D to 4D.\n    const xShape = x.shape;\n    const xTexData = backend.texData.get(x.dataId);\n    const sharedMatMulDim = convInfo.inChannels;\n    const outerShapeX = xShape[0] * xShape[1] * xShape[2];\n    const outerShapeFilter = convInfo.outChannels;\n    const isChannelsLast = convInfo.dataFormat === 'channelsLast';\n    const transposeA = false;\n    const transposeB = false;\n    let out;\n    const intermediates = [];\n    // TODO: Once reduction ops are packed, batchMatMul will always be packed\n    // and we can remove this condition.\n    const batchMatMulWillBeUnpacked = (outerShapeX === 1 || outerShapeFilter === 1) &&\n        sharedMatMulDim > MATMUL_SHARED_DIM_THRESHOLD;\n    const reshapeWillBeExpensive = xShape[2] % 2 !== 0 && !!xTexData.isPacked;\n    if (batchMatMulWillBeUnpacked || !env().getBool('WEBGL_LAZILY_UNPACK') ||\n        !env().getBool('WEBGL_PACK_BINARY_OPERATIONS') ||\n        !reshapeWillBeExpensive) {\n        const targetShape = isChannelsLast ? xShape[0] * xShape[1] * xShape[2] :\n            xShape[0] * xShape[2] * xShape[3];\n        const xReshaped = reshape({\n            inputs: { x },\n            backend,\n            attrs: { shape: [1, targetShape, convInfo.inChannels] }\n        });\n        const filterReshaped = reshape({\n            inputs: { x: filter },\n            backend,\n            attrs: { shape: [1, convInfo.inChannels, convInfo.outChannels] }\n        });\n        const result = batchMatMulImpl({\n            a: xReshaped,\n            b: filterReshaped,\n            transposeA,\n            transposeB,\n            backend,\n            bias,\n            activation,\n            preluActivationWeights,\n            leakyreluAlpha\n        });\n        out = reshape({ inputs: { x: result }, backend, attrs: { shape: convInfo.outShape } });\n        intermediates.push(xReshaped);\n        intermediates.push(filterReshaped);\n        intermediates.push(result);\n    }\n    else {\n        // Following optimization is specific to packed |x| with odd row count\n        // (For example, in channelLast mode, 'row count' refers to x.shape[2]):\n        // we avoid expensive packed 2x2 reshape by padding row count to next,\n        // even number. When x.shape[2] is odd, the result of packed batchMatMul is\n        // the same (has the same texture layout and and values in the texture) as\n        // it is for even x.shape[2] + 1. We make the odd-rows tensor to look like\n        // even-rows tensor before the operation and, after the batchMatMul,\n        // fix the even-rows result to have odd number of rows.\n        const targetShape = isChannelsLast ?\n            xShape[0] * xShape[1] * (xShape[2] + 1) :\n            xShape[0] * xShape[2] * (xShape[3] + 1);\n        const xReshaped = {\n            dataId: x.dataId,\n            shape: [1, targetShape, convInfo.inChannels],\n            dtype: x.dtype\n        };\n        // xTexData.shape gets referenced from GPGPUBinary.inShapeInfos.\n        // Decrementing row count, after batchMatMul->...->compileProgram leads to\n        // invalid row count within the reference in GPGPUBinary.inShapeInfos.\n        // Alternative fix would be to provide a copy to GPGPUBinary.inShapeInfos\n        // in compileProgram method, but that would affect compilation of all\n        // programs - instead, provide a copy here, with even row count, before\n        // calling batchMatMul->...->compileProgram and after that, the original\n        // xTexData.shape is restored.\n        const originalXTexDataShape = xTexData.shape;\n        xTexData.shape = xTexData.shape.slice();\n        xTexData.shape[xTexData.shape.length - 2]++;\n        util.assert(webgl_util.isReshapeFree(xTexData.shape, xReshaped.shape), () => `packed reshape ${xTexData.shape} to ${xReshaped.shape} isn't free`);\n        const filterReshaped = reshape({\n            inputs: { x: filter },\n            backend,\n            attrs: { shape: [1, convInfo.inChannels, convInfo.outChannels] }\n        });\n        intermediates.push(filterReshaped);\n        const pointwiseConv = batchMatMulImpl({\n            a: xReshaped,\n            b: filterReshaped,\n            backend,\n            transposeA,\n            transposeB,\n            bias,\n            activation,\n            preluActivationWeights,\n            leakyreluAlpha\n        });\n        const pointwiseConvTexData = backend.texData.get(pointwiseConv.dataId);\n        util.assert(pointwiseConvTexData.isPacked, () => 'batchMatMul result is expected to be packed');\n        // Restore the input shape to original.\n        xTexData.shape = originalXTexDataShape;\n        // Set the output shape - there is no need for expensive reshape as data\n        // layout is already correct.\n        pointwiseConvTexData.shape = convInfo.outShape;\n        out = identity({ inputs: { x: pointwiseConv }, backend });\n        out.shape = convInfo.outShape;\n        intermediates.push(pointwiseConv);\n    }\n    for (const i of intermediates) {\n        backend.disposeIntermediateTensorInfo(i);\n    }\n    return out;\n}\n// Implements the im2row algorithm as outlined in \"High Performance\n// Convolutional Neural Networks for Document Processing\" (Suvisoft, 2006)\nexport function conv2dWithIm2Row({ x, filter, convInfo, backend, bias = null, preluActivationWeights = null, leakyreluAlpha = 0, activation = null }) {\n    // Rearranges conv2d input so each block to be convolved over forms the\n    // column of a new matrix with shape [filterWidth * filterHeight *\n    // inChannels, outHeight * outWidth]. The filter is also rearranged so each\n    // output channel forms a row of a new matrix with shape [outChannels,\n    // filterWidth * filterHeight * inChannels]. The convolution is then\n    // computed by multiplying these matrices and reshaping the result.\n    const { filterWidth, filterHeight, inChannels, outWidth, outHeight, dataFormat } = convInfo;\n    const isChannelsLast = dataFormat === 'channelsLast';\n    const sharedDim = filterWidth * filterHeight * inChannels;\n    const numCols = outHeight * outWidth;\n    const x2ColShape = [sharedDim, numCols];\n    const transposeA = true;\n    const transposeB = false;\n    const intermediates = [];\n    const xSqueezed = reshape({ inputs: { x }, backend, attrs: { shape: x.shape.slice(1) } });\n    const w2Row = reshape({\n        inputs: { x: filter },\n        backend,\n        attrs: { shape: [1, sharedDim, util.sizeFromShape(filter.shape) / sharedDim] }\n    });\n    intermediates.push(xSqueezed);\n    intermediates.push(w2Row);\n    const im2ColProgram = new Im2ColPackedProgram(x2ColShape, xSqueezed.shape, convInfo);\n    const im2Col = backend.runWebGLProgram(im2ColProgram, [xSqueezed], 'float32');\n    const im2ColReshaped = reshape({\n        inputs: { x: im2Col },\n        backend,\n        attrs: { shape: [1, x2ColShape[0], x2ColShape[1]] }\n    });\n    intermediates.push(im2Col);\n    intermediates.push(im2ColReshaped);\n    const hasBias = bias != null;\n    const hasPreluActivationWeights = preluActivationWeights != null;\n    const hasLeakyreluAlpha = activation === 'leakyrelu';\n    const fusedActivation = activation ? mapActivationToShaderProgram(activation, true) : null;\n    const matmulProgram = new MatMulPackedProgram(im2ColReshaped.shape, w2Row.shape, [1, numCols, convInfo.outChannels], transposeA, transposeB, hasBias, fusedActivation, hasPreluActivationWeights, hasLeakyreluAlpha);\n    const inputs = [im2ColReshaped, w2Row];\n    if (bias) {\n        inputs.push(bias);\n    }\n    if (hasPreluActivationWeights) {\n        inputs.push(preluActivationWeights);\n    }\n    if (hasLeakyreluAlpha) {\n        const $leakyreluAlpha = backend.makeTensorInfo([], 'float32', util.createScalarValue(leakyreluAlpha, 'float32'));\n        inputs.push($leakyreluAlpha);\n        intermediates.push($leakyreluAlpha);\n    }\n    const product = backend.runWebGLProgram(matmulProgram, inputs, 'float32');\n    const outShape = isChannelsLast ?\n        [1, outHeight, outWidth, convInfo.outChannels] :\n        [1, convInfo.outChannels, outHeight, outWidth];\n    const out = reshape({ inputs: { x: product }, backend, attrs: { shape: outShape } });\n    intermediates.push(product);\n    for (const i of intermediates) {\n        backend.disposeIntermediateTensorInfo(i);\n    }\n    return out;\n}\n//# sourceMappingURL=Conv2D_impl.js.map"]},"metadata":{},"sourceType":"module"}