{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n *  Advanced activation layers.\n */\nimport { clipByValue, elu, leakyRelu, prelu, relu, serialization } from '@tensorflow/tfjs-core';\nimport { Softmax as softmaxActivation } from '../activations';\nimport { cast } from '../backend/tfjs_backend';\nimport { getConstraint, serializeConstraint } from '../constraints';\nimport { InputSpec, Layer } from '../engine/topology';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { getInitializer, serializeInitializer } from '../initializers';\nimport { getRegularizer, serializeRegularizer } from '../regularizers';\nimport { getExactlyOneShape, getExactlyOneTensor } from '../utils/types_utils';\nexport class ReLU extends Layer {\n  constructor(args) {\n    super(args == null ? {} : args);\n    this.supportsMasking = true;\n\n    if (args != null) {\n      this.maxValue = args.maxValue;\n    }\n  }\n\n  call(inputs, kwargs) {\n    inputs = getExactlyOneTensor(inputs);\n    let output = relu(inputs);\n\n    if (this.maxValue != null) {\n      output = clipByValue(output, 0, this.maxValue);\n    }\n\n    return output;\n  }\n\n  computeOutputShape(inputShape) {\n    return inputShape;\n  }\n\n  getConfig() {\n    const config = {\n      maxValue: this.maxValue\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n}\n/** @nocollapse */\n\nReLU.className = 'ReLU';\nserialization.registerClass(ReLU);\nexport class LeakyReLU extends Layer {\n  constructor(args) {\n    super(args == null ? {} : args);\n    this.DEFAULT_ALPHA = 0.3;\n\n    if (args == null) {\n      args = {};\n    }\n\n    this.alpha = args.alpha == null ? this.DEFAULT_ALPHA : args.alpha;\n  }\n\n  call(inputs, kwargs) {\n    const x = getExactlyOneTensor(inputs);\n    return leakyRelu(x, this.alpha);\n  }\n\n  computeOutputShape(inputShape) {\n    return inputShape;\n  }\n\n  getConfig() {\n    const config = {\n      alpha: this.alpha\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n}\n/** @nocollapse */\n\nLeakyReLU.className = 'LeakyReLU';\nserialization.registerClass(LeakyReLU);\nexport class PReLU extends Layer {\n  constructor(args) {\n    super(args == null ? {} : args);\n    this.DEFAULT_ALPHA_INITIALIZER = 'zeros';\n\n    if (args == null) {\n      args = {};\n    }\n\n    this.supportsMasking = true;\n    this.alphaInitializer = getInitializer(args.alphaInitializer || this.DEFAULT_ALPHA_INITIALIZER);\n    this.alphaRegularizer = getRegularizer(args.alphaRegularizer);\n    this.alphaConstraint = getConstraint(args.alphaConstraint);\n\n    if (args.sharedAxes == null) {\n      this.sharedAxes = null;\n    } else if (Array.isArray(args.sharedAxes)) {\n      this.sharedAxes = args.sharedAxes;\n    } else if (typeof args.sharedAxes === 'number') {\n      this.sharedAxes = [args.sharedAxes];\n    } else {\n      throw new ValueError(`Expected sharedAxes to be a number or an array of numbers, ` + `but got ${args.sharedAxes}`);\n    }\n  }\n\n  build(inputShape) {\n    inputShape = getExactlyOneShape(inputShape);\n    const paramShape = inputShape.slice(1);\n\n    if (this.sharedAxes != null) {\n      for (const i of this.sharedAxes) {\n        paramShape[i - 1] = 1;\n      }\n    }\n\n    this.alpha = this.addWeight('alpha', paramShape, 'float32', this.alphaInitializer, this.alphaRegularizer, true, this.alphaConstraint); // Set input spec.\n\n    const axes = {};\n\n    if (this.sharedAxes != null) {\n      for (let i = 1; i < inputShape.length; ++i) {\n        axes[i] = inputShape[i];\n      }\n    }\n\n    this.inputSpec = [new InputSpec({\n      ndim: inputShape.length,\n      axes\n    })];\n    this.built = true;\n  }\n\n  call(inputs, kwargs) {\n    inputs = getExactlyOneTensor(inputs);\n    return prelu(inputs, this.alpha.read());\n  }\n\n  getConfig() {\n    const config = {\n      alphaInitializer: serializeInitializer(this.alphaInitializer),\n      alphaRegularizer: serializeRegularizer(this.alphaRegularizer),\n      alphaConstraint: serializeConstraint(this.alphaConstraint),\n      sharedAxes: this.sharedAxes\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n}\n/** @nocollapse */\n\nPReLU.className = 'PReLU';\nserialization.registerClass(PReLU);\nexport class ELU extends Layer {\n  constructor(args) {\n    super(args == null ? {} : args);\n    this.DEFAULT_ALPHA = 1.0;\n\n    if (args == null) {\n      args = {};\n    }\n\n    if (args.alpha != null && args.alpha !== this.DEFAULT_ALPHA) {\n      throw new NotImplementedError(`Non-default alpha value (${args.alpha}) is not supported by the ` + `ELU layer yet.`);\n    }\n\n    this.alpha = args.alpha == null ? this.DEFAULT_ALPHA : args.alpha;\n  }\n\n  call(inputs, kwargs) {\n    const x = getExactlyOneTensor(inputs);\n    return elu(x);\n  }\n\n  computeOutputShape(inputShape) {\n    return inputShape;\n  }\n\n  getConfig() {\n    const config = {\n      alpha: this.alpha\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n}\n/** @nocollapse */\n\nELU.className = 'ELU';\nserialization.registerClass(ELU);\nexport class ThresholdedReLU extends Layer {\n  constructor(args) {\n    super(args == null ? {} : args);\n    this.DEFAULT_THETA = 1.0;\n\n    if (args == null) {\n      args = {};\n    }\n\n    this.theta = args.theta == null ? this.DEFAULT_THETA : args.theta;\n  }\n\n  call(inputs, kwargs) {\n    const x = getExactlyOneTensor(inputs);\n    return x.mul(cast(x.greater(this.theta), 'float32'));\n  }\n\n  computeOutputShape(inputShape) {\n    return inputShape;\n  }\n\n  getConfig() {\n    const config = {\n      theta: this.theta\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n}\n/** @nocollapse */\n\nThresholdedReLU.className = 'ThresholdedReLU';\nserialization.registerClass(ThresholdedReLU);\nexport class Softmax extends Layer {\n  constructor(args) {\n    super(args == null ? {} : args);\n    this.DEFAULT_AXIS = 1.0;\n\n    if (args == null) {\n      args = {};\n    }\n\n    this.softmax = new softmaxActivation().apply;\n    this.axis = args.axis == null ? this.DEFAULT_AXIS : args.axis;\n  }\n\n  call(inputs, kwargs) {\n    const x = getExactlyOneTensor(inputs);\n    return this.softmax(x, this.axis);\n  }\n\n  computeOutputShape(inputShape) {\n    return inputShape;\n  }\n\n  getConfig() {\n    const config = {\n      axis: this.axis\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n}\n/** @nocollapse */\n\nSoftmax.className = 'Softmax';\nserialization.registerClass(Softmax);","map":{"version":3,"sources":["../../src/layers/advanced_activations.ts"],"names":[],"mappings":"AAAA;;;;;;;;AAQG;;AAEH;;AAEG;AAEH,SAAQ,WAAR,EAAqB,GAArB,EAA0B,SAA1B,EAAqC,KAArC,EAA4C,IAA5C,EAAkD,aAAlD,QAA8E,uBAA9E;AAEA,SAAQ,OAAO,IAAI,iBAAnB,QAA2C,gBAA3C;AACA,SAAQ,IAAR,QAAmB,yBAAnB;AACA,SAAoB,aAApB,EAAmC,mBAAnC,QAA6D,gBAA7D;AACA,SAAQ,SAAR,EAAmB,KAAnB,QAA0C,oBAA1C;AACA,SAAQ,mBAAR,EAA6B,UAA7B,QAA8C,WAA9C;AACA,SAAQ,cAAR,EAA4D,oBAA5D,QAAuF,iBAAvF;AAEA,SAAQ,cAAR,EAAqC,oBAArC,QAAgE,iBAAhE;AAEA,SAAQ,kBAAR,EAA4B,mBAA5B,QAAsD,sBAAtD;AAUA,OAAM,MAAO,IAAP,SAAoB,KAApB,CAAyB;AAK7B,EAAA,WAAA,CAAY,IAAZ,EAAgC;AAC9B,UAAM,IAAI,IAAI,IAAR,GAAe,EAAf,GAAoB,IAA1B;AACA,SAAK,eAAL,GAAuB,IAAvB;;AACA,QAAI,IAAI,IAAI,IAAZ,EAAkB;AAChB,WAAK,QAAL,GAAgB,IAAI,CAAC,QAArB;AACD;AACF;;AAED,EAAA,IAAI,CAAC,MAAD,EAA0B,MAA1B,EAAwC;AAC1C,IAAA,MAAM,GAAG,mBAAmB,CAAC,MAAD,CAA5B;AACA,QAAI,MAAM,GAAG,IAAI,CAAC,MAAD,CAAjB;;AACA,QAAI,KAAK,QAAL,IAAiB,IAArB,EAA2B;AACzB,MAAA,MAAM,GAAG,WAAW,CAAC,MAAD,EAAS,CAAT,EAAY,KAAK,QAAjB,CAApB;AACD;;AACD,WAAO,MAAP;AACD;;AAED,EAAA,kBAAkB,CAAC,UAAD,EAA0B;AAC1C,WAAO,UAAP;AACD;;AAED,EAAA,SAAS,GAAA;AACP,UAAM,MAAM,GAA6B;AAAC,MAAA,QAAQ,EAAE,KAAK;AAAhB,KAAzC;AACA,UAAM,UAAU,GAAG,MAAM,SAAN,EAAnB;AACA,IAAA,MAAM,CAAC,MAAP,CAAc,MAAd,EAAsB,UAAtB;AACA,WAAO,MAAP;AACD;;AA/B4B;AAC7B;;AACO,IAAA,CAAA,SAAA,GAAY,MAAZ;AA+BT,aAAa,CAAC,aAAd,CAA4B,IAA5B;AASA,OAAM,MAAO,SAAP,SAAyB,KAAzB,CAA8B;AAOlC,EAAA,WAAA,CAAY,IAAZ,EAAqC;AACnC,UAAM,IAAI,IAAI,IAAR,GAAe,EAAf,GAAoB,IAA1B;AAHO,SAAA,aAAA,GAAgB,GAAhB;;AAIP,QAAI,IAAI,IAAI,IAAZ,EAAkB;AAChB,MAAA,IAAI,GAAG,EAAP;AACD;;AACD,SAAK,KAAL,GAAa,IAAI,CAAC,KAAL,IAAc,IAAd,GAAqB,KAAK,aAA1B,GAA0C,IAAI,CAAC,KAA5D;AACD;;AAED,EAAA,IAAI,CAAC,MAAD,EAA0B,MAA1B,EAAwC;AAC1C,UAAM,CAAC,GAAG,mBAAmB,CAAC,MAAD,CAA7B;AACA,WAAO,SAAS,CAAC,CAAD,EAAI,KAAK,KAAT,CAAhB;AACD;;AAED,EAAA,kBAAkB,CAAC,UAAD,EAA0B;AAC1C,WAAO,UAAP;AACD;;AAED,EAAA,SAAS,GAAA;AACP,UAAM,MAAM,GAA6B;AAAC,MAAA,KAAK,EAAE,KAAK;AAAb,KAAzC;AACA,UAAM,UAAU,GAAG,MAAM,SAAN,EAAnB;AACA,IAAA,MAAM,CAAC,MAAP,CAAc,MAAd,EAAsB,UAAtB;AACA,WAAO,MAAP;AACD;;AA7BiC;AAClC;;AACO,SAAA,CAAA,SAAA,GAAY,WAAZ;AA6BT,aAAa,CAAC,aAAd,CAA4B,SAA5B;AA6BA,OAAM,MAAO,KAAP,SAAqB,KAArB,CAA0B;AAW9B,EAAA,WAAA,CAAY,IAAZ,EAAiC;AAC/B,UAAM,IAAI,IAAI,IAAR,GAAe,EAAf,GAAoB,IAA1B;AAHO,SAAA,yBAAA,GAAmD,OAAnD;;AAIP,QAAI,IAAI,IAAI,IAAZ,EAAkB;AAChB,MAAA,IAAI,GAAG,EAAP;AACD;;AAED,SAAK,eAAL,GAAuB,IAAvB;AACA,SAAK,gBAAL,GACI,cAAc,CAAC,IAAI,CAAC,gBAAL,IAAyB,KAAK,yBAA/B,CADlB;AAEA,SAAK,gBAAL,GAAwB,cAAc,CAAC,IAAI,CAAC,gBAAN,CAAtC;AACA,SAAK,eAAL,GAAuB,aAAa,CAAC,IAAI,CAAC,eAAN,CAApC;;AACA,QAAI,IAAI,CAAC,UAAL,IAAmB,IAAvB,EAA6B;AAC3B,WAAK,UAAL,GAAkB,IAAlB;AACD,KAFD,MAEO,IAAI,KAAK,CAAC,OAAN,CAAc,IAAI,CAAC,UAAnB,CAAJ,EAAoC;AACzC,WAAK,UAAL,GAAkB,IAAI,CAAC,UAAvB;AACD,KAFM,MAEA,IAAI,OAAO,IAAI,CAAC,UAAZ,KAA2B,QAA/B,EAAyC;AAC9C,WAAK,UAAL,GAAkB,CAAC,IAAI,CAAC,UAAN,CAAlB;AACD,KAFM,MAEA;AACL,YAAM,IAAI,UAAJ,CACF,6DAAA,GACA,WAAW,IAAI,CAAC,UAAU,EAFxB,CAAN;AAGD;AACF;;AAED,EAAA,KAAK,CAAC,UAAD,EAA0B;AAC7B,IAAA,UAAU,GAAG,kBAAkB,CAAC,UAAD,CAA/B;AACA,UAAM,UAAU,GAAU,UAAU,CAAC,KAAX,CAAiB,CAAjB,CAA1B;;AACA,QAAI,KAAK,UAAL,IAAmB,IAAvB,EAA6B;AAC3B,WAAK,MAAM,CAAX,IAAgB,KAAK,UAArB,EAAiC;AAC/B,QAAA,UAAU,CAAC,CAAC,GAAG,CAAL,CAAV,GAAoB,CAApB;AACD;AACF;;AACD,SAAK,KAAL,GAAa,KAAK,SAAL,CACT,OADS,EACA,UADA,EACY,SADZ,EACuB,KAAK,gBAD5B,EAET,KAAK,gBAFI,EAEc,IAFd,EAEoB,KAAK,eAFzB,CAAb,CAR6B,CAW7B;;AACA,UAAM,IAAI,GAA6B,EAAvC;;AACA,QAAI,KAAK,UAAL,IAAmB,IAAvB,EAA6B;AAC3B,WAAK,IAAI,CAAC,GAAG,CAAb,EAAgB,CAAC,GAAG,UAAU,CAAC,MAA/B,EAAuC,EAAE,CAAzC,EAA4C;AAC1C,QAAA,IAAI,CAAC,CAAD,CAAJ,GAAU,UAAU,CAAC,CAAD,CAApB;AACD;AACF;;AACD,SAAK,SAAL,GAAiB,CAAC,IAAI,SAAJ,CAAc;AAC9B,MAAA,IAAI,EAAE,UAAU,CAAC,MADa;AAE9B,MAAA;AAF8B,KAAd,CAAD,CAAjB;AAIA,SAAK,KAAL,GAAa,IAAb;AACD;;AAED,EAAA,IAAI,CAAC,MAAD,EAA0B,MAA1B,EAAwC;AAC1C,IAAA,MAAM,GAAG,mBAAmB,CAAC,MAAD,CAA5B;AACA,WAAO,KAAK,CAAC,MAAD,EAAS,KAAK,KAAL,CAAW,IAAX,EAAT,CAAZ;AACD;;AAED,EAAA,SAAS,GAAA;AACP,UAAM,MAAM,GAA6B;AACvC,MAAA,gBAAgB,EAAE,oBAAoB,CAAC,KAAK,gBAAN,CADC;AAEvC,MAAA,gBAAgB,EAAE,oBAAoB,CAAC,KAAK,gBAAN,CAFC;AAGvC,MAAA,eAAe,EAAE,mBAAmB,CAAC,KAAK,eAAN,CAHG;AAIvC,MAAA,UAAU,EAAE,KAAK;AAJsB,KAAzC;AAMA,UAAM,UAAU,GAAG,MAAM,SAAN,EAAnB;AACA,IAAA,MAAM,CAAC,MAAP,CAAc,MAAd,EAAsB,UAAtB;AACA,WAAO,MAAP;AACD;;AA3E6B;AAC9B;;AACO,KAAA,CAAA,SAAA,GAAY,OAAZ;AA2ET,aAAa,CAAC,aAAd,CAA4B,KAA5B;AASA,OAAM,MAAO,GAAP,SAAmB,KAAnB,CAAwB;AAO5B,EAAA,WAAA,CAAY,IAAZ,EAA+B;AAC7B,UAAM,IAAI,IAAI,IAAR,GAAe,EAAf,GAAoB,IAA1B;AAHO,SAAA,aAAA,GAAgB,GAAhB;;AAIP,QAAI,IAAI,IAAI,IAAZ,EAAkB;AAChB,MAAA,IAAI,GAAG,EAAP;AACD;;AAED,QAAI,IAAI,CAAC,KAAL,IAAc,IAAd,IAAsB,IAAI,CAAC,KAAL,KAAe,KAAK,aAA9C,EAA6D;AAC3D,YAAM,IAAI,mBAAJ,CACF,4BAA4B,IAAI,CAAC,KAAK,4BAAtC,GACA,gBAFE,CAAN;AAGD;;AAED,SAAK,KAAL,GAAa,IAAI,CAAC,KAAL,IAAc,IAAd,GAAqB,KAAK,aAA1B,GAA0C,IAAI,CAAC,KAA5D;AACD;;AAED,EAAA,IAAI,CAAC,MAAD,EAA0B,MAA1B,EAAwC;AAC1C,UAAM,CAAC,GAAG,mBAAmB,CAAC,MAAD,CAA7B;AACA,WAAO,GAAG,CAAC,CAAD,CAAV;AACD;;AAED,EAAA,kBAAkB,CAAC,UAAD,EAA0B;AAC1C,WAAO,UAAP;AACD;;AAED,EAAA,SAAS,GAAA;AACP,UAAM,MAAM,GAA6B;AAAC,MAAA,KAAK,EAAE,KAAK;AAAb,KAAzC;AACA,UAAM,UAAU,GAAG,MAAM,SAAN,EAAnB;AACA,IAAA,MAAM,CAAC,MAAP,CAAc,MAAd,EAAsB,UAAtB;AACA,WAAO,MAAP;AACD;;AApC2B;AAC5B;;AACO,GAAA,CAAA,SAAA,GAAY,KAAZ;AAoCT,aAAa,CAAC,aAAd,CAA4B,GAA5B;AASA,OAAM,MAAO,eAAP,SAA+B,KAA/B,CAAoC;AAOxC,EAAA,WAAA,CAAY,IAAZ,EAA2C;AACzC,UAAM,IAAI,IAAI,IAAR,GAAe,EAAf,GAAoB,IAA1B;AAHO,SAAA,aAAA,GAAgB,GAAhB;;AAIP,QAAI,IAAI,IAAI,IAAZ,EAAkB;AAChB,MAAA,IAAI,GAAG,EAAP;AACD;;AAED,SAAK,KAAL,GAAa,IAAI,CAAC,KAAL,IAAc,IAAd,GAAqB,KAAK,aAA1B,GAA0C,IAAI,CAAC,KAA5D;AACD;;AAED,EAAA,IAAI,CAAC,MAAD,EAA0B,MAA1B,EAAwC;AAC1C,UAAM,CAAC,GAAG,mBAAmB,CAAC,MAAD,CAA7B;AACA,WAAO,CAAC,CAAC,GAAF,CAAM,IAAI,CAAC,CAAC,CAAC,OAAF,CAAU,KAAK,KAAf,CAAD,EAAwB,SAAxB,CAAV,CAAP;AACD;;AAED,EAAA,kBAAkB,CAAC,UAAD,EAA0B;AAC1C,WAAO,UAAP;AACD;;AAED,EAAA,SAAS,GAAA;AACP,UAAM,MAAM,GAA6B;AAAC,MAAA,KAAK,EAAE,KAAK;AAAb,KAAzC;AACA,UAAM,UAAU,GAAG,MAAM,SAAN,EAAnB;AACA,IAAA,MAAM,CAAC,MAAP,CAAc,MAAd,EAAsB,UAAtB;AACA,WAAO,MAAP;AACD;;AA9BuC;AACxC;;AACO,eAAA,CAAA,SAAA,GAAY,iBAAZ;AA8BT,aAAa,CAAC,aAAd,CAA4B,eAA5B;AAUA,OAAM,MAAO,OAAP,SAAuB,KAAvB,CAA4B;AAOhC,EAAA,WAAA,CAAY,IAAZ,EAAmC;AACjC,UAAM,IAAI,IAAI,IAAR,GAAe,EAAf,GAAoB,IAA1B;AAHO,SAAA,YAAA,GAAe,GAAf;;AAIP,QAAI,IAAI,IAAI,IAAZ,EAAkB;AAChB,MAAA,IAAI,GAAG,EAAP;AACD;;AACD,SAAK,OAAL,GAAe,IAAI,iBAAJ,GAAwB,KAAvC;AACA,SAAK,IAAL,GAAY,IAAI,CAAC,IAAL,IAAa,IAAb,GAAoB,KAAK,YAAzB,GAAwC,IAAI,CAAC,IAAzD;AACD;;AAED,EAAA,IAAI,CAAC,MAAD,EAA0B,MAA1B,EAAwC;AAC1C,UAAM,CAAC,GAAG,mBAAmB,CAAC,MAAD,CAA7B;AACA,WAAO,KAAK,OAAL,CAAa,CAAb,EAAgB,KAAK,IAArB,CAAP;AACD;;AAED,EAAA,kBAAkB,CAAC,UAAD,EAA0B;AAC1C,WAAO,UAAP;AACD;;AAED,EAAA,SAAS,GAAA;AACP,UAAM,MAAM,GAA6B;AAAC,MAAA,IAAI,EAAE,KAAK;AAAZ,KAAzC;AACA,UAAM,UAAU,GAAG,MAAM,SAAN,EAAnB;AACA,IAAA,MAAM,CAAC,MAAP,CAAc,MAAd,EAAsB,UAAtB;AACA,WAAO,MAAP;AACD;;AA9B+B;AAChC;;AACO,OAAA,CAAA,SAAA,GAAY,SAAZ;AA8BT,aAAa,CAAC,aAAd,CAA4B,OAA5B","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n *  Advanced activation layers.\n */\nimport { clipByValue, elu, leakyRelu, prelu, relu, serialization } from '@tensorflow/tfjs-core';\nimport { Softmax as softmaxActivation } from '../activations';\nimport { cast } from '../backend/tfjs_backend';\nimport { getConstraint, serializeConstraint } from '../constraints';\nimport { InputSpec, Layer } from '../engine/topology';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { getInitializer, serializeInitializer } from '../initializers';\nimport { getRegularizer, serializeRegularizer } from '../regularizers';\nimport { getExactlyOneShape, getExactlyOneTensor } from '../utils/types_utils';\nexport class ReLU extends Layer {\n    constructor(args) {\n        super(args == null ? {} : args);\n        this.supportsMasking = true;\n        if (args != null) {\n            this.maxValue = args.maxValue;\n        }\n    }\n    call(inputs, kwargs) {\n        inputs = getExactlyOneTensor(inputs);\n        let output = relu(inputs);\n        if (this.maxValue != null) {\n            output = clipByValue(output, 0, this.maxValue);\n        }\n        return output;\n    }\n    computeOutputShape(inputShape) {\n        return inputShape;\n    }\n    getConfig() {\n        const config = { maxValue: this.maxValue };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n}\n/** @nocollapse */\nReLU.className = 'ReLU';\nserialization.registerClass(ReLU);\nexport class LeakyReLU extends Layer {\n    constructor(args) {\n        super(args == null ? {} : args);\n        this.DEFAULT_ALPHA = 0.3;\n        if (args == null) {\n            args = {};\n        }\n        this.alpha = args.alpha == null ? this.DEFAULT_ALPHA : args.alpha;\n    }\n    call(inputs, kwargs) {\n        const x = getExactlyOneTensor(inputs);\n        return leakyRelu(x, this.alpha);\n    }\n    computeOutputShape(inputShape) {\n        return inputShape;\n    }\n    getConfig() {\n        const config = { alpha: this.alpha };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n}\n/** @nocollapse */\nLeakyReLU.className = 'LeakyReLU';\nserialization.registerClass(LeakyReLU);\nexport class PReLU extends Layer {\n    constructor(args) {\n        super(args == null ? {} : args);\n        this.DEFAULT_ALPHA_INITIALIZER = 'zeros';\n        if (args == null) {\n            args = {};\n        }\n        this.supportsMasking = true;\n        this.alphaInitializer =\n            getInitializer(args.alphaInitializer || this.DEFAULT_ALPHA_INITIALIZER);\n        this.alphaRegularizer = getRegularizer(args.alphaRegularizer);\n        this.alphaConstraint = getConstraint(args.alphaConstraint);\n        if (args.sharedAxes == null) {\n            this.sharedAxes = null;\n        }\n        else if (Array.isArray(args.sharedAxes)) {\n            this.sharedAxes = args.sharedAxes;\n        }\n        else if (typeof args.sharedAxes === 'number') {\n            this.sharedAxes = [args.sharedAxes];\n        }\n        else {\n            throw new ValueError(`Expected sharedAxes to be a number or an array of numbers, ` +\n                `but got ${args.sharedAxes}`);\n        }\n    }\n    build(inputShape) {\n        inputShape = getExactlyOneShape(inputShape);\n        const paramShape = inputShape.slice(1);\n        if (this.sharedAxes != null) {\n            for (const i of this.sharedAxes) {\n                paramShape[i - 1] = 1;\n            }\n        }\n        this.alpha = this.addWeight('alpha', paramShape, 'float32', this.alphaInitializer, this.alphaRegularizer, true, this.alphaConstraint);\n        // Set input spec.\n        const axes = {};\n        if (this.sharedAxes != null) {\n            for (let i = 1; i < inputShape.length; ++i) {\n                axes[i] = inputShape[i];\n            }\n        }\n        this.inputSpec = [new InputSpec({\n                ndim: inputShape.length,\n                axes,\n            })];\n        this.built = true;\n    }\n    call(inputs, kwargs) {\n        inputs = getExactlyOneTensor(inputs);\n        return prelu(inputs, this.alpha.read());\n    }\n    getConfig() {\n        const config = {\n            alphaInitializer: serializeInitializer(this.alphaInitializer),\n            alphaRegularizer: serializeRegularizer(this.alphaRegularizer),\n            alphaConstraint: serializeConstraint(this.alphaConstraint),\n            sharedAxes: this.sharedAxes\n        };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n}\n/** @nocollapse */\nPReLU.className = 'PReLU';\nserialization.registerClass(PReLU);\nexport class ELU extends Layer {\n    constructor(args) {\n        super(args == null ? {} : args);\n        this.DEFAULT_ALPHA = 1.0;\n        if (args == null) {\n            args = {};\n        }\n        if (args.alpha != null && args.alpha !== this.DEFAULT_ALPHA) {\n            throw new NotImplementedError(`Non-default alpha value (${args.alpha}) is not supported by the ` +\n                `ELU layer yet.`);\n        }\n        this.alpha = args.alpha == null ? this.DEFAULT_ALPHA : args.alpha;\n    }\n    call(inputs, kwargs) {\n        const x = getExactlyOneTensor(inputs);\n        return elu(x);\n    }\n    computeOutputShape(inputShape) {\n        return inputShape;\n    }\n    getConfig() {\n        const config = { alpha: this.alpha };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n}\n/** @nocollapse */\nELU.className = 'ELU';\nserialization.registerClass(ELU);\nexport class ThresholdedReLU extends Layer {\n    constructor(args) {\n        super(args == null ? {} : args);\n        this.DEFAULT_THETA = 1.0;\n        if (args == null) {\n            args = {};\n        }\n        this.theta = args.theta == null ? this.DEFAULT_THETA : args.theta;\n    }\n    call(inputs, kwargs) {\n        const x = getExactlyOneTensor(inputs);\n        return x.mul(cast(x.greater(this.theta), 'float32'));\n    }\n    computeOutputShape(inputShape) {\n        return inputShape;\n    }\n    getConfig() {\n        const config = { theta: this.theta };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n}\n/** @nocollapse */\nThresholdedReLU.className = 'ThresholdedReLU';\nserialization.registerClass(ThresholdedReLU);\nexport class Softmax extends Layer {\n    constructor(args) {\n        super(args == null ? {} : args);\n        this.DEFAULT_AXIS = 1.0;\n        if (args == null) {\n            args = {};\n        }\n        this.softmax = new softmaxActivation().apply;\n        this.axis = args.axis == null ? this.DEFAULT_AXIS : args.axis;\n    }\n    call(inputs, kwargs) {\n        const x = getExactlyOneTensor(inputs);\n        return this.softmax(x, this.axis);\n    }\n    computeOutputShape(inputShape) {\n        return inputShape;\n    }\n    getConfig() {\n        const config = { axis: this.axis };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n}\n/** @nocollapse */\nSoftmax.className = 'Softmax';\nserialization.registerClass(Softmax);\n//# sourceMappingURL=advanced_activations.js.map"]},"metadata":{},"sourceType":"module"}