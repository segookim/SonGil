{"ast":null,"code":"import _regeneratorRuntime from \"/Users/kimkiwoong/SonGil/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/regenerator\";\nimport _asyncToGenerator from \"/Users/kimkiwoong/SonGil/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/asyncToGenerator\";\nimport _classCallCheck from \"/Users/kimkiwoong/SonGil/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/classCallCheck\";\nimport _createClass from \"/Users/kimkiwoong/SonGil/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/createClass\";\nimport _inherits from \"/Users/kimkiwoong/SonGil/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/inherits\";\nimport _createSuper from \"/Users/kimkiwoong/SonGil/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/createSuper\";\n\n/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { dispose as _dispose } from '../globals';\nimport { variableGrads } from '../gradients';\nimport { scalar } from '../ops/ops';\nimport { Serializable } from '../serialization';\n/** @doc {heading: 'Training', subheading: 'Classes', namespace: 'train'} */\n\nexport var Optimizer = /*#__PURE__*/function (_Serializable) {\n  _inherits(Optimizer, _Serializable);\n\n  var _super = _createSuper(Optimizer);\n\n  function Optimizer() {\n    _classCallCheck(this, Optimizer);\n\n    return _super.apply(this, arguments);\n  }\n\n  _createClass(Optimizer, [{\n    key: \"minimize\",\n    value:\n    /**\n     * Executes `f()` and minimizes the scalar output of `f()` by computing\n     * gradients of y with respect to the list of trainable variables provided by\n     * `varList`. If no list is provided, it defaults to all trainable variables.\n     *\n     * @param f The function to execute and whose output to minimize.\n     * @param returnCost Whether to return the scalar cost value produced by\n     * executing `f()`.\n     * @param varList An optional list of variables to update. If specified, only\n     * the trainable variables in varList will be updated by minimize. Defaults to\n     * all trainable variables.\n     *\n     * @doc {heading: 'Training', subheading: 'Optimizers'}\n     */\n    function minimize(f) {\n      var returnCost = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : false;\n      var varList = arguments.length > 2 ? arguments[2] : undefined;\n\n      var _this$computeGradient = this.computeGradients(f, varList),\n          value = _this$computeGradient.value,\n          grads = _this$computeGradient.grads;\n\n      if (varList != null) {\n        var gradArray = varList.map(function (v) {\n          return {\n            name: v.name,\n            tensor: grads[v.name]\n          };\n        });\n        this.applyGradients(gradArray);\n      } else {\n        this.applyGradients(grads);\n      } // Dispose gradients.\n\n\n      _dispose(grads);\n\n      if (returnCost) {\n        return value;\n      } else {\n        value.dispose();\n        return null;\n      }\n    }\n    /**\n     * The number of iterations that this optimizer instance has been invoked for.\n     */\n\n  }, {\n    key: \"iterations\",\n    get: function get() {\n      if (this.iterations_ == null) {\n        this.iterations_ = 0;\n      }\n\n      return this.iterations_;\n    }\n  }, {\n    key: \"incrementIterations\",\n    value: function incrementIterations() {\n      this.iterations_ = this.iterations + 1;\n    }\n    /**\n     * Executes f() and computes the gradient of the scalar output of f() with\n     * respect to the list of trainable variables provided by `varList`. If no\n     * list is provided, it defaults to all trainable variables.\n     *\n     * @param f The function to execute and whose output to use for computing\n     * gradients with respect to variables.\n     * @param varList An optional list of variables to compute gradients with\n     * respect to. If specified, only the trainable variables in varList will have\n     * gradients computed with respect to. Defaults to all trainable variables.\n     *\n     * @doc {heading: 'Training', subheading: 'Optimizers'}\n     */\n\n  }, {\n    key: \"computeGradients\",\n    value: function computeGradients(f, varList) {\n      return variableGrads(f, varList);\n    }\n    /**\n     * Dispose the variables (if any) owned by this optimizer instance.\n     */\n\n  }, {\n    key: \"dispose\",\n    value: function dispose() {\n      if (this.iterations_ != null) {\n        _dispose(this.iterations_);\n      }\n    }\n  }, {\n    key: \"saveIterations\",\n    value: function () {\n      var _saveIterations = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee() {\n        return _regeneratorRuntime.wrap(function _callee$(_context) {\n          while (1) {\n            switch (_context.prev = _context.next) {\n              case 0:\n                if (this.iterations_ == null) {\n                  this.iterations_ = 0;\n                }\n\n                return _context.abrupt(\"return\", {\n                  name: 'iter',\n                  // TODO(cais): Use 'int64' type when available.\n                  tensor: scalar(this.iterations_, 'int32')\n                });\n\n              case 2:\n              case \"end\":\n                return _context.stop();\n            }\n          }\n        }, _callee, this);\n      }));\n\n      function saveIterations() {\n        return _saveIterations.apply(this, arguments);\n      }\n\n      return saveIterations;\n    }()\n  }, {\n    key: \"getWeights\",\n    value: function () {\n      var _getWeights = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee2() {\n        return _regeneratorRuntime.wrap(function _callee2$(_context2) {\n          while (1) {\n            switch (_context2.prev = _context2.next) {\n              case 0:\n                throw new Error('getWeights() is not implemented for this optimizer yet.');\n\n              case 1:\n              case \"end\":\n                return _context2.stop();\n            }\n          }\n        }, _callee2);\n      }));\n\n      function getWeights() {\n        return _getWeights.apply(this, arguments);\n      }\n\n      return getWeights;\n    }()\n  }, {\n    key: \"setWeights\",\n    value: function () {\n      var _setWeights = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee3(weightValues) {\n        return _regeneratorRuntime.wrap(function _callee3$(_context3) {\n          while (1) {\n            switch (_context3.prev = _context3.next) {\n              case 0:\n                throw new Error(\"setWeights() is not implemented for this optimizer class \" + \"\".concat(this.getClassName()));\n\n              case 1:\n              case \"end\":\n                return _context3.stop();\n            }\n          }\n        }, _callee3, this);\n      }));\n\n      function setWeights(_x) {\n        return _setWeights.apply(this, arguments);\n      }\n\n      return setWeights;\n    }()\n    /**\n     * Extract the first element of the weight values and set it\n     * as the iterations counter variable of this instance of optimizer.\n     *\n     * @param weightValues\n     * @returns Weight values with the first element consumed and excluded.\n     */\n\n  }, {\n    key: \"extractIterations\",\n    value: function () {\n      var _extractIterations = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee4(weightValues) {\n        return _regeneratorRuntime.wrap(function _callee4$(_context4) {\n          while (1) {\n            switch (_context4.prev = _context4.next) {\n              case 0:\n                _context4.next = 2;\n                return weightValues[0].tensor.data();\n\n              case 2:\n                this.iterations_ = _context4.sent[0];\n                return _context4.abrupt(\"return\", weightValues.slice(1));\n\n              case 4:\n              case \"end\":\n                return _context4.stop();\n            }\n          }\n        }, _callee4, this);\n      }));\n\n      function extractIterations(_x2) {\n        return _extractIterations.apply(this, arguments);\n      }\n\n      return extractIterations;\n    }()\n  }]);\n\n  return Optimizer;\n}(Serializable);\nObject.defineProperty(Optimizer, Symbol.hasInstance, {\n  value: function value(instance) {\n    return instance.minimize != null && instance.computeGradients != null && instance.applyGradients != null;\n  }\n});","map":{"version":3,"sources":["../../src/optimizers/optimizer.ts"],"names":[],"mappings":";;;;;;;AAAA;;;;;;;;;;;;;;;AAeG;AAEH,SAAQ,OAAO,IAAP,QAAR,QAAsB,YAAtB;AACA,SAAQ,aAAR,QAA4B,cAA5B;AACA,SAAQ,MAAR,QAAqB,YAArB;AACA,SAAQ,YAAR,QAA2B,kBAA3B;AAoBA;;AACA,WAAsB,SAAtB;AAAA;;AAAA;;AAAA;AAAA;;AAAA;AAAA;;AAAA;AAAA;AAAA;AAGE;;;;;;;;;;;;;AAaG;AACH,sBAAS,CAAT,EAAkE;AAAA,UAAxC,UAAwC,uEAA3B,KAA2B;AAAA,UAApB,OAAoB;;AAAA,kCAEzC,KAAK,gBAAL,CAAsB,CAAtB,EAAyB,OAAzB,CAFyC;AAAA,UAEzD,KAFyD,yBAEzD,KAFyD;AAAA,UAElD,KAFkD,yBAElD,KAFkD;;AAIhE,UAAI,OAAO,IAAI,IAAf,EAAqB;AACnB,YAAM,SAAS,GACX,OAAO,CAAC,GAAR,CAAY,UAAA,CAAC;AAAA,iBAAK;AAAC,YAAA,IAAI,EAAE,CAAC,CAAC,IAAT;AAAe,YAAA,MAAM,EAAE,KAAK,CAAC,CAAC,CAAC,IAAH;AAA5B,WAAL;AAAA,SAAb,CADJ;AAEA,aAAK,cAAL,CAAoB,SAApB;AACD,OAJD,MAIO;AACL,aAAK,cAAL,CAAoB,KAApB;AACD,OAV+D,CAYhE;;;AACA,MAAA,QAAO,CAAC,KAAD,CAAP;;AAEA,UAAI,UAAJ,EAAgB;AACd,eAAO,KAAP;AACD,OAFD,MAEO;AACL,QAAA,KAAK,CAAC,OAAN;AACA,eAAO,IAAP;AACD;AACF;AAED;;AAEG;;AA1CL;AAAA;AAAA,SA2CE,eAAc;AACZ,UAAI,KAAK,WAAL,IAAoB,IAAxB,EAA8B;AAC5B,aAAK,WAAL,GAAmB,CAAnB;AACD;;AACD,aAAO,KAAK,WAAZ;AACD;AAhDH;AAAA;AAAA,WAkDY,+BAAmB;AAC3B,WAAK,WAAL,GAAmB,KAAK,UAAL,GAAkB,CAArC;AACD;AAED;;;;;;;;;;;;AAYG;;AAlEL;AAAA;AAAA,WAmEE,0BAAiB,CAAjB,EAAkC,OAAlC,EAAsD;AAEpD,aAAO,aAAa,CAAC,CAAD,EAAI,OAAJ,CAApB;AACD;AAYD;;AAEG;;AApFL;AAAA;AAAA,WAqFE,mBAAO;AACL,UAAI,KAAK,WAAL,IAAoB,IAAxB,EAA8B;AAC5B,QAAA,QAAO,CAAC,KAAK,WAAN,CAAP;AACD;AACF;AAzFH;AAAA;AAAA;AAAA,qFA2FE;AAAA;AAAA;AAAA;AAAA;AACE,oBAAI,KAAK,WAAL,IAAoB,IAAxB,EAA8B;AAC5B,uBAAK,WAAL,GAAmB,CAAnB;AACD;;AAHH,iDAIS;AACL,kBAAA,IAAI,EAAE,MADD;AAEL;AACA,kBAAA,MAAM,EAAE,MAAM,CAAC,KAAK,WAAN,EAAmB,OAAnB;AAHT,iBAJT;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,OA3FF;;AAAA;AAAA;AAAA;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,iFAsGE;AAAA;AAAA;AAAA;AAAA;AAAA,sBACQ,IAAI,KAAJ,CAAU,yDAAV,CADR;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,OAtGF;;AAAA;AAAA;AAAA;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,iFA0GE,kBAAiB,YAAjB;AAAA;AAAA;AAAA;AAAA;AAAA,sBACQ,IAAI,KAAJ,CACF,wEACG,KAAK,YAAL,EADH,CADE,CADR;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,OA1GF;;AAAA;AAAA;AAAA;;AAAA;AAAA;AAgHE;;;;;;AAMG;;AAtHL;AAAA;AAAA;AAAA,wFAuHY,kBAAwB,YAAxB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,uBAEkB,YAAY,CAAC,CAAD,CAAZ,CAAgB,MAAhB,CAAuB,IAAvB,EAFlB;;AAAA;AAER,qBAAK,WAFG,kBAEiD,CAFjD;AAAA,kDAGD,YAAY,CAAC,KAAb,CAAmB,CAAnB,CAHC;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,OAvHZ;;AAAA;AAAA;AAAA;;AAAA;AAAA;AAAA;;AAAA;AAAA,EAAwC,YAAxC;AA8HA,MAAM,CAAC,cAAP,CAAsB,SAAtB,EAAiC,MAAM,CAAC,WAAxC,EAAqD;AACnD,EAAA,KAAK,EAAE,eAAC,QAAD,EAAwB;AAC7B,WAAO,QAAQ,CAAC,QAAT,IAAqB,IAArB,IAA6B,QAAQ,CAAC,gBAAT,IAA6B,IAA1D,IACH,QAAQ,CAAC,cAAT,IAA2B,IAD/B;AAED;AAJkD,CAArD","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { dispose } from '../globals';\nimport { variableGrads } from '../gradients';\nimport { scalar } from '../ops/ops';\nimport { Serializable } from '../serialization';\n/** @doc {heading: 'Training', subheading: 'Classes', namespace: 'train'} */\nexport class Optimizer extends Serializable {\n    /**\n     * Executes `f()` and minimizes the scalar output of `f()` by computing\n     * gradients of y with respect to the list of trainable variables provided by\n     * `varList`. If no list is provided, it defaults to all trainable variables.\n     *\n     * @param f The function to execute and whose output to minimize.\n     * @param returnCost Whether to return the scalar cost value produced by\n     * executing `f()`.\n     * @param varList An optional list of variables to update. If specified, only\n     * the trainable variables in varList will be updated by minimize. Defaults to\n     * all trainable variables.\n     *\n     * @doc {heading: 'Training', subheading: 'Optimizers'}\n     */\n    minimize(f, returnCost = false, varList) {\n        const { value, grads } = this.computeGradients(f, varList);\n        if (varList != null) {\n            const gradArray = varList.map(v => ({ name: v.name, tensor: grads[v.name] }));\n            this.applyGradients(gradArray);\n        }\n        else {\n            this.applyGradients(grads);\n        }\n        // Dispose gradients.\n        dispose(grads);\n        if (returnCost) {\n            return value;\n        }\n        else {\n            value.dispose();\n            return null;\n        }\n    }\n    /**\n     * The number of iterations that this optimizer instance has been invoked for.\n     */\n    get iterations() {\n        if (this.iterations_ == null) {\n            this.iterations_ = 0;\n        }\n        return this.iterations_;\n    }\n    incrementIterations() {\n        this.iterations_ = this.iterations + 1;\n    }\n    /**\n     * Executes f() and computes the gradient of the scalar output of f() with\n     * respect to the list of trainable variables provided by `varList`. If no\n     * list is provided, it defaults to all trainable variables.\n     *\n     * @param f The function to execute and whose output to use for computing\n     * gradients with respect to variables.\n     * @param varList An optional list of variables to compute gradients with\n     * respect to. If specified, only the trainable variables in varList will have\n     * gradients computed with respect to. Defaults to all trainable variables.\n     *\n     * @doc {heading: 'Training', subheading: 'Optimizers'}\n     */\n    computeGradients(f, varList) {\n        return variableGrads(f, varList);\n    }\n    /**\n     * Dispose the variables (if any) owned by this optimizer instance.\n     */\n    dispose() {\n        if (this.iterations_ != null) {\n            dispose(this.iterations_);\n        }\n    }\n    async saveIterations() {\n        if (this.iterations_ == null) {\n            this.iterations_ = 0;\n        }\n        return {\n            name: 'iter',\n            // TODO(cais): Use 'int64' type when available.\n            tensor: scalar(this.iterations_, 'int32')\n        };\n    }\n    async getWeights() {\n        throw new Error('getWeights() is not implemented for this optimizer yet.');\n    }\n    async setWeights(weightValues) {\n        throw new Error(`setWeights() is not implemented for this optimizer class ` +\n            `${this.getClassName()}`);\n    }\n    /**\n     * Extract the first element of the weight values and set it\n     * as the iterations counter variable of this instance of optimizer.\n     *\n     * @param weightValues\n     * @returns Weight values with the first element consumed and excluded.\n     */\n    async extractIterations(weightValues) {\n        this.iterations_ = (await weightValues[0].tensor.data())[0];\n        return weightValues.slice(1);\n    }\n}\nObject.defineProperty(Optimizer, Symbol.hasInstance, {\n    value: (instance) => {\n        return instance.minimize != null && instance.computeGradients != null &&\n            instance.applyGradients != null;\n    }\n});\n//# sourceMappingURL=optimizer.js.map"]},"metadata":{},"sourceType":"module"}