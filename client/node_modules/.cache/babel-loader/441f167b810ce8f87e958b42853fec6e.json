{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/* Original Source: losses.py */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { tidy, util } from '@tensorflow/tfjs-core';\nimport { epsilon } from './backend/common';\nimport * as K from './backend/tfjs_backend';\nimport { ValueError } from './errors';\n/**\n * Normalizes a tensor wrt the L2 norm alongside the specified axis.\n * @param x\n * @param axis Axis along which to perform normalization.\n */\n\nexport function l2Normalize(x, axis) {\n  return tidy(function () {\n    if (x.dtype !== 'float32') {\n      x = x.asType('float32');\n    }\n\n    var squareSum = tfc.sum(K.square(x), axis, true);\n    var epsilonTensor = tfc.fill(squareSum.shape, epsilon());\n    var norm = tfc.sqrt(tfc.maximum(squareSum, epsilonTensor));\n    return tfc.div(x, norm);\n  });\n}\nexport function meanSquaredError(yTrue, yPred) {\n  return tidy(function () {\n    return tfc.mean(K.square(tfc.sub(yPred, yTrue)), -1);\n  });\n}\nexport function meanAbsoluteError(yTrue, yPred) {\n  return tidy(function () {\n    return tfc.mean(tfc.abs(tfc.sub(yPred, yTrue)), -1);\n  });\n}\nexport function meanAbsolutePercentageError(yTrue, yPred) {\n  return tidy(function () {\n    var diff = tfc.sub(yTrue, yPred);\n    var clippedTrue = tfc.clipByValue(tfc.abs(yTrue), epsilon(), Number.MAX_VALUE);\n    var absResult = tfc.abs(tfc.div(diff, clippedTrue));\n    return tfc.mul(100, tfc.mean(absResult, -1));\n  });\n}\nexport function meanSquaredLogarithmicError(yTrue, yPred) {\n  return tidy(function () {\n    var clippedPred = tfc.clipByValue(yPred, epsilon(), Number.MAX_VALUE);\n    var firstLog = tfc.log(tfc.add(1, clippedPred));\n    var clippedTrue = tfc.clipByValue(yTrue, epsilon(), Number.MAX_VALUE);\n    var secondLog = tfc.log(tfc.add(1, clippedTrue));\n    return tfc.mean(K.square(tfc.sub(firstLog, secondLog)), -1);\n  });\n}\nexport function squaredHinge(yTrue, yPred) {\n  return tidy(function () {\n    var maxResult = tfc.maximum(0, tfc.sub(1, tfc.mul(yTrue, yPred)));\n    return tfc.mean(K.square(maxResult), -1);\n  });\n}\nexport function hinge(yTrue, yPred) {\n  return tidy(function () {\n    var maxResult = tfc.maximum(0, tfc.sub(1, tfc.mul(yTrue, yPred)));\n    return tfc.mean(maxResult, -1);\n  });\n}\nexport function categoricalHinge(yTrue, yPred) {\n  return tidy(function () {\n    var pos = tfc.sum(tfc.mul(yTrue, yPred), -1);\n    var neg = tfc.max(tfc.mul(tfc.sub(1, yTrue), yPred), -1);\n    return tfc.maximum(0, tfc.add(1, tfc.sub(neg, pos)));\n  });\n}\n/**\n * Logarithm of the hyperbolic cosine of the prediction error.\n *\n * `log(cosh(x))` is approximately equal to `(x ** 2) / 2` for small `x` and\n * to `abs(x) - log(2)` for large `x`. This means that 'logcosh' works mostly\n * like the mean squared error, but will not be so strongly affected by the\n * occasional wildly incorrect prediction.\n */\n\nexport function logcosh(yTrue, yPred) {\n  return tidy(function () {\n    var log2 = Math.log(2);\n    var predictionDiff = tfc.sub(yPred, yTrue);\n    var logcoshResult = tfc.sub(tfc.add(predictionDiff, tfc.softplus(tfc.mul(-2, predictionDiff))), log2);\n    return tfc.mean(logcoshResult, -1);\n  });\n}\nexport function categoricalCrossentropy(target, output) {\n  var fromLogits = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : false;\n  return tidy(function () {\n    if (fromLogits) {\n      output = tfc.softmax(output);\n    } else {\n      // scale preds so that the class probabilities of each sample sum to 1.\n      var outputSum = tfc.sum(output, output.shape.length - 1, true);\n      output = tfc.div(output, outputSum);\n    }\n\n    output = tfc.clipByValue(output, epsilon(), 1 - epsilon());\n    return tfc.neg(tfc.sum(tfc.mul(target.toFloat(), tfc.log(output)), output.shape.length - 1));\n  });\n}\n/**\n * Categorical crossentropy with integer targets.\n *\n * @param target An integer tensor.\n * @param output A tensor resulting from a softmax (unless `fromLogits` is\n *  `true`, in which case `output` is expected to be the logits).\n * @param fromLogits Boolean, whether `output` is the result of a softmax, or is\n *   a tensor of logits.\n */\n\nexport function sparseCategoricalCrossentropy(target, output) {\n  var fromLogits = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : false;\n  return tidy(function () {\n    var flatTarget = tfc.floor(K.flatten(target)).toInt();\n    output = tfc.clipByValue(output, epsilon(), 1 - epsilon());\n    var outputShape = output.shape;\n    var oneHotTarget = tfc.oneHot(flatTarget, outputShape[outputShape.length - 1]).reshape(outputShape);\n    return categoricalCrossentropy(oneHotTarget, output, fromLogits);\n  });\n}\n/**\n * From TensorFlow's implementation in nn_impl.py:\n *\n * For brevity, let `x = logits`, `z = labels`.  The logistic loss is\n *      z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n *    = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))\n *    = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\n *    = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\n *    = (1 - z) * x + log(1 + exp(-x))\n *    = x - x * z + log(1 + exp(-x))\n * For x < 0, to avoid overflow in exp(-x), we reformulate the above\n *      x - x * z + log(1 + exp(-x))\n *    = log(exp(x)) - x * z + log(1 + exp(-x))\n *    = - x * z + log(1 + exp(x))\n * Hence, to ensure stability and avoid overflow, the implementation uses this\n * equivalent formulation\n *    max(x, 0) - x * z + log(1 + exp(-abs(x)))\n *\n * @param labels The labels.\n * @param logits The logits.\n */\n\nexport function sigmoidCrossEntropyWithLogits(labels, logits) {\n  if (!util.arraysEqual(labels.shape, logits.shape)) {\n    throw new ValueError(\"logits and labels must have the same shape, but got shapes \" + \"\".concat(JSON.stringify(labels.shape), \" and \").concat(JSON.stringify(logits.shape)));\n  }\n\n  return tidy(function () {\n    // The logistic loss formula from above is\n    //   x - x * z + log(1 + exp(-x))\n    // For x < 0, a more numerically stable formula is\n    //   -x * z + log(1 + exp(x))\n    // Note that these two expressions can be combined into the following:\n    //   max(x, 0) - x * z + log(1 + exp(-abs(x)))\n    var reluLogits = logits.relu();\n    var negAbsLogits = logits.abs().neg();\n    return reluLogits.sub(logits.mul(labels)).add(negAbsLogits.exp().log1p());\n  });\n}\nexport function binaryCrossentropy(yTrue, yPred) {\n  return tidy(function () {\n    var y;\n    y = tfc.clipByValue(yPred, epsilon(), 1 - epsilon());\n    y = tfc.log(tfc.div(y, tfc.sub(1, y)));\n    return tfc.mean(sigmoidCrossEntropyWithLogits(yTrue, y), -1);\n  });\n}\nexport function kullbackLeiblerDivergence(yTrue, yPred) {\n  return tidy(function () {\n    var clippedTrue = tfc.clipByValue(yTrue, epsilon(), 1);\n    var clippedPred = tfc.clipByValue(yPred, epsilon(), 1);\n    return tfc.sum(tfc.mul(yTrue, tfc.log(tfc.div(clippedTrue, clippedPred))), -1);\n  });\n}\nexport function poisson(yTrue, yPred) {\n  return tidy(function () {\n    var logPred = tfc.log(tfc.add(epsilon(), yPred));\n    return tfc.mean(tfc.sub(yPred, tfc.mul(yTrue, logPred)), -1);\n  });\n}\nexport function cosineProximity(yTrue, yPred) {\n  return tidy(function () {\n    var trueNormalized = l2Normalize(yTrue, -1);\n    var predNormalized = l2Normalize(yPred, -1);\n    var trueXPred = tfc.mul(trueNormalized, predNormalized);\n    return tfc.neg(tfc.sum(trueXPred, -1));\n  });\n}\nexport var mse = meanSquaredError;\nexport var MSE = meanSquaredError;\nexport var mae = meanAbsoluteError;\nexport var MAE = meanAbsoluteError;\nexport var mape = meanAbsolutePercentageError;\nexport var MAPE = meanAbsolutePercentageError;\nexport var msle = meanSquaredLogarithmicError;\nexport var MSLE = meanSquaredLogarithmicError;\nexport var kld = kullbackLeiblerDivergence;\nexport var KLD = kullbackLeiblerDivergence;\nexport var cosine = cosineProximity; // TODO(michaelterry): Add deserialize() function.\n\nexport var lossesMap = {\n  meanSquaredError: meanSquaredError,\n  meanAbsoluteError: meanAbsoluteError,\n  meanAbsolutePercentageError: meanAbsolutePercentageError,\n  meanSquaredLogarithmicError: meanSquaredLogarithmicError,\n  squaredHinge: squaredHinge,\n  hinge: hinge,\n  categoricalHinge: categoricalHinge,\n  logcosh: logcosh,\n  categoricalCrossentropy: categoricalCrossentropy,\n  sparseCategoricalCrossentropy: sparseCategoricalCrossentropy,\n  binaryCrossentropy: binaryCrossentropy,\n  kullbackLeiblerDivergence: kullbackLeiblerDivergence,\n  poisson: poisson,\n  cosineProximity: cosineProximity\n}; // Porting note: This diverges from the PyKeras implementation and may need to\n// change based on (de)serialization requirements.\n\nexport function get(identifierOrFn) {\n  if (typeof identifierOrFn === 'string') {\n    if (identifierOrFn in lossesMap) {\n      return lossesMap[identifierOrFn];\n    }\n\n    var errMsg = \"Unknown loss \".concat(identifierOrFn);\n\n    if (identifierOrFn.toLowerCase().includes('softmaxcrossentropy')) {\n      errMsg = \"Unknown loss \".concat(identifierOrFn, \". \") + 'Use \"categoricalCrossentropy\" as the string name for ' + 'tf.losses.softmaxCrossEntropy';\n    }\n\n    throw new ValueError(errMsg);\n  } else {\n    return identifierOrFn;\n  }\n}","map":{"version":3,"sources":["../src/losses.ts"],"names":[],"mappings":"AAAA;;;;;;;;AAQG;;AAEH;AACA,OAAO,KAAK,GAAZ,MAAqB,uBAArB;AACA,SAA0B,IAA1B,EAAgC,IAAhC,QAA2C,uBAA3C;AAEA,SAAQ,OAAR,QAAsB,kBAAtB;AACA,OAAO,KAAK,CAAZ,MAAmB,wBAAnB;AACA,SAAQ,UAAR,QAAyB,UAAzB;AAGA;;;;AAIG;;AACH,OAAM,SAAU,WAAV,CAAsB,CAAtB,EAAiC,IAAjC,EAA8C;AAClD,SAAO,IAAI,CAAC,YAAK;AACf,QAAI,CAAC,CAAC,KAAF,KAAY,SAAhB,EAA2B;AACzB,MAAA,CAAC,GAAG,CAAC,CAAC,MAAF,CAAS,SAAT,CAAJ;AACD;;AACD,QAAM,SAAS,GAAG,GAAG,CAAC,GAAJ,CAAQ,CAAC,CAAC,MAAF,CAAS,CAAT,CAAR,EAAqB,IAArB,EAA2B,IAA3B,CAAlB;AACA,QAAM,aAAa,GAAG,GAAG,CAAC,IAAJ,CAAS,SAAS,CAAC,KAAnB,EAA0B,OAAO,EAAjC,CAAtB;AACA,QAAM,IAAI,GAAG,GAAG,CAAC,IAAJ,CAAS,GAAG,CAAC,OAAJ,CAAY,SAAZ,EAAuB,aAAvB,CAAT,CAAb;AACA,WAAO,GAAG,CAAC,GAAJ,CAAQ,CAAR,EAAW,IAAX,CAAP;AACD,GARU,CAAX;AASD;AAED,OAAM,SAAU,gBAAV,CAA2B,KAA3B,EAA0C,KAA1C,EAAuD;AAC3D,SAAO,IAAI,CAAC;AAAA,WAAM,GAAG,CAAC,IAAJ,CAAS,CAAC,CAAC,MAAF,CAAS,GAAG,CAAC,GAAJ,CAAQ,KAAR,EAAe,KAAf,CAAT,CAAT,EAA0C,CAAC,CAA3C,CAAN;AAAA,GAAD,CAAX;AACD;AAED,OAAM,SAAU,iBAAV,CAA4B,KAA5B,EAA2C,KAA3C,EAAwD;AAC5D,SAAO,IAAI,CAAC;AAAA,WAAM,GAAG,CAAC,IAAJ,CAAS,GAAG,CAAC,GAAJ,CAAQ,GAAG,CAAC,GAAJ,CAAQ,KAAR,EAAe,KAAf,CAAR,CAAT,EAAyC,CAAC,CAA1C,CAAN;AAAA,GAAD,CAAX;AACD;AAED,OAAM,SAAU,2BAAV,CACF,KADE,EACa,KADb,EAC0B;AAC9B,SAAO,IAAI,CAAC,YAAK;AACf,QAAM,IAAI,GAAG,GAAG,CAAC,GAAJ,CAAQ,KAAR,EAAe,KAAf,CAAb;AACA,QAAM,WAAW,GACb,GAAG,CAAC,WAAJ,CAAgB,GAAG,CAAC,GAAJ,CAAQ,KAAR,CAAhB,EAAgC,OAAO,EAAvC,EAA2C,MAAM,CAAC,SAAlD,CADJ;AAEA,QAAM,SAAS,GAAG,GAAG,CAAC,GAAJ,CAAQ,GAAG,CAAC,GAAJ,CAAQ,IAAR,EAAc,WAAd,CAAR,CAAlB;AACA,WAAO,GAAG,CAAC,GAAJ,CAAQ,GAAR,EAAa,GAAG,CAAC,IAAJ,CAAS,SAAT,EAAoB,CAAC,CAArB,CAAb,CAAP;AACD,GANU,CAAX;AAOD;AAED,OAAM,SAAU,2BAAV,CACF,KADE,EACa,KADb,EAC0B;AAC9B,SAAO,IAAI,CAAC,YAAK;AACf,QAAM,WAAW,GAAG,GAAG,CAAC,WAAJ,CAAgB,KAAhB,EAAuB,OAAO,EAA9B,EAAkC,MAAM,CAAC,SAAzC,CAApB;AACA,QAAM,QAAQ,GAAG,GAAG,CAAC,GAAJ,CAAQ,GAAG,CAAC,GAAJ,CAAQ,CAAR,EAAW,WAAX,CAAR,CAAjB;AAEA,QAAM,WAAW,GAAG,GAAG,CAAC,WAAJ,CAAgB,KAAhB,EAAuB,OAAO,EAA9B,EAAkC,MAAM,CAAC,SAAzC,CAApB;AACA,QAAM,SAAS,GAAG,GAAG,CAAC,GAAJ,CAAQ,GAAG,CAAC,GAAJ,CAAQ,CAAR,EAAW,WAAX,CAAR,CAAlB;AAEA,WAAO,GAAG,CAAC,IAAJ,CAAS,CAAC,CAAC,MAAF,CAAS,GAAG,CAAC,GAAJ,CAAQ,QAAR,EAAkB,SAAlB,CAAT,CAAT,EAAiD,CAAC,CAAlD,CAAP;AACD,GARU,CAAX;AASD;AAED,OAAM,SAAU,YAAV,CAAuB,KAAvB,EAAsC,KAAtC,EAAmD;AACvD,SAAO,IAAI,CAAC,YAAK;AACf,QAAM,SAAS,GAAG,GAAG,CAAC,OAAJ,CAAY,CAAZ,EAAe,GAAG,CAAC,GAAJ,CAAQ,CAAR,EAAW,GAAG,CAAC,GAAJ,CAAQ,KAAR,EAAe,KAAf,CAAX,CAAf,CAAlB;AACA,WAAO,GAAG,CAAC,IAAJ,CAAS,CAAC,CAAC,MAAF,CAAS,SAAT,CAAT,EAA8B,CAAC,CAA/B,CAAP;AACD,GAHU,CAAX;AAID;AAED,OAAM,SAAU,KAAV,CAAgB,KAAhB,EAA+B,KAA/B,EAA4C;AAChD,SAAO,IAAI,CAAC,YAAK;AACf,QAAM,SAAS,GAAG,GAAG,CAAC,OAAJ,CAAY,CAAZ,EAAe,GAAG,CAAC,GAAJ,CAAQ,CAAR,EAAW,GAAG,CAAC,GAAJ,CAAQ,KAAR,EAAe,KAAf,CAAX,CAAf,CAAlB;AACA,WAAO,GAAG,CAAC,IAAJ,CAAS,SAAT,EAAoB,CAAC,CAArB,CAAP;AACD,GAHU,CAAX;AAID;AAED,OAAM,SAAU,gBAAV,CAA2B,KAA3B,EAA0C,KAA1C,EAAuD;AAC3D,SAAO,IAAI,CAAC,YAAK;AACf,QAAM,GAAG,GAAG,GAAG,CAAC,GAAJ,CAAQ,GAAG,CAAC,GAAJ,CAAQ,KAAR,EAAe,KAAf,CAAR,EAA+B,CAAC,CAAhC,CAAZ;AACA,QAAM,GAAG,GAAG,GAAG,CAAC,GAAJ,CAAQ,GAAG,CAAC,GAAJ,CAAQ,GAAG,CAAC,GAAJ,CAAQ,CAAR,EAAW,KAAX,CAAR,EAA2B,KAA3B,CAAR,EAA2C,CAAC,CAA5C,CAAZ;AACA,WAAO,GAAG,CAAC,OAAJ,CAAY,CAAZ,EAAe,GAAG,CAAC,GAAJ,CAAQ,CAAR,EAAW,GAAG,CAAC,GAAJ,CAAQ,GAAR,EAAa,GAAb,CAAX,CAAf,CAAP;AACD,GAJU,CAAX;AAKD;AAED;;;;;;;AAOG;;AACH,OAAM,SAAU,OAAV,CAAkB,KAAlB,EAAiC,KAAjC,EAA8C;AAClD,SAAO,IAAI,CAAC,YAAK;AACf,QAAM,IAAI,GAAG,IAAI,CAAC,GAAL,CAAS,CAAT,CAAb;AACA,QAAM,cAAc,GAAG,GAAG,CAAC,GAAJ,CAAQ,KAAR,EAAe,KAAf,CAAvB;AACA,QAAM,aAAa,GAAG,GAAG,CAAC,GAAJ,CAClB,GAAG,CAAC,GAAJ,CAAQ,cAAR,EAAwB,GAAG,CAAC,QAAJ,CAAa,GAAG,CAAC,GAAJ,CAAQ,CAAC,CAAT,EAAY,cAAZ,CAAb,CAAxB,CADkB,EAElB,IAFkB,CAAtB;AAGA,WAAO,GAAG,CAAC,IAAJ,CAAS,aAAT,EAAwB,CAAC,CAAzB,CAAP;AACD,GAPU,CAAX;AAQD;AAED,OAAM,SAAU,uBAAV,CACF,MADE,EACc,MADd,EACgD;AAAA,MAAlB,UAAkB,uEAAL,KAAK;AACpD,SAAO,IAAI,CAAC,YAAK;AACf,QAAI,UAAJ,EAAgB;AACd,MAAA,MAAM,GAAG,GAAG,CAAC,OAAJ,CAAY,MAAZ,CAAT;AACD,KAFD,MAEO;AACL;AACA,UAAM,SAAS,GAAG,GAAG,CAAC,GAAJ,CAAQ,MAAR,EAAgB,MAAM,CAAC,KAAP,CAAa,MAAb,GAAsB,CAAtC,EAAyC,IAAzC,CAAlB;AACA,MAAA,MAAM,GAAG,GAAG,CAAC,GAAJ,CAAQ,MAAR,EAAgB,SAAhB,CAAT;AACD;;AACD,IAAA,MAAM,GAAG,GAAG,CAAC,WAAJ,CAAgB,MAAhB,EAAwB,OAAO,EAA/B,EAAmC,IAAI,OAAO,EAA9C,CAAT;AACA,WAAO,GAAG,CAAC,GAAJ,CAAQ,GAAG,CAAC,GAAJ,CACX,GAAG,CAAC,GAAJ,CAAQ,MAAM,CAAC,OAAP,EAAR,EAA0B,GAAG,CAAC,GAAJ,CAAQ,MAAR,CAA1B,CADW,EACiC,MAAM,CAAC,KAAP,CAAa,MAAb,GAAsB,CADvD,CAAR,CAAP;AAED,GAXU,CAAX;AAYD;AAED;;;;;;;;AAQG;;AACH,OAAM,SAAU,6BAAV,CACF,MADE,EACc,MADd,EACgD;AAAA,MAAlB,UAAkB,uEAAL,KAAK;AACpD,SAAO,IAAI,CAAC,YAAK;AACf,QAAM,UAAU,GAAG,GAAG,CAAC,KAAJ,CAAU,CAAC,CAAC,OAAF,CAAU,MAAV,CAAV,EAA6B,KAA7B,EAAnB;AACA,IAAA,MAAM,GAAG,GAAG,CAAC,WAAJ,CAAgB,MAAhB,EAAwB,OAAO,EAA/B,EAAmC,IAAI,OAAO,EAA9C,CAAT;AACA,QAAM,WAAW,GAAG,MAAM,CAAC,KAA3B;AACA,QAAM,YAAY,GACd,GAAG,CAAC,MAAJ,CAAW,UAAX,EAAuB,WAAW,CAAC,WAAW,CAAC,MAAZ,GAAqB,CAAtB,CAAlC,EACK,OADL,CACa,WADb,CADJ;AAGA,WAAO,uBAAuB,CAAC,YAAD,EAAe,MAAf,EAAuB,UAAvB,CAA9B;AACD,GARU,CAAX;AASD;AAED;;;;;;;;;;;;;;;;;;;;AAoBG;;AACH,OAAM,SAAU,6BAAV,CACF,MADE,EACc,MADd,EAC4B;AAChC,MAAI,CAAC,IAAI,CAAC,WAAL,CAAiB,MAAM,CAAC,KAAxB,EAA+B,MAAM,CAAC,KAAtC,CAAL,EAAmD;AACjD,UAAM,IAAI,UAAJ,CACF,0EACG,IAAI,CAAC,SAAL,CAAe,MAAM,CAAC,KAAtB,CADH,kBACuC,IAAI,CAAC,SAAL,CAAe,MAAM,CAAC,KAAtB,CADvC,CADE,CAAN;AAGD;;AACD,SAAO,IAAI,CAAC,YAAK;AACf;AACA;AACA;AACA;AACA;AACA;AACA,QAAM,UAAU,GAAG,MAAM,CAAC,IAAP,EAAnB;AACA,QAAM,YAAY,GAAG,MAAM,CAAC,GAAP,GAAa,GAAb,EAArB;AACA,WAAO,UAAU,CAAC,GAAX,CAAe,MAAM,CAAC,GAAP,CAAW,MAAX,CAAf,EAAmC,GAAnC,CAAuC,YAAY,CAAC,GAAb,GAAmB,KAAnB,EAAvC,CAAP;AACD,GAVU,CAAX;AAWD;AAED,OAAM,SAAU,kBAAV,CAA6B,KAA7B,EAA4C,KAA5C,EAAyD;AAC7D,SAAO,IAAI,CAAC,YAAK;AACf,QAAI,CAAJ;AACA,IAAA,CAAC,GAAG,GAAG,CAAC,WAAJ,CAAgB,KAAhB,EAAuB,OAAO,EAA9B,EAAkC,IAAI,OAAO,EAA7C,CAAJ;AACA,IAAA,CAAC,GAAG,GAAG,CAAC,GAAJ,CAAQ,GAAG,CAAC,GAAJ,CAAQ,CAAR,EAAW,GAAG,CAAC,GAAJ,CAAQ,CAAR,EAAW,CAAX,CAAX,CAAR,CAAJ;AACA,WAAO,GAAG,CAAC,IAAJ,CAAS,6BAA6B,CAAC,KAAD,EAAQ,CAAR,CAAtC,EAAkD,CAAC,CAAnD,CAAP;AACD,GALU,CAAX;AAMD;AAED,OAAM,SAAU,yBAAV,CACF,KADE,EACa,KADb,EAC0B;AAC9B,SAAO,IAAI,CAAC,YAAK;AACf,QAAM,WAAW,GAAG,GAAG,CAAC,WAAJ,CAAgB,KAAhB,EAAuB,OAAO,EAA9B,EAAkC,CAAlC,CAApB;AACA,QAAM,WAAW,GAAG,GAAG,CAAC,WAAJ,CAAgB,KAAhB,EAAuB,OAAO,EAA9B,EAAkC,CAAlC,CAApB;AACA,WAAO,GAAG,CAAC,GAAJ,CACH,GAAG,CAAC,GAAJ,CAAQ,KAAR,EAAe,GAAG,CAAC,GAAJ,CAAQ,GAAG,CAAC,GAAJ,CAAQ,WAAR,EAAqB,WAArB,CAAR,CAAf,CADG,EACyD,CAAC,CAD1D,CAAP;AAED,GALU,CAAX;AAMD;AAED,OAAM,SAAU,OAAV,CAAkB,KAAlB,EAAiC,KAAjC,EAA8C;AAClD,SAAO,IAAI,CAAC,YAAK;AACf,QAAM,OAAO,GAAG,GAAG,CAAC,GAAJ,CAAQ,GAAG,CAAC,GAAJ,CAAQ,OAAO,EAAf,EAAmB,KAAnB,CAAR,CAAhB;AACA,WAAO,GAAG,CAAC,IAAJ,CAAS,GAAG,CAAC,GAAJ,CAAQ,KAAR,EAAe,GAAG,CAAC,GAAJ,CAAQ,KAAR,EAAe,OAAf,CAAf,CAAT,EAAkD,CAAC,CAAnD,CAAP;AACD,GAHU,CAAX;AAID;AAED,OAAM,SAAU,eAAV,CAA0B,KAA1B,EAAyC,KAAzC,EAAsD;AAC1D,SAAO,IAAI,CAAC,YAAK;AACf,QAAM,cAAc,GAAG,WAAW,CAAC,KAAD,EAAQ,CAAC,CAAT,CAAlC;AACA,QAAM,cAAc,GAAG,WAAW,CAAC,KAAD,EAAQ,CAAC,CAAT,CAAlC;AACA,QAAM,SAAS,GAAG,GAAG,CAAC,GAAJ,CAAQ,cAAR,EAAwB,cAAxB,CAAlB;AACA,WAAO,GAAG,CAAC,GAAJ,CAAQ,GAAG,CAAC,GAAJ,CAAQ,SAAR,EAAmB,CAAC,CAApB,CAAR,CAAP;AACD,GALU,CAAX;AAMD;AAED,OAAO,IAAM,GAAG,GAAG,gBAAZ;AACP,OAAO,IAAM,GAAG,GAAG,gBAAZ;AACP,OAAO,IAAM,GAAG,GAAG,iBAAZ;AACP,OAAO,IAAM,GAAG,GAAG,iBAAZ;AACP,OAAO,IAAM,IAAI,GAAG,2BAAb;AACP,OAAO,IAAM,IAAI,GAAG,2BAAb;AACP,OAAO,IAAM,IAAI,GAAG,2BAAb;AACP,OAAO,IAAM,IAAI,GAAG,2BAAb;AACP,OAAO,IAAM,GAAG,GAAG,yBAAZ;AACP,OAAO,IAAM,GAAG,GAAG,yBAAZ;AACP,OAAO,IAAM,MAAM,GAAG,eAAf,C,CAEP;;AAEA,OAAO,IAAM,SAAS,GAA6C;AACjE,EAAA,gBAAgB,EAAhB,gBADiE;AAEjE,EAAA,iBAAiB,EAAjB,iBAFiE;AAGjE,EAAA,2BAA2B,EAA3B,2BAHiE;AAIjE,EAAA,2BAA2B,EAA3B,2BAJiE;AAKjE,EAAA,YAAY,EAAZ,YALiE;AAMjE,EAAA,KAAK,EAAL,KANiE;AAOjE,EAAA,gBAAgB,EAAhB,gBAPiE;AAQjE,EAAA,OAAO,EAAP,OARiE;AASjE,EAAA,uBAAuB,EAAvB,uBATiE;AAUjE,EAAA,6BAA6B,EAA7B,6BAViE;AAWjE,EAAA,kBAAkB,EAAlB,kBAXiE;AAYjE,EAAA,yBAAyB,EAAzB,yBAZiE;AAajE,EAAA,OAAO,EAAP,OAbiE;AAcjE,EAAA,eAAe,EAAf;AAdiE,CAA5D,C,CAiBP;AACA;;AACA,OAAM,SAAU,GAAV,CAAc,cAAd,EAAmD;AACvD,MAAI,OAAO,cAAP,KAA0B,QAA9B,EAAwC;AACtC,QAAI,cAAc,IAAI,SAAtB,EAAiC;AAC/B,aAAO,SAAS,CAAC,cAAD,CAAhB;AACD;;AACD,QAAI,MAAM,0BAAmB,cAAnB,CAAV;;AACA,QAAI,cAAc,CAAC,WAAf,GAA6B,QAA7B,CAAsC,qBAAtC,CAAJ,EAAkE;AAChE,MAAA,MAAM,GAAG,uBAAgB,cAAhB,UACL,uDADK,GAEL,+BAFJ;AAGD;;AACD,UAAM,IAAI,UAAJ,CAAe,MAAf,CAAN;AACD,GAXD,MAWO;AACL,WAAO,cAAP;AACD;AACF","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/* Original Source: losses.py */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { tidy, util } from '@tensorflow/tfjs-core';\nimport { epsilon } from './backend/common';\nimport * as K from './backend/tfjs_backend';\nimport { ValueError } from './errors';\n/**\n * Normalizes a tensor wrt the L2 norm alongside the specified axis.\n * @param x\n * @param axis Axis along which to perform normalization.\n */\nexport function l2Normalize(x, axis) {\n    return tidy(() => {\n        if (x.dtype !== 'float32') {\n            x = x.asType('float32');\n        }\n        const squareSum = tfc.sum(K.square(x), axis, true);\n        const epsilonTensor = tfc.fill(squareSum.shape, epsilon());\n        const norm = tfc.sqrt(tfc.maximum(squareSum, epsilonTensor));\n        return tfc.div(x, norm);\n    });\n}\nexport function meanSquaredError(yTrue, yPred) {\n    return tidy(() => tfc.mean(K.square(tfc.sub(yPred, yTrue)), -1));\n}\nexport function meanAbsoluteError(yTrue, yPred) {\n    return tidy(() => tfc.mean(tfc.abs(tfc.sub(yPred, yTrue)), -1));\n}\nexport function meanAbsolutePercentageError(yTrue, yPred) {\n    return tidy(() => {\n        const diff = tfc.sub(yTrue, yPred);\n        const clippedTrue = tfc.clipByValue(tfc.abs(yTrue), epsilon(), Number.MAX_VALUE);\n        const absResult = tfc.abs(tfc.div(diff, clippedTrue));\n        return tfc.mul(100, tfc.mean(absResult, -1));\n    });\n}\nexport function meanSquaredLogarithmicError(yTrue, yPred) {\n    return tidy(() => {\n        const clippedPred = tfc.clipByValue(yPred, epsilon(), Number.MAX_VALUE);\n        const firstLog = tfc.log(tfc.add(1, clippedPred));\n        const clippedTrue = tfc.clipByValue(yTrue, epsilon(), Number.MAX_VALUE);\n        const secondLog = tfc.log(tfc.add(1, clippedTrue));\n        return tfc.mean(K.square(tfc.sub(firstLog, secondLog)), -1);\n    });\n}\nexport function squaredHinge(yTrue, yPred) {\n    return tidy(() => {\n        const maxResult = tfc.maximum(0, tfc.sub(1, tfc.mul(yTrue, yPred)));\n        return tfc.mean(K.square(maxResult), -1);\n    });\n}\nexport function hinge(yTrue, yPred) {\n    return tidy(() => {\n        const maxResult = tfc.maximum(0, tfc.sub(1, tfc.mul(yTrue, yPred)));\n        return tfc.mean(maxResult, -1);\n    });\n}\nexport function categoricalHinge(yTrue, yPred) {\n    return tidy(() => {\n        const pos = tfc.sum(tfc.mul(yTrue, yPred), -1);\n        const neg = tfc.max(tfc.mul(tfc.sub(1, yTrue), yPred), -1);\n        return tfc.maximum(0, tfc.add(1, tfc.sub(neg, pos)));\n    });\n}\n/**\n * Logarithm of the hyperbolic cosine of the prediction error.\n *\n * `log(cosh(x))` is approximately equal to `(x ** 2) / 2` for small `x` and\n * to `abs(x) - log(2)` for large `x`. This means that 'logcosh' works mostly\n * like the mean squared error, but will not be so strongly affected by the\n * occasional wildly incorrect prediction.\n */\nexport function logcosh(yTrue, yPred) {\n    return tidy(() => {\n        const log2 = Math.log(2);\n        const predictionDiff = tfc.sub(yPred, yTrue);\n        const logcoshResult = tfc.sub(tfc.add(predictionDiff, tfc.softplus(tfc.mul(-2, predictionDiff))), log2);\n        return tfc.mean(logcoshResult, -1);\n    });\n}\nexport function categoricalCrossentropy(target, output, fromLogits = false) {\n    return tidy(() => {\n        if (fromLogits) {\n            output = tfc.softmax(output);\n        }\n        else {\n            // scale preds so that the class probabilities of each sample sum to 1.\n            const outputSum = tfc.sum(output, output.shape.length - 1, true);\n            output = tfc.div(output, outputSum);\n        }\n        output = tfc.clipByValue(output, epsilon(), 1 - epsilon());\n        return tfc.neg(tfc.sum(tfc.mul(target.toFloat(), tfc.log(output)), output.shape.length - 1));\n    });\n}\n/**\n * Categorical crossentropy with integer targets.\n *\n * @param target An integer tensor.\n * @param output A tensor resulting from a softmax (unless `fromLogits` is\n *  `true`, in which case `output` is expected to be the logits).\n * @param fromLogits Boolean, whether `output` is the result of a softmax, or is\n *   a tensor of logits.\n */\nexport function sparseCategoricalCrossentropy(target, output, fromLogits = false) {\n    return tidy(() => {\n        const flatTarget = tfc.floor(K.flatten(target)).toInt();\n        output = tfc.clipByValue(output, epsilon(), 1 - epsilon());\n        const outputShape = output.shape;\n        const oneHotTarget = tfc.oneHot(flatTarget, outputShape[outputShape.length - 1])\n            .reshape(outputShape);\n        return categoricalCrossentropy(oneHotTarget, output, fromLogits);\n    });\n}\n/**\n * From TensorFlow's implementation in nn_impl.py:\n *\n * For brevity, let `x = logits`, `z = labels`.  The logistic loss is\n *      z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n *    = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))\n *    = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\n *    = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\n *    = (1 - z) * x + log(1 + exp(-x))\n *    = x - x * z + log(1 + exp(-x))\n * For x < 0, to avoid overflow in exp(-x), we reformulate the above\n *      x - x * z + log(1 + exp(-x))\n *    = log(exp(x)) - x * z + log(1 + exp(-x))\n *    = - x * z + log(1 + exp(x))\n * Hence, to ensure stability and avoid overflow, the implementation uses this\n * equivalent formulation\n *    max(x, 0) - x * z + log(1 + exp(-abs(x)))\n *\n * @param labels The labels.\n * @param logits The logits.\n */\nexport function sigmoidCrossEntropyWithLogits(labels, logits) {\n    if (!util.arraysEqual(labels.shape, logits.shape)) {\n        throw new ValueError(`logits and labels must have the same shape, but got shapes ` +\n            `${JSON.stringify(labels.shape)} and ${JSON.stringify(logits.shape)}`);\n    }\n    return tidy(() => {\n        // The logistic loss formula from above is\n        //   x - x * z + log(1 + exp(-x))\n        // For x < 0, a more numerically stable formula is\n        //   -x * z + log(1 + exp(x))\n        // Note that these two expressions can be combined into the following:\n        //   max(x, 0) - x * z + log(1 + exp(-abs(x)))\n        const reluLogits = logits.relu();\n        const negAbsLogits = logits.abs().neg();\n        return reluLogits.sub(logits.mul(labels)).add(negAbsLogits.exp().log1p());\n    });\n}\nexport function binaryCrossentropy(yTrue, yPred) {\n    return tidy(() => {\n        let y;\n        y = tfc.clipByValue(yPred, epsilon(), 1 - epsilon());\n        y = tfc.log(tfc.div(y, tfc.sub(1, y)));\n        return tfc.mean(sigmoidCrossEntropyWithLogits(yTrue, y), -1);\n    });\n}\nexport function kullbackLeiblerDivergence(yTrue, yPred) {\n    return tidy(() => {\n        const clippedTrue = tfc.clipByValue(yTrue, epsilon(), 1);\n        const clippedPred = tfc.clipByValue(yPred, epsilon(), 1);\n        return tfc.sum(tfc.mul(yTrue, tfc.log(tfc.div(clippedTrue, clippedPred))), -1);\n    });\n}\nexport function poisson(yTrue, yPred) {\n    return tidy(() => {\n        const logPred = tfc.log(tfc.add(epsilon(), yPred));\n        return tfc.mean(tfc.sub(yPred, tfc.mul(yTrue, logPred)), -1);\n    });\n}\nexport function cosineProximity(yTrue, yPred) {\n    return tidy(() => {\n        const trueNormalized = l2Normalize(yTrue, -1);\n        const predNormalized = l2Normalize(yPred, -1);\n        const trueXPred = tfc.mul(trueNormalized, predNormalized);\n        return tfc.neg(tfc.sum(trueXPred, -1));\n    });\n}\nexport const mse = meanSquaredError;\nexport const MSE = meanSquaredError;\nexport const mae = meanAbsoluteError;\nexport const MAE = meanAbsoluteError;\nexport const mape = meanAbsolutePercentageError;\nexport const MAPE = meanAbsolutePercentageError;\nexport const msle = meanSquaredLogarithmicError;\nexport const MSLE = meanSquaredLogarithmicError;\nexport const kld = kullbackLeiblerDivergence;\nexport const KLD = kullbackLeiblerDivergence;\nexport const cosine = cosineProximity;\n// TODO(michaelterry): Add deserialize() function.\nexport const lossesMap = {\n    meanSquaredError,\n    meanAbsoluteError,\n    meanAbsolutePercentageError,\n    meanSquaredLogarithmicError,\n    squaredHinge,\n    hinge,\n    categoricalHinge,\n    logcosh,\n    categoricalCrossentropy,\n    sparseCategoricalCrossentropy,\n    binaryCrossentropy,\n    kullbackLeiblerDivergence,\n    poisson,\n    cosineProximity\n};\n// Porting note: This diverges from the PyKeras implementation and may need to\n// change based on (de)serialization requirements.\nexport function get(identifierOrFn) {\n    if (typeof identifierOrFn === 'string') {\n        if (identifierOrFn in lossesMap) {\n            return lossesMap[identifierOrFn];\n        }\n        let errMsg = `Unknown loss ${identifierOrFn}`;\n        if (identifierOrFn.toLowerCase().includes('softmaxcrossentropy')) {\n            errMsg = `Unknown loss ${identifierOrFn}. ` +\n                'Use \"categoricalCrossentropy\" as the string name for ' +\n                'tf.losses.softmaxCrossEntropy';\n        }\n        throw new ValueError(errMsg);\n    }\n    else {\n        return identifierOrFn;\n    }\n}\n//# sourceMappingURL=losses.js.map"]},"metadata":{},"sourceType":"module"}