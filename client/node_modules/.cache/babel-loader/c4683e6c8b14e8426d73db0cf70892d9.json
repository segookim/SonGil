{"ast":null,"code":"import _regeneratorRuntime from \"/Users/kimkiwoong/SonGil/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/regenerator\";\nimport _asyncToGenerator from \"/Users/kimkiwoong/SonGil/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/asyncToGenerator\";\nimport _createForOfIteratorHelper from \"/Users/kimkiwoong/SonGil/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/createForOfIteratorHelper\";\n\n/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Interfaces and methods for training models using TensorFlow.js datasets.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { scalar } from '@tensorflow/tfjs-core';\nimport { configureCallbacks, standardizeCallbacks } from '../base_callbacks';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { disposeTensorsInLogs } from '../logs';\nimport { singletonOrArray, toList } from '../utils/generic_utils';\nimport { standardizeClassWeights, standardizeWeights } from './training_utils'; // Default batch size used during tensor-based validation.\n\nvar DEFAULT_VALIDATION_BATCH_SIZE = 32;\n/**\n * Standardize the output of a dataset iterator for use by\n * LayersModel.fitDataset().\n *\n * @param model: A `tf.LayersModel` object.\n * @param iteratorOut The output of a dataset iterator. It is required to be\n *   an object of the form `{xs: TensorOrArrayOrMap, ys:\n * TensorOrArrayOrMap}`, where `TensorOrArrayOrMap` is a single `tf.Tensor`,\n * a `tf.Tensor[]`, or a flat map from string names to `tf.Tensor`s.\n * @returns A flat array of `tf.Tensor` objects: the input `tf.Tensor`s\n *   followed by the target `tf.Tensor`s.  When `tf.Tensor`s are provided\n *   as a map, the order in the resulting array is taken from the `inputNames`\n *   and `outputNames` of the model.\n */\n\nfunction standardizeDataIteratorOutput( // Type `model` as `any` here to avoid circular dependency w/\n// training.ts.\n// tslint:disable-next-line:no-any\nmodel, iteratorOut) {\n  var xs;\n  var ys;\n  var iteratorOutObj = iteratorOut;\n  xs = iteratorOutObj['xs'];\n  ys = iteratorOutObj['ys'];\n  tfc.util.assert(xs != null && ys != null, function () {\n    return 'A Dataset iterator for fitDataset() is expected to generate ' + 'objects of the form `{xs: xVal, ys: yVal}`, where the two ' + 'values may be `tf.Tensor`, an array of Tensors, or a map of ' + 'string to Tensor.  The provided Dataset instead generates ' + \"\".concat(iteratorOut);\n  });\n  var flattenedXs = flattenTensorOrArrayOrMap('input', model.inputNames, xs);\n  var flattenedYs = flattenTensorOrArrayOrMap('output', model.outputNames, ys);\n  var batchSize = flattenedXs[0].shape[0];\n  tfc.util.assert(flattenedXs.length === model.inputs.length, function () {\n    return \"LayersModel has \".concat(model.inputs.length, \" inputs, but the dataset \") + \"provides \".concat(flattenedXs.length, \" inputs.  (Expected input keys: \") + \"\".concat(JSON.stringify(model.inputNames), \")\");\n  });\n  tfc.util.assert(flattenedYs.length === model.outputs.length, function () {\n    return \"LayersModel has \".concat(model.outputs.length, \" outputs, but the dataset \") + \"provides \".concat(flattenedYs.length, \" outputs.  (Expected output keys: \") + \"\".concat(JSON.stringify(model.outputNames), \")\");\n  });\n\n  var _loop = function _loop(xIndex) {\n    tfc.util.assert(flattenedXs[xIndex].shape[0] === batchSize, function () {\n      return \"Batch size mismatch: input \" + \"\".concat(model.inputNames[xIndex], \" has \").concat(flattenedXs[xIndex].shape[0], \"; \") + \"expected  \".concat(batchSize, \" based on input \").concat(model.inputNames[0], \".\");\n    });\n  };\n\n  for (var xIndex = 0; xIndex < flattenedXs.length; xIndex++) {\n    _loop(xIndex);\n  }\n\n  var _loop2 = function _loop2(yIndex) {\n    tfc.util.assert(flattenedYs[yIndex].shape[0] === batchSize, function () {\n      return \"Batch size mismatch: output \" + \"\".concat(model.outputNames[yIndex], \" has \").concat(flattenedYs[yIndex].shape[0], \"; \") + \"expected  \".concat(batchSize, \" based on input \").concat(model.inputNames[0], \".\");\n    });\n  };\n\n  for (var yIndex = 0; yIndex < flattenedYs.length; yIndex++) {\n    _loop2(yIndex);\n  }\n\n  return {\n    xs: flattenedXs,\n    ys: flattenedYs\n  };\n}\n\nfunction flattenTensorOrArrayOrMap(inputOrOutput, names, values) {\n  if (values instanceof tfc.Tensor) {\n    return [values];\n  } else if (Array.isArray(values)) {\n    tfc.util.assert(values.length === names.length, function () {\n      return \"Received an array of \".concat(values.length, \" Tensors, but expected \").concat(names.length, \" to match the \").concat(inputOrOutput, \" keys \").concat(names, \".\");\n    });\n    return values;\n  } else {\n    var result = []; // Check that all the required keys are available.\n\n    var _iterator = _createForOfIteratorHelper(names),\n        _step;\n\n    try {\n      for (_iterator.s(); !(_step = _iterator.n()).done;) {\n        var name = _step.value;\n\n        if (values[name] == null) {\n          throw new ValueError(\"The feature data generated by the dataset lacks the required \" + \"\".concat(inputOrOutput, \" key '\").concat(name, \"'.\"));\n        }\n\n        result.push(values[name]);\n      }\n    } catch (err) {\n      _iterator.e(err);\n    } finally {\n      _iterator.f();\n    }\n\n    return result;\n  }\n}\n\nfunction standardizeTensorValidationData(data) {\n  if (data.length === 3) {\n    throw new NotImplementedError('Validation with sample weights is not implemented yet.');\n  }\n\n  return {\n    xs: data[0],\n    ys: data[1]\n  };\n}\n\nexport function fitDataset(_x, _x2, _x3) {\n  return _fitDataset.apply(this, arguments);\n}\n/** Helper function that determines number of steps (batches) per epoch. */\n\nfunction _fitDataset() {\n  _fitDataset = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee( // Type `model` as `any` here to avoid circular dependency w/\n  // training.ts.\n  // tslint:disable-next-line:no-any\n  model, dataset, args) {\n    var hasBatchesPerEpoch, doValidation, valXs, valYs, validationData, trainFunction, outLabels, callbackMetrics, callbacks, verbose, _configureCallbacks, callbackList, history, epoch, dataIterator, epochLogs, stepsDone, batchIndex, iteratorOut, _standardizeDataItera, xs, ys, batchLogs, sampleWeights, standardClassWeights, i, ins, outs, _i, label, out, valOuts, _i2;\n\n    return _regeneratorRuntime.wrap(function _callee$(_context) {\n      while (1) {\n        switch (_context.prev = _context.next) {\n          case 0:\n            hasBatchesPerEpoch = args.batchesPerEpoch != null;\n            tfc.util.assert(model.optimizer != null, function () {\n              return 'You must compile a model before training/testing. Use ' + 'LayersModel.compile(modelCompileConfig).';\n            });\n            tfc.util.assert(args != null, function () {\n              return \"For fitDataset(), the 2nd argument (config) is required, \" + \"but it is not provided in this call.\";\n            });\n            tfc.util.assert(args.epochs != null && args.epochs > 0 && Number.isInteger(args.epochs), function () {\n              return \"For fitDataset(), config.epochs is expected to be a positive \" + \"integer, but got \".concat(args.epochs);\n            });\n            tfc.util.assert(!hasBatchesPerEpoch || args.batchesPerEpoch > 0 && Number.isInteger(args.batchesPerEpoch), function () {\n              return \"For fitDataset(), config.batchesPerEpoch is expected to be a \" + \"positive integer if specified, but got \".concat(args.batchesPerEpoch);\n            });\n            tfc.util.assert( // tslint:disable-next-line:no-any\n            args['validationSplit'] == null, function () {\n              return '`validationSplit` is not supported by `fitDataset()`. ' + 'Use validationData instead.';\n            });\n\n            if (!model.isTraining) {\n              _context.next = 8;\n              break;\n            }\n\n            throw new Error('Cannot start training because another fit() call is ongoing.');\n\n          case 8:\n            model.isTraining = true;\n            _context.prev = 9;\n            doValidation = args.validationData != null;\n\n            if (doValidation) {\n              if (isDatasetObject(args.validationData)) {\n                tfc.util.assert(args.validationBatches == null || args.validationBatches > 0 && Number.isInteger(args.validationBatches), function () {\n                  return \"For fitDataset() with dataset-based validation, \" + \"config.validationBatches is expected not to be provided, \" + \"or to be a positive integer, \" + \"but got \".concat(args.validationBatches);\n                });\n              } else {\n                validationData = standardizeTensorValidationData(args.validationData);\n                valXs = validationData.xs;\n                valYs = validationData.ys;\n              }\n            }\n\n            trainFunction = model.makeTrainFunction();\n            outLabels = model.getDedupedMetricsNames();\n\n            if (doValidation) {\n              callbackMetrics = outLabels.slice().concat(outLabels.map(function (n) {\n                return 'val_' + n;\n              }));\n            } else {\n              callbackMetrics = outLabels.slice();\n            }\n\n            callbacks = standardizeCallbacks(args.callbacks, args.yieldEvery);\n            verbose = args.verbose == null ? 1 : args.verbose;\n            _configureCallbacks = configureCallbacks(callbacks, verbose, args.epochs, null, null, getStepsPerEpoch(dataset, args), null, // Batch size determined by the dataset itself.\n            doValidation, callbackMetrics), callbackList = _configureCallbacks.callbackList, history = _configureCallbacks.history;\n            callbackList.setModel(model);\n            model.history = history;\n            _context.next = 22;\n            return callbackList.onTrainBegin();\n\n          case 22:\n            model.stopTraining_ = false;\n            epoch = args.initialEpoch == null ? 0 : args.initialEpoch;\n            _context.next = 26;\n            return dataset.iterator();\n\n          case 26:\n            dataIterator = _context.sent;\n\n          case 27:\n            if (!(epoch < args.epochs)) {\n              _context.next = 98;\n              break;\n            }\n\n            epochLogs = {};\n            _context.next = 31;\n            return callbackList.onEpochBegin(epoch);\n\n          case 31:\n            stepsDone = 0;\n            batchIndex = 0;\n\n            if (hasBatchesPerEpoch) {\n              _context.next = 37;\n              break;\n            }\n\n            _context.next = 36;\n            return dataset.iterator();\n\n          case 36:\n            dataIterator = _context.sent;\n\n          case 37:\n            if (!(hasBatchesPerEpoch ? stepsDone < args.batchesPerEpoch : true)) {\n              _context.next = 91;\n              break;\n            }\n\n            _context.next = 40;\n            return dataIterator.next();\n\n          case 40:\n            iteratorOut = _context.sent;\n\n            if (!(hasBatchesPerEpoch && iteratorOut.done)) {\n              _context.next = 44;\n              break;\n            }\n\n            console.warn('You provided `batchesPerEpoch` as ' + \"\".concat(args.batchesPerEpoch, \", \") + 'but your dataset iterator ran out of data after ' + \"\".concat(stepsDone, \" batches; \") + 'interrupting training. Make sure that your ' + 'dataset can generate at least `batchesPerEpoch * epochs` ' + 'batches (in this case, ' + \"\".concat(args.batchesPerEpoch * args.epochs, \" batches). \") + 'You may need to use the repeat() function when building ' + 'your dataset.');\n            return _context.abrupt(\"break\", 91);\n\n          case 44:\n            if (!(iteratorOut.value != null)) {\n              _context.next = 73;\n              break;\n            }\n\n            _standardizeDataItera = standardizeDataIteratorOutput(model, iteratorOut.value), xs = _standardizeDataItera.xs, ys = _standardizeDataItera.ys;\n            batchLogs = {};\n            batchLogs['batch'] = batchIndex;\n            batchLogs['size'] = xs[0].shape[0];\n            _context.next = 51;\n            return callbackList.onBatchBegin(batchIndex, batchLogs);\n\n          case 51:\n            sampleWeights = [];\n\n            if (!(args.classWeight != null)) {\n              _context.next = 64;\n              break;\n            }\n\n            standardClassWeights = standardizeClassWeights(args.classWeight, model.outputNames);\n            i = 0;\n\n          case 55:\n            if (!(i < standardClassWeights.length)) {\n              _context.next = 64;\n              break;\n            }\n\n            _context.t0 = sampleWeights;\n            _context.next = 59;\n            return standardizeWeights(ys[i], null, standardClassWeights[i]);\n\n          case 59:\n            _context.t1 = _context.sent;\n\n            _context.t0.push.call(_context.t0, _context.t1);\n\n          case 61:\n            ++i;\n            _context.next = 55;\n            break;\n\n          case 64:\n            // Train on batch.\n            ins = xs.concat(ys).concat(sampleWeights);\n            outs = trainFunction(ins);\n            tfc.dispose(ins);\n\n            for (_i = 0; _i < outLabels.length; ++_i) {\n              label = outLabels[_i];\n              out = outs[_i];\n              batchLogs[label] = out;\n              tfc.keep(out);\n            }\n\n            _context.next = 70;\n            return callbackList.onBatchEnd(batchIndex, batchLogs);\n\n          case 70:\n            disposeTensorsInLogs(batchLogs);\n            batchIndex++;\n            stepsDone++;\n\n          case 73:\n            if (!(hasBatchesPerEpoch ? stepsDone >= args.batchesPerEpoch : iteratorOut.done)) {\n              _context.next = 87;\n              break;\n            }\n\n            if (!doValidation) {\n              _context.next = 86;\n              break;\n            }\n\n            valOuts = void 0;\n\n            if (!isDatasetObject(args.validationData)) {\n              _context.next = 84;\n              break;\n            }\n\n            _context.t2 = toList;\n            _context.next = 80;\n            return model.evaluateDataset(args.validationData, {\n              batches: args.validationBatches\n            });\n\n          case 80:\n            _context.t3 = _context.sent;\n            valOuts = (0, _context.t2)(_context.t3);\n            _context.next = 85;\n            break;\n\n          case 84:\n            valOuts = toList(model.evaluate(valXs, valYs, {\n              batchSize: args.validationBatchSize == null ? DEFAULT_VALIDATION_BATCH_SIZE : args.validationBatchSize,\n              verbose: 0\n            }));\n\n          case 85:\n            for (_i2 = 0; _i2 < model.metricsNames.length; ++_i2) {\n              epochLogs[\"val_\".concat(model.metricsNames[_i2])] = valOuts[_i2];\n            }\n\n          case 86:\n            return _context.abrupt(\"break\", 91);\n\n          case 87:\n            if (!model.stopTraining_) {\n              _context.next = 89;\n              break;\n            }\n\n            return _context.abrupt(\"break\", 91);\n\n          case 89:\n            _context.next = 37;\n            break;\n\n          case 91:\n            _context.next = 93;\n            return callbackList.onEpochEnd(epoch, epochLogs);\n\n          case 93:\n            epoch++;\n\n            if (!model.stopTraining_) {\n              _context.next = 96;\n              break;\n            }\n\n            return _context.abrupt(\"break\", 98);\n\n          case 96:\n            _context.next = 27;\n            break;\n\n          case 98:\n            _context.next = 100;\n            return callbackList.onTrainEnd();\n\n          case 100:\n            _context.next = 102;\n            return model.history.syncData();\n\n          case 102:\n            return _context.abrupt(\"return\", model.history);\n\n          case 103:\n            _context.prev = 103;\n            model.isTraining = false;\n            return _context.finish(103);\n\n          case 106:\n          case \"end\":\n            return _context.stop();\n        }\n      }\n    }, _callee, null, [[9,, 103, 106]]);\n  }));\n  return _fitDataset.apply(this, arguments);\n}\n\nfunction getStepsPerEpoch(dataset, args) {\n  // Attempt to determine # of batches in an epoch.\n  var stepsPerEpoch = null;\n\n  if (args.batchesPerEpoch != null) {\n    stepsPerEpoch = args.batchesPerEpoch;\n  } else if (Number.isFinite(dataset.size)) {\n    stepsPerEpoch = dataset.size;\n  }\n\n  return stepsPerEpoch;\n} // Check if provided object is a Dataset object by checking its .iterator\n// element.\n\n\nfunction isDatasetObject(dataset) {\n  return typeof dataset.iterator === 'function';\n} // Check if provided object is a LazyIterator object by checking it's .next\n// element.\n\n\nfunction isLazyIteratorObject(iterator) {\n  return typeof iterator.next === 'function';\n}\n\nexport function evaluateDataset(_x4, _x5, _x6) {\n  return _evaluateDataset.apply(this, arguments);\n}\n\nfunction _evaluateDataset() {\n  _evaluateDataset = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee2( // Type `model` as `any` here to avoid circular dependency w/\n  // training.ts.\n  // tslint:disable-next-line:no-any\n  model, dataset, args) {\n    var hasBatches, f, outs, dataIterator, numExamples, batch, _loop3, _ret, i, oldScalar;\n\n    return _regeneratorRuntime.wrap(function _callee2$(_context3) {\n      while (1) {\n        switch (_context3.prev = _context3.next) {\n          case 0:\n            args = args || {};\n            hasBatches = args.batches != null;\n            f = model.testFunction;\n            outs = [];\n\n            if (!(args.verbose > 0)) {\n              _context3.next = 6;\n              break;\n            }\n\n            throw new NotImplementedError('Verbose mode is not implemented yet.');\n\n          case 6:\n            tfc.util.assert(!hasBatches || args.batches > 0 && Number.isInteger(args.batches), function () {\n              return 'Test loop expects `batches` to be a positive integer, but ' + \"received \".concat(JSON.stringify(args.batches));\n            });\n\n            if (!isLazyIteratorObject(dataset)) {\n              _context3.next = 11;\n              break;\n            }\n\n            _context3.t0 = dataset;\n            _context3.next = 14;\n            break;\n\n          case 11:\n            _context3.next = 13;\n            return dataset.iterator();\n\n          case 13:\n            _context3.t0 = _context3.sent;\n\n          case 14:\n            dataIterator = _context3.t0;\n            // Keeps track of number of examples used in this evaluation.\n            numExamples = 0;\n            batch = 0;\n            _loop3 = /*#__PURE__*/_regeneratorRuntime.mark(function _loop3() {\n              var iteratorOut;\n              return _regeneratorRuntime.wrap(function _loop3$(_context2) {\n                while (1) {\n                  switch (_context2.prev = _context2.next) {\n                    case 0:\n                      _context2.next = 2;\n                      return dataIterator.next();\n\n                    case 2:\n                      iteratorOut = _context2.sent;\n                      outs = tfc.tidy(function () {\n                        if (iteratorOut.value) {\n                          (function () {\n                            // TODO(cais): Once real dataset is available, use\n                            //   `map(x => standardizeDataIteratorOutput(model, x).map(f)`.\n                            var _standardizeDataItera2 = standardizeDataIteratorOutput(model, iteratorOut.value),\n                                xs = _standardizeDataItera2.xs,\n                                ys = _standardizeDataItera2.ys;\n\n                            var xsAndYs = xs.concat(ys);\n                            var batchOuts = tfc.tidy(function () {\n                              return f(xsAndYs);\n                            });\n                            tfc.dispose(xsAndYs);\n\n                            if (batch === 0) {\n                              for (var _i3 = 0; _i3 < batchOuts.length; ++_i3) {\n                                outs.push(scalar(0));\n                              }\n                            }\n\n                            var batchSize = xsAndYs[0].shape[0];\n\n                            var _loop4 = function _loop4(_i4) {\n                              var batchOut = batchOuts[_i4];\n                              var oldScalar = outs[_i4];\n                              outs[_i4] = tfc.tidy(function () {\n                                return tfc.add(outs[_i4], tfc.mul(batchSize, batchOut));\n                              });\n\n                              if (batch > 0) {\n                                tfc.dispose(oldScalar);\n                              }\n                            };\n\n                            for (var _i4 = 0; _i4 < batchOuts.length; ++_i4) {\n                              _loop4(_i4);\n                            }\n\n                            tfc.dispose(batchOuts);\n                            numExamples += batchSize;\n                            ++batch;\n                          })();\n                        }\n\n                        return outs;\n                      });\n\n                      if (!iteratorOut.done) {\n                        _context2.next = 7;\n                        break;\n                      }\n\n                      if (hasBatches) {\n                        console.warn('Your dataset iterator ran out of data during evaluateDataset(). ' + 'Interrupting evalution. Make sure that your ' + 'dataset can generate at least `batches` ' + \"batches (in this case, \".concat(args.batches, \" batches). \") + 'You may need to use the repeat() function when building ' + 'your dataset.');\n                      }\n\n                      return _context2.abrupt(\"return\", \"break\");\n\n                    case 7:\n                    case \"end\":\n                      return _context2.stop();\n                  }\n                }\n              }, _loop3);\n            });\n\n          case 18:\n            if (!(hasBatches ? batch < args.batches : true)) {\n              _context3.next = 25;\n              break;\n            }\n\n            return _context3.delegateYield(_loop3(), \"t1\", 20);\n\n          case 20:\n            _ret = _context3.t1;\n\n            if (!(_ret === \"break\")) {\n              _context3.next = 23;\n              break;\n            }\n\n            return _context3.abrupt(\"break\", 25);\n\n          case 23:\n            _context3.next = 18;\n            break;\n\n          case 25:\n            for (i = 0; i < outs.length; ++i) {\n              oldScalar = outs[i];\n              outs[i] = tfc.div(outs[i], numExamples);\n              tfc.dispose(oldScalar);\n            }\n\n            return _context3.abrupt(\"return\", singletonOrArray(outs));\n\n          case 27:\n          case \"end\":\n            return _context3.stop();\n        }\n      }\n    }, _callee2);\n  }));\n  return _evaluateDataset.apply(this, arguments);\n}","map":{"version":3,"sources":["../../src/engine/training_dataset.ts"],"names":[],"mappings":";;;;AAAA;;;;;;;;AAQG;;AAEH;;AAEG;AAEH,OAAO,KAAK,GAAZ,MAAqB,uBAArB;AACA,SAAQ,MAAR,QAAqB,uBAArB;AACA,SAAsB,kBAAtB,EAA8F,oBAA9F,QAA4I,mBAA5I;AACA,SAAQ,mBAAR,EAA6B,UAA7B,QAA8C,WAA9C;AACA,SAAQ,oBAAR,QAAmD,SAAnD;AAEA,SAAQ,gBAAR,EAA0B,MAA1B,QAAuC,wBAAvC;AAGA,SAAqC,uBAArC,EAA8D,kBAA9D,QAAuF,kBAAvF,C,CAiKA;;AACA,IAAM,6BAA6B,GAAG,EAAtC;AAEA;;;;;;;;;;;;;AAaG;;AACH,SAAS,6BAAT,EACI;AACA;AACA;AACA,KAJJ,EAIgB,WAJhB,EAI+B;AAC7B,MAAI,EAAJ;AACA,MAAI,EAAJ;AAEA,MAAM,cAAc,GAAG,WAAvB;AACA,EAAA,EAAE,GAAG,cAAc,CAAC,IAAD,CAAnB;AACA,EAAA,EAAE,GAAG,cAAc,CAAC,IAAD,CAAnB;AACA,EAAA,GAAG,CAAC,IAAJ,CAAS,MAAT,CACI,EAAE,IAAI,IAAN,IAAc,EAAE,IAAI,IADxB,EAEI;AAAA,WAAM,iEACF,4DADE,GAEF,8DAFE,GAGF,4DAHE,aAIC,WAJD,CAAN;AAAA,GAFJ;AAQA,MAAM,WAAW,GACb,yBAAyB,CAAC,OAAD,EAAU,KAAK,CAAC,UAAhB,EAA4B,EAA5B,CAD7B;AAEA,MAAM,WAAW,GACb,yBAAyB,CAAC,QAAD,EAAW,KAAK,CAAC,WAAjB,EAA8B,EAA9B,CAD7B;AAGA,MAAM,SAAS,GAAW,WAAW,CAAC,CAAD,CAAX,CAAe,KAAf,CAAqB,CAArB,CAA1B;AAEA,EAAA,GAAG,CAAC,IAAJ,CAAS,MAAT,CACI,WAAW,CAAC,MAAZ,KAAuB,KAAK,CAAC,MAAN,CAAa,MADxC,EAEI;AAAA,WAAM,0BAAmB,KAAK,CAAC,MAAN,CAAa,MAAhC,oDACU,WAAW,CAAC,MADtB,kDAEC,IAAI,CAAC,SAAL,CAAe,KAAK,CAAC,UAArB,CAFD,MAAN;AAAA,GAFJ;AAMA,EAAA,GAAG,CAAC,IAAJ,CAAS,MAAT,CACI,WAAW,CAAC,MAAZ,KAAuB,KAAK,CAAC,OAAN,CAAc,MADzC,EAEI;AAAA,WACI,0BAAmB,KAAK,CAAC,OAAN,CAAc,MAAjC,qDACY,WAAW,CAAC,MADxB,oDAEG,IAAI,CAAC,SAAL,CAAe,KAAK,CAAC,WAArB,CAFH,MADJ;AAAA,GAFJ;;AA5B6B,6BAmCpB,MAnCoB;AAoC3B,IAAA,GAAG,CAAC,IAAJ,CAAS,MAAT,CACI,WAAW,CAAC,MAAD,CAAX,CAAoB,KAApB,CAA0B,CAA1B,MAAiC,SADrC,EAEI;AAAA,aAAM,0CACC,KAAK,CAAC,UAAN,CAAiB,MAAjB,CADD,kBAEI,WAAW,CAAC,MAAD,CAAX,CAAoB,KAApB,CAA0B,CAA1B,CAFJ,8BAGW,SAHX,6BAGuC,KAAK,CAAC,UAAN,CAAiB,CAAjB,CAHvC,MAAN;AAAA,KAFJ;AApC2B;;AAmC7B,OAAK,IAAI,MAAM,GAAG,CAAlB,EAAqB,MAAM,GAAG,WAAW,CAAC,MAA1C,EAAkD,MAAM,EAAxD,EAA4D;AAAA,UAAnD,MAAmD;AAO3D;;AA1C4B,+BA4CpB,MA5CoB;AA6C3B,IAAA,GAAG,CAAC,IAAJ,CAAS,MAAT,CACI,WAAW,CAAC,MAAD,CAAX,CAAoB,KAApB,CAA0B,CAA1B,MAAiC,SADrC,EAEI;AAAA,aAAM,2CACC,KAAK,CAAC,WAAN,CAAkB,MAAlB,CADD,kBAEI,WAAW,CAAC,MAAD,CAAX,CAAoB,KAApB,CAA0B,CAA1B,CAFJ,8BAGW,SAHX,6BAGuC,KAAK,CAAC,UAAN,CAAiB,CAAjB,CAHvC,MAAN;AAAA,KAFJ;AA7C2B;;AA4C7B,OAAK,IAAI,MAAM,GAAG,CAAlB,EAAqB,MAAM,GAAG,WAAW,CAAC,MAA1C,EAAkD,MAAM,EAAxD,EAA4D;AAAA,WAAnD,MAAmD;AAO3D;;AAED,SAAO;AAAC,IAAA,EAAE,EAAE,WAAL;AAAkB,IAAA,EAAE,EAAE;AAAtB,GAAP;AACD;;AAED,SAAS,yBAAT,CACI,aADJ,EAC2B,KAD3B,EAC4C,MAD5C,EACsE;AACpE,MAAI,MAAM,YAAY,GAAG,CAAC,MAA1B,EAAkC;AAChC,WAAO,CAAC,MAAD,CAAP;AACD,GAFD,MAEO,IAAI,KAAK,CAAC,OAAN,CAAc,MAAd,CAAJ,EAA2B;AAChC,IAAA,GAAG,CAAC,IAAJ,CAAS,MAAT,CACI,MAAM,CAAC,MAAP,KAAkB,KAAK,CAAC,MAD5B,EAEI;AAAA,4CAA8B,MAAM,CAAC,MAArC,oCACI,KAAK,CAAC,MADV,2BACiC,aADjC,mBACuD,KADvD;AAAA,KAFJ;AAIA,WAAO,MAAP;AACD,GANM,MAMA;AACL,QAAM,MAAM,GAAiB,EAA7B,CADK,CAEL;;AAFK,+CAGc,KAHd;AAAA;;AAAA;AAGL,0DAA0B;AAAA,YAAf,IAAe;;AACxB,YAAI,MAAM,CAAC,IAAD,CAAN,IAAgB,IAApB,EAA0B;AACxB,gBAAM,IAAI,UAAJ,CACF,4EACG,aADH,mBACyB,IADzB,OADE,CAAN;AAGD;;AACD,QAAA,MAAM,CAAC,IAAP,CAAY,MAAM,CAAC,IAAD,CAAlB;AACD;AAVI;AAAA;AAAA;AAAA;AAAA;;AAWL,WAAO,MAAP;AACD;AACF;;AAED,SAAS,+BAAT,CACI,IADJ,EAKqC;AAEnC,MAAI,IAAI,CAAC,MAAL,KAAgB,CAApB,EAAuB;AACrB,UAAM,IAAI,mBAAJ,CACF,wDADE,CAAN;AAED;;AACD,SAAO;AAAC,IAAA,EAAE,EAAE,IAAI,CAAC,CAAD,CAAT;AAAc,IAAA,EAAE,EAAE,IAAI,CAAC,CAAD;AAAtB,GAAP;AACD;;AAED,gBAAsB,UAAtB;AAAA;AAAA;AAyMA;;;yEAzMO,kBACH;AACA;AACA;AACA,EAAA,KAJG,EAIS,OAJT,EAKH,IALG;AAAA;;AAAA;AAAA;AAAA;AAAA;AAMC,YAAA,kBAND,GAMsB,IAAI,CAAC,eAAL,IAAwB,IAN9C;AAOL,YAAA,GAAG,CAAC,IAAJ,CAAS,MAAT,CACI,KAAK,CAAC,SAAN,IAAmB,IADvB,EAEI;AAAA,qBAAM,2DACF,0CADJ;AAAA,aAFJ;AAKA,YAAA,GAAG,CAAC,IAAJ,CAAS,MAAT,CACI,IAAI,IAAI,IADZ,EAEI;AAAA,qBAAM,oGAAN;AAAA,aAFJ;AAIA,YAAA,GAAG,CAAC,IAAJ,CAAS,MAAT,CACI,IAAI,CAAC,MAAL,IAAe,IAAf,IAAuB,IAAI,CAAC,MAAL,GAAc,CAArC,IAA0C,MAAM,CAAC,SAAP,CAAiB,IAAI,CAAC,MAAtB,CAD9C,EAEI;AAAA,qBAAM,6FACkB,IAAI,CAAC,MADvB,CAAN;AAAA,aAFJ;AAIA,YAAA,GAAG,CAAC,IAAJ,CAAS,MAAT,CACI,CAAC,kBAAD,IACK,IAAI,CAAC,eAAL,GAAuB,CAAvB,IAA4B,MAAM,CAAC,SAAP,CAAiB,IAAI,CAAC,eAAtB,CAFrC,EAGI;AAAA,qBAAM,mHACwC,IAAI,CAAC,eAD7C,CAAN;AAAA,aAHJ;AAKA,YAAA,GAAG,CAAC,IAAJ,CAAS,MAAT,EACI;AACC,YAAA,IAAY,CAAC,iBAAD,CAAZ,IAAmC,IAFxC,EAGI;AAAA,qBAAM,2DACF,6BADJ;AAAA,aAHJ;;AAzBK,iBA+BD,KAAK,CAAC,UA/BL;AAAA;AAAA;AAAA;;AAAA,kBAgCG,IAAI,KAAJ,CACF,8DADE,CAhCH;;AAAA;AAmCL,YAAA,KAAK,CAAC,UAAN,GAAmB,IAAnB;AAnCK;AAsCG,YAAA,YAtCH,GAsCkB,IAAI,CAAC,cAAL,IAAuB,IAtCzC;;AAyCH,gBAAI,YAAJ,EAAkB;AAChB,kBAAI,eAAe,CAAC,IAAI,CAAC,cAAN,CAAnB,EAA0C;AACxC,gBAAA,GAAG,CAAC,IAAJ,CAAS,MAAT,CACI,IAAI,CAAC,iBAAL,IAA0B,IAA1B,IACK,IAAI,CAAC,iBAAL,GAAyB,CAAzB,IACA,MAAM,CAAC,SAAP,CAAiB,IAAI,CAAC,iBAAtB,CAHT,EAII;AAAA,yBAAM,uKAGS,IAAI,CAAC,iBAHd,CAAN;AAAA,iBAJJ;AAQD,eATD,MASO;AACC,gBAAA,cADD,GACkB,+BAA+B,CAClD,IAAI,CAAC,cAD6C,CADjD;AAQL,gBAAA,KAAK,GAAG,cAAc,CAAC,EAAvB;AACA,gBAAA,KAAK,GAAG,cAAc,CAAC,EAAvB;AACD;AACF;;AAEK,YAAA,aAhEH,GAgEmB,KAAK,CAAC,iBAAN,EAhEnB;AAiEG,YAAA,SAjEH,GAiEe,KAAK,CAAC,sBAAN,EAjEf;;AAoEH,gBAAI,YAAJ,EAAkB;AAChB,cAAA,eAAe,GACX,SAAS,CAAC,KAAV,GAAkB,MAAlB,CAAyB,SAAS,CAAC,GAAV,CAAc,UAAA,CAAC;AAAA,uBAAI,SAAS,CAAb;AAAA,eAAf,CAAzB,CADJ;AAED,aAHD,MAGO;AACL,cAAA,eAAe,GAAG,SAAS,CAAC,KAAV,EAAlB;AACD;;AAEK,YAAA,SA3EH,GA2Ee,oBAAoB,CAAC,IAAI,CAAC,SAAN,EAAiB,IAAI,CAAC,UAAtB,CA3EnC;AA4EG,YAAA,OA5EH,GA4Ea,IAAI,CAAC,OAAL,IAAgB,IAAhB,GAAuB,CAAvB,GAA2B,IAAI,CAAC,OA5E7C;AAAA,kCA6E6B,kBAAkB,CAC9C,SAD8C,EACnC,OADmC,EAC1B,IAAI,CAAC,MADqB,EACb,IADa,EACP,IADO,EAE9C,gBAAgB,CAAC,OAAD,EAAU,IAAV,CAF8B,EAG9C,IAH8C,EAGvC;AACP,YAAA,YAJ8C,EAIhC,eAJgC,CA7E/C,EA6EI,YA7EJ,uBA6EI,YA7EJ,EA6EkB,OA7ElB,uBA6EkB,OA7ElB;AAkFH,YAAA,YAAY,CAAC,QAAb,CAAsB,KAAtB;AACA,YAAA,KAAK,CAAC,OAAN,GAAgB,OAAhB;AAnFG;AAAA,mBAqFG,YAAY,CAAC,YAAb,EArFH;;AAAA;AAsFH,YAAA,KAAK,CAAC,aAAN,GAAsB,KAAtB;AACI,YAAA,KAvFD,GAuFS,IAAI,CAAC,YAAL,IAAqB,IAArB,GAA4B,CAA5B,GAAgC,IAAI,CAAC,YAvF9C;AAAA;AAAA,mBAyFsB,OAAO,CAAC,QAAR,EAzFtB;;AAAA;AAyFC,YAAA,YAzFD;;AAAA;AAAA,kBA0FI,KAAK,GAAG,IAAI,CAAC,MA1FjB;AAAA;AAAA;AAAA;;AA2FK,YAAA,SA3FL,GA2FiC,EA3FjC;AAAA;AAAA,mBA4FK,YAAY,CAAC,YAAb,CAA0B,KAA1B,CA5FL;;AAAA;AA6FG,YAAA,SA7FH,GA6Fe,CA7Ff;AA8FG,YAAA,UA9FH,GA8FgB,CA9FhB;;AAAA,gBA+FI,kBA/FJ;AAAA;AAAA;AAAA;;AAAA;AAAA,mBAgGsB,OAAO,CAAC,QAAR,EAhGtB;;AAAA;AAgGC,YAAA,YAhGD;;AAAA;AAAA,kBAkGM,kBAAkB,GAAG,SAAS,GAAG,IAAI,CAAC,eAApB,GAAsC,IAlG9D;AAAA;AAAA;AAAA;;AAAA;AAAA,mBAmG2B,YAAY,CAAC,IAAb,EAnG3B;;AAAA;AAmGO,YAAA,WAnGP;;AAAA,kBAuGK,kBAAkB,IAAI,WAAW,CAAC,IAvGvC;AAAA;AAAA;AAAA;;AAwGG,YAAA,OAAO,CAAC,IAAR,CACI,iDACG,IAAI,CAAC,eADR,UAEA,kDAFA,aAGG,SAHH,kBAIA,6CAJA,GAKA,2DALA,GAMA,yBANA,aAOG,IAAI,CAAC,eAAL,GAAuB,IAAI,CAAC,MAP/B,mBAQA,0DARA,GASA,eAVJ;AAxGH;;AAAA;AAAA,kBAsHK,WAAW,CAAC,KAAZ,IAAqB,IAtH1B;AAAA;AAAA;AAAA;;AAAA,oCAwHO,6BAA6B,CAAC,KAAD,EAAQ,WAAW,CAAC,KAApB,CAxHpC,EAuHU,EAvHV,yBAuHU,EAvHV,EAuHc,EAvHd,yBAuHc,EAvHd;AAyHS,YAAA,SAzHT,GAyHqC,EAzHrC;AA0HG,YAAA,SAAS,CAAC,OAAD,CAAT,GAAqB,UAArB;AACA,YAAA,SAAS,CAAC,MAAD,CAAT,GAAoB,EAAE,CAAC,CAAD,CAAF,CAAM,KAAN,CAAY,CAAZ,CAApB;AA3HH;AAAA,mBA6HS,YAAY,CAAC,YAAb,CAA0B,UAA1B,EAAsC,SAAtC,CA7HT;;AAAA;AA+HS,YAAA,aA/HT,GA+HuC,EA/HvC;;AAAA,kBAgIO,IAAI,CAAC,WAAL,IAAoB,IAhI3B;AAAA;AAAA;AAAA;;AAiIW,YAAA,oBAjIX,GAkIS,uBAAuB,CAAC,IAAI,CAAC,WAAN,EAAmB,KAAK,CAAC,WAAzB,CAlIhC;AAmIc,YAAA,CAnId,GAmIkB,CAnIlB;;AAAA;AAAA,kBAmIqB,CAAC,GAAG,oBAAoB,CAAC,MAnI9C;AAAA;AAAA;AAAA;;AAAA,0BAoIO,aApIP;AAAA;AAAA,mBAoIgC,kBAAkB,CACvC,EAAE,CAAC,CAAD,CADqC,EAChC,IADgC,EAC1B,oBAAoB,CAAC,CAAD,CADM,CApIlD;;AAAA;AAAA;;AAAA,wBAoIqB,IApIrB;;AAAA;AAmIsD,cAAE,CAnIxD;AAAA;AAAA;;AAAA;AAyIG;AACM,YAAA,GA1IT,GA0Ie,EAAE,CAAC,MAAH,CAAU,EAAV,EAAc,MAAd,CAAqB,aAArB,CA1If;AA2IS,YAAA,IA3IT,GA2IgB,aAAa,CAAC,GAAD,CA3I7B;AA4IG,YAAA,GAAG,CAAC,OAAJ,CAAY,GAAZ;;AACA,iBAAS,EAAT,GAAa,CAAb,EAAgB,EAAC,GAAG,SAAS,CAAC,MAA9B,EAAsC,EAAE,EAAxC,EAA2C;AACnC,cAAA,KADmC,GAC3B,SAAS,CAAC,EAAD,CADkB;AAEnC,cAAA,GAFmC,GAE7B,IAAI,CAAC,EAAD,CAFyB;AAGzC,cAAA,SAAS,CAAC,KAAD,CAAT,GAAmB,GAAnB;AACA,cAAA,GAAG,CAAC,IAAJ,CAAS,GAAT;AACD;;AAlJJ;AAAA,mBAoJS,YAAY,CAAC,UAAb,CAAwB,UAAxB,EAAoC,SAApC,CApJT;;AAAA;AAqJG,YAAA,oBAAoB,CAAC,SAAD,CAApB;AAEA,YAAA,UAAU;AACV,YAAA,SAAS;;AAxJZ;AAAA,kBA2JK,kBAAkB,GAAG,SAAS,IAAI,IAAI,CAAC,eAArB,GACG,WAAW,CAAC,IA5JtC;AAAA;AAAA;AAAA;;AAAA,iBA8JO,YA9JP;AAAA;AAAA;AAAA;;AA+JS,YAAA,OA/JT;;AAAA,iBAgKS,eAAe,CAAC,IAAI,CAAC,cAAN,CAhKxB;AAAA;AAAA;AAAA;;AAAA,0BAiKiB,MAjKjB;AAAA;AAAA,mBAiK8B,KAAK,CAAC,eAAN,CACnB,IAAI,CAAC,cADc,EACE;AAAC,cAAA,OAAO,EAAE,IAAI,CAAC;AAAf,aADF,CAjK9B;;AAAA;AAAA;AAiKO,YAAA,OAjKP;AAAA;AAAA;;AAAA;AAoKO,YAAA,OAAO,GAAG,MAAM,CAAC,KAAK,CAAC,QAAN,CAAe,KAAf,EAAsB,KAAtB,EAA6B;AAC5C,cAAA,SAAS,EAAE,IAAI,CAAC,mBAAL,IAA4B,IAA5B,GACP,6BADO,GAEP,IAAI,CAAC,mBAHmC;AAI5C,cAAA,OAAO,EAAE;AAJmC,aAA7B,CAAD,CAAhB;;AApKP;AA2KK,iBAAS,GAAT,GAAa,CAAb,EAAgB,GAAC,GAAG,KAAK,CAAC,YAAN,CAAmB,MAAvC,EAA+C,EAAE,GAAjD,EAAoD;AAClD,cAAA,SAAS,eAAQ,KAAK,CAAC,YAAN,CAAmB,GAAnB,CAAR,EAAT,GAA4C,OAAO,CAAC,GAAD,CAAnD;AACD;;AA7KN;AAAA;;AAAA;AAAA,iBAuLK,KAAK,CAAC,aAvLX;AAAA;AAAA;AAAA;;AAAA;;AAAA;AAAA;AAAA;;AAAA;AAAA;AAAA,mBA2LK,YAAY,CAAC,UAAb,CAAwB,KAAxB,EAA+B,SAA/B,CA3LL;;AAAA;AA4LD,YAAA,KAAK;;AA5LJ,iBA6LG,KAAK,CAAC,aA7LT;AAAA;AAAA;AAAA;;AAAA;;AAAA;AAAA;AAAA;;AAAA;AAAA;AAAA,mBAiMG,YAAY,CAAC,UAAb,EAjMH;;AAAA;AAAA;AAAA,mBAkMG,KAAK,CAAC,OAAN,CAAc,QAAd,EAlMH;;AAAA;AAAA,6CAmMI,KAAK,CAAC,OAnMV;;AAAA;AAAA;AAqMH,YAAA,KAAK,CAAC,UAAN,GAAmB,KAAnB;AArMG;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,G;;;;AA0MP,SAAS,gBAAT,CACI,OADJ,EACyB,IADzB,EACqD;AACnD;AACA,MAAI,aAAa,GAAW,IAA5B;;AACA,MAAI,IAAI,CAAC,eAAL,IAAwB,IAA5B,EAAkC;AAChC,IAAA,aAAa,GAAG,IAAI,CAAC,eAArB;AACD,GAFD,MAEO,IAAI,MAAM,CAAC,QAAP,CAAgB,OAAO,CAAC,IAAxB,CAAJ,EAAmC;AACxC,IAAA,aAAa,GAAG,OAAO,CAAC,IAAxB;AACD;;AACD,SAAO,aAAP;AACD,C,CAED;AACA;;;AACA,SAAS,eAAT,CACI,OADJ,EAKc;AACZ,SAAQ,OAAQ,OAAsB,CAAC,QAA/B,KAA4C,UAApD;AACD,C,CAED;AACA;;;AACA,SAAS,oBAAT,CAAiC,QAAjC,EACgD;AAC9C,SAAQ,OAAQ,QAA4B,CAAC,IAArC,KAA8C,UAAtD;AACD;;AAED,gBAAsB,eAAtB;AAAA;AAAA;;;8EAAO,mBACH;AACA;AACA;AACA,EAAA,KAJG,EAIS,OAJT,EAKH,IALG;AAAA;;AAAA;AAAA;AAAA;AAAA;AAML,YAAA,IAAI,GAAG,IAAI,IAAI,EAAf;AACM,YAAA,UAPD,GAOc,IAAI,CAAC,OAAL,IAAgB,IAP9B;AAQC,YAAA,CARD,GAQK,KAAK,CAAC,YARX;AASD,YAAA,IATC,GASoB,EATpB;;AAAA,kBAUD,IAAI,CAAC,OAAL,GAAe,CAVd;AAAA;AAAA;AAAA;;AAAA,kBAWG,IAAI,mBAAJ,CAAwB,sCAAxB,CAXH;;AAAA;AAcL,YAAA,GAAG,CAAC,IAAJ,CAAS,MAAT,CACI,CAAC,UAAD,IAAgB,IAAI,CAAC,OAAL,GAAe,CAAf,IAAoB,MAAM,CAAC,SAAP,CAAiB,IAAI,CAAC,OAAtB,CADxC,EAEI;AAAA,qBAAM,kFACU,IAAI,CAAC,SAAL,CAAe,IAAI,CAAC,OAApB,CADV,CAAN;AAAA,aAFJ;;AAdK,iBAkBgB,oBAAoB,CAAC,OAAD,CAlBpC;AAAA;AAAA;AAAA;;AAAA,2BAmBD,OAnBC;AAAA;AAAA;;AAAA;AAAA;AAAA,mBAoBM,OAAsB,CAAC,QAAvB,EApBN;;AAAA;AAAA;;AAAA;AAkBC,YAAA,YAlBD;AAqBL;AACI,YAAA,WAtBC,GAsBa,CAtBb;AAuBD,YAAA,KAvBC,GAuBO,CAvBP;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,6BA0BuB,YAAY,CAAC,IAAb,EA1BvB;;AAAA;AA0BG,sBAAA,WA1BH;AA2BH,sBAAA,IAAI,GAAG,GAAG,CAAC,IAAJ,CAAS,YAAK;AACnB,4BAAI,WAAW,CAAC,KAAhB,EAAuB;AAAA;AACrB;AACA;AAFqB,yDAIjB,6BAA6B,CAAC,KAAD,EAAQ,WAAW,CAAC,KAApB,CAJZ;AAAA,gCAGd,EAHc,0BAGd,EAHc;AAAA,gCAGV,EAHU,0BAGV,EAHU;;AAKrB,gCAAM,OAAO,GAAG,EAAE,CAAC,MAAH,CAAU,EAAV,CAAhB;AACA,gCAAM,SAAS,GAAG,GAAG,CAAC,IAAJ,CAAS;AAAA,qCAAM,CAAC,CAAC,OAAD,CAAP;AAAA,6BAAT,CAAlB;AACA,4BAAA,GAAG,CAAC,OAAJ,CAAY,OAAZ;;AAEA,gCAAI,KAAK,KAAK,CAAd,EAAiB;AACf,mCAAK,IAAI,GAAC,GAAG,CAAb,EAAgB,GAAC,GAAG,SAAS,CAAC,MAA9B,EAAsC,EAAE,GAAxC,EAA2C;AACzC,gCAAA,IAAI,CAAC,IAAL,CAAU,MAAM,CAAC,CAAD,CAAhB;AACD;AACF;;AAED,gCAAM,SAAS,GAAG,OAAO,CAAC,CAAD,CAAP,CAAW,KAAX,CAAiB,CAAjB,CAAlB;;AAfqB,yDAgBZ,GAhBY;AAiBnB,kCAAM,QAAQ,GAAG,SAAS,CAAC,GAAD,CAA1B;AACA,kCAAM,SAAS,GAAG,IAAI,CAAC,GAAD,CAAtB;AACA,8BAAA,IAAI,CAAC,GAAD,CAAJ,GACI,GAAG,CAAC,IAAJ,CAAS;AAAA,uCAAM,GAAG,CAAC,GAAJ,CAAQ,IAAI,CAAC,GAAD,CAAZ,EAAiB,GAAG,CAAC,GAAJ,CAAQ,SAAR,EAAmB,QAAnB,CAAjB,CAAN;AAAA,+BAAT,CADJ;;AAEA,kCAAI,KAAK,GAAG,CAAZ,EAAe;AACb,gCAAA,GAAG,CAAC,OAAJ,CAAY,SAAZ;AACD;AAvBkB;;AAgBrB,iCAAK,IAAI,GAAC,GAAG,CAAb,EAAgB,GAAC,GAAG,SAAS,CAAC,MAA9B,EAAsC,EAAE,GAAxC,EAA2C;AAAA,qCAAlC,GAAkC;AAQ1C;;AACD,4BAAA,GAAG,CAAC,OAAJ,CAAY,SAAZ;AACA,4BAAA,WAAW,IAAI,SAAf;AAEA,8BAAE,KAAF;AA5BqB;AA6BtB;;AACD,+BAAO,IAAP;AACD,uBAhCM,CAAP;;AA3BG,2BA6DC,WAAW,CAAC,IA7Db;AAAA;AAAA;AAAA;;AA8DD,0BAAI,UAAJ,EAAgB;AACd,wBAAA,OAAO,CAAC,IAAR,CACI,qEACA,8CADA,GAEA,0CAFA,oCAG0B,IAAI,CAAC,OAH/B,mBAIA,0DAJA,GAKA,eANJ;AAOD;;AAtEA;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;;AAAA;AAAA,kBAyBE,UAAU,GAAG,KAAK,GAAG,IAAI,CAAC,OAAhB,GAA0B,IAzBtC;AAAA;AAAA;AAAA;;AAAA;;AAAA;AAAA;;AAAA;AAAA;AAAA;AAAA;;AAAA;;AAAA;AAAA;AAAA;;AAAA;AA2EL,iBAAS,CAAT,GAAa,CAAb,EAAgB,CAAC,GAAG,IAAI,CAAC,MAAzB,EAAiC,EAAE,CAAnC,EAAsC;AAC9B,cAAA,SAD8B,GAClB,IAAI,CAAC,CAAD,CADc;AAEpC,cAAA,IAAI,CAAC,CAAD,CAAJ,GAAU,GAAG,CAAC,GAAJ,CAAQ,IAAI,CAAC,CAAD,CAAZ,EAAiB,WAAjB,CAAV;AACA,cAAA,GAAG,CAAC,OAAJ,CAAY,SAAZ;AACD;;AA/EI,8CAiFE,gBAAgB,CAAC,IAAD,CAjFlB;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,G","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * Interfaces and methods for training models using TensorFlow.js datasets.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { scalar } from '@tensorflow/tfjs-core';\nimport { configureCallbacks, standardizeCallbacks } from '../base_callbacks';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { disposeTensorsInLogs } from '../logs';\nimport { singletonOrArray, toList } from '../utils/generic_utils';\nimport { standardizeClassWeights, standardizeWeights } from './training_utils';\n// Default batch size used during tensor-based validation.\nconst DEFAULT_VALIDATION_BATCH_SIZE = 32;\n/**\n * Standardize the output of a dataset iterator for use by\n * LayersModel.fitDataset().\n *\n * @param model: A `tf.LayersModel` object.\n * @param iteratorOut The output of a dataset iterator. It is required to be\n *   an object of the form `{xs: TensorOrArrayOrMap, ys:\n * TensorOrArrayOrMap}`, where `TensorOrArrayOrMap` is a single `tf.Tensor`,\n * a `tf.Tensor[]`, or a flat map from string names to `tf.Tensor`s.\n * @returns A flat array of `tf.Tensor` objects: the input `tf.Tensor`s\n *   followed by the target `tf.Tensor`s.  When `tf.Tensor`s are provided\n *   as a map, the order in the resulting array is taken from the `inputNames`\n *   and `outputNames` of the model.\n */\nfunction standardizeDataIteratorOutput(\n// Type `model` as `any` here to avoid circular dependency w/\n// training.ts.\n// tslint:disable-next-line:no-any\nmodel, iteratorOut) {\n    let xs;\n    let ys;\n    const iteratorOutObj = iteratorOut;\n    xs = iteratorOutObj['xs'];\n    ys = iteratorOutObj['ys'];\n    tfc.util.assert(xs != null && ys != null, () => 'A Dataset iterator for fitDataset() is expected to generate ' +\n        'objects of the form `{xs: xVal, ys: yVal}`, where the two ' +\n        'values may be `tf.Tensor`, an array of Tensors, or a map of ' +\n        'string to Tensor.  The provided Dataset instead generates ' +\n        `${iteratorOut}`);\n    const flattenedXs = flattenTensorOrArrayOrMap('input', model.inputNames, xs);\n    const flattenedYs = flattenTensorOrArrayOrMap('output', model.outputNames, ys);\n    const batchSize = flattenedXs[0].shape[0];\n    tfc.util.assert(flattenedXs.length === model.inputs.length, () => `LayersModel has ${model.inputs.length} inputs, but the dataset ` +\n        `provides ${flattenedXs.length} inputs.  (Expected input keys: ` +\n        `${JSON.stringify(model.inputNames)})`);\n    tfc.util.assert(flattenedYs.length === model.outputs.length, () => `LayersModel has ${model.outputs.length} outputs, but the dataset ` +\n        `provides ${flattenedYs.length} outputs.  (Expected output keys: ` +\n        `${JSON.stringify(model.outputNames)})`);\n    for (let xIndex = 0; xIndex < flattenedXs.length; xIndex++) {\n        tfc.util.assert(flattenedXs[xIndex].shape[0] === batchSize, () => `Batch size mismatch: input ` +\n            `${model.inputNames[xIndex]} has ${flattenedXs[xIndex].shape[0]}; ` +\n            `expected  ${batchSize} based on input ${model.inputNames[0]}.`);\n    }\n    for (let yIndex = 0; yIndex < flattenedYs.length; yIndex++) {\n        tfc.util.assert(flattenedYs[yIndex].shape[0] === batchSize, () => `Batch size mismatch: output ` +\n            `${model.outputNames[yIndex]} has ${flattenedYs[yIndex].shape[0]}; ` +\n            `expected  ${batchSize} based on input ${model.inputNames[0]}.`);\n    }\n    return { xs: flattenedXs, ys: flattenedYs };\n}\nfunction flattenTensorOrArrayOrMap(inputOrOutput, names, values) {\n    if (values instanceof tfc.Tensor) {\n        return [values];\n    }\n    else if (Array.isArray(values)) {\n        tfc.util.assert(values.length === names.length, () => `Received an array of ${values.length} Tensors, but expected ${names.length} to match the ${inputOrOutput} keys ${names}.`);\n        return values;\n    }\n    else {\n        const result = [];\n        // Check that all the required keys are available.\n        for (const name of names) {\n            if (values[name] == null) {\n                throw new ValueError(`The feature data generated by the dataset lacks the required ` +\n                    `${inputOrOutput} key '${name}'.`);\n            }\n            result.push(values[name]);\n        }\n        return result;\n    }\n}\nfunction standardizeTensorValidationData(data) {\n    if (data.length === 3) {\n        throw new NotImplementedError('Validation with sample weights is not implemented yet.');\n    }\n    return { xs: data[0], ys: data[1] };\n}\nexport async function fitDataset(\n// Type `model` as `any` here to avoid circular dependency w/\n// training.ts.\n// tslint:disable-next-line:no-any\nmodel, dataset, args) {\n    const hasBatchesPerEpoch = args.batchesPerEpoch != null;\n    tfc.util.assert(model.optimizer != null, () => 'You must compile a model before training/testing. Use ' +\n        'LayersModel.compile(modelCompileConfig).');\n    tfc.util.assert(args != null, () => `For fitDataset(), the 2nd argument (config) is required, ` +\n        `but it is not provided in this call.`);\n    tfc.util.assert(args.epochs != null && args.epochs > 0 && Number.isInteger(args.epochs), () => `For fitDataset(), config.epochs is expected to be a positive ` +\n        `integer, but got ${args.epochs}`);\n    tfc.util.assert(!hasBatchesPerEpoch ||\n        (args.batchesPerEpoch > 0 && Number.isInteger(args.batchesPerEpoch)), () => `For fitDataset(), config.batchesPerEpoch is expected to be a ` +\n        `positive integer if specified, but got ${args.batchesPerEpoch}`);\n    tfc.util.assert(\n    // tslint:disable-next-line:no-any\n    args['validationSplit'] == null, () => '`validationSplit` is not supported by `fitDataset()`. ' +\n        'Use validationData instead.');\n    if (model.isTraining) {\n        throw new Error('Cannot start training because another fit() call is ongoing.');\n    }\n    model.isTraining = true;\n    try {\n        const doValidation = args.validationData != null;\n        let valXs;\n        let valYs;\n        if (doValidation) {\n            if (isDatasetObject(args.validationData)) {\n                tfc.util.assert(args.validationBatches == null ||\n                    (args.validationBatches > 0 &&\n                        Number.isInteger(args.validationBatches)), () => `For fitDataset() with dataset-based validation, ` +\n                    `config.validationBatches is expected not to be provided, ` +\n                    `or to be a positive integer, ` +\n                    `but got ${args.validationBatches}`);\n            }\n            else {\n                const validationData = standardizeTensorValidationData(args.validationData);\n                valXs = validationData.xs;\n                valYs = validationData.ys;\n            }\n        }\n        const trainFunction = model.makeTrainFunction();\n        const outLabels = model.getDedupedMetricsNames();\n        let callbackMetrics;\n        if (doValidation) {\n            callbackMetrics =\n                outLabels.slice().concat(outLabels.map(n => 'val_' + n));\n        }\n        else {\n            callbackMetrics = outLabels.slice();\n        }\n        const callbacks = standardizeCallbacks(args.callbacks, args.yieldEvery);\n        const verbose = args.verbose == null ? 1 : args.verbose;\n        const { callbackList, history } = configureCallbacks(callbacks, verbose, args.epochs, null, null, getStepsPerEpoch(dataset, args), null, // Batch size determined by the dataset itself.\n        doValidation, callbackMetrics);\n        callbackList.setModel(model);\n        model.history = history;\n        await callbackList.onTrainBegin();\n        model.stopTraining_ = false;\n        let epoch = args.initialEpoch == null ? 0 : args.initialEpoch;\n        let dataIterator = await dataset.iterator();\n        while (epoch < args.epochs) {\n            const epochLogs = {};\n            await callbackList.onEpochBegin(epoch);\n            let stepsDone = 0;\n            let batchIndex = 0;\n            if (!hasBatchesPerEpoch) {\n                dataIterator = await dataset.iterator();\n            }\n            while (hasBatchesPerEpoch ? stepsDone < args.batchesPerEpoch : true) {\n                const iteratorOut = await dataIterator.next();\n                // If `batchesPerEpoch` is specified, the dataset should not be\n                // exhausted until all epoches are done.\n                if (hasBatchesPerEpoch && iteratorOut.done) {\n                    console.warn('You provided `batchesPerEpoch` as ' +\n                        `${args.batchesPerEpoch}, ` +\n                        'but your dataset iterator ran out of data after ' +\n                        `${stepsDone} batches; ` +\n                        'interrupting training. Make sure that your ' +\n                        'dataset can generate at least `batchesPerEpoch * epochs` ' +\n                        'batches (in this case, ' +\n                        `${args.batchesPerEpoch * args.epochs} batches). ` +\n                        'You may need to use the repeat() function when building ' +\n                        'your dataset.');\n                    break;\n                }\n                if (iteratorOut.value != null) {\n                    const { xs, ys } = standardizeDataIteratorOutput(model, iteratorOut.value);\n                    const batchLogs = {};\n                    batchLogs['batch'] = batchIndex;\n                    batchLogs['size'] = xs[0].shape[0];\n                    await callbackList.onBatchBegin(batchIndex, batchLogs);\n                    const sampleWeights = [];\n                    if (args.classWeight != null) {\n                        const standardClassWeights = standardizeClassWeights(args.classWeight, model.outputNames);\n                        for (let i = 0; i < standardClassWeights.length; ++i) {\n                            sampleWeights.push(await standardizeWeights(ys[i], null, standardClassWeights[i]));\n                        }\n                    }\n                    // Train on batch.\n                    const ins = xs.concat(ys).concat(sampleWeights);\n                    const outs = trainFunction(ins);\n                    tfc.dispose(ins);\n                    for (let i = 0; i < outLabels.length; ++i) {\n                        const label = outLabels[i];\n                        const out = outs[i];\n                        batchLogs[label] = out;\n                        tfc.keep(out);\n                    }\n                    await callbackList.onBatchEnd(batchIndex, batchLogs);\n                    disposeTensorsInLogs(batchLogs);\n                    batchIndex++;\n                    stepsDone++;\n                }\n                if (hasBatchesPerEpoch ? stepsDone >= args.batchesPerEpoch :\n                    iteratorOut.done) {\n                    // Epoch finished. Perform validation.\n                    if (doValidation) {\n                        let valOuts;\n                        if (isDatasetObject(args.validationData)) {\n                            valOuts = toList(await model.evaluateDataset(args.validationData, { batches: args.validationBatches }));\n                        }\n                        else {\n                            valOuts = toList(model.evaluate(valXs, valYs, {\n                                batchSize: args.validationBatchSize == null ?\n                                    DEFAULT_VALIDATION_BATCH_SIZE :\n                                    args.validationBatchSize,\n                                verbose: 0\n                            }));\n                        }\n                        for (let i = 0; i < model.metricsNames.length; ++i) {\n                            epochLogs[`val_${model.metricsNames[i]}`] = valOuts[i];\n                        }\n                    }\n                    // Call `break` to exit one epoch lopp after validation is done. If\n                    // config.batchesPerEpoch is specified, an epoch while loop will\n                    // stop when `stepsDone >= config.batchesPerEpoch`. When\n                    // config.batchesPerEpoch is not provided, the following `break` is\n                    // required to exit the while lopp after dataset is exhausted.\n                    break;\n                }\n                if (model.stopTraining_) {\n                    break;\n                }\n            }\n            await callbackList.onEpochEnd(epoch, epochLogs);\n            epoch++;\n            if (model.stopTraining_) {\n                break;\n            }\n        }\n        await callbackList.onTrainEnd();\n        await model.history.syncData();\n        return model.history;\n    }\n    finally {\n        model.isTraining = false;\n    }\n}\n/** Helper function that determines number of steps (batches) per epoch. */\nfunction getStepsPerEpoch(dataset, args) {\n    // Attempt to determine # of batches in an epoch.\n    let stepsPerEpoch = null;\n    if (args.batchesPerEpoch != null) {\n        stepsPerEpoch = args.batchesPerEpoch;\n    }\n    else if (Number.isFinite(dataset.size)) {\n        stepsPerEpoch = dataset.size;\n    }\n    return stepsPerEpoch;\n}\n// Check if provided object is a Dataset object by checking its .iterator\n// element.\nfunction isDatasetObject(dataset) {\n    return (typeof dataset.iterator === 'function');\n}\n// Check if provided object is a LazyIterator object by checking it's .next\n// element.\nfunction isLazyIteratorObject(iterator) {\n    return (typeof iterator.next === 'function');\n}\nexport async function evaluateDataset(\n// Type `model` as `any` here to avoid circular dependency w/\n// training.ts.\n// tslint:disable-next-line:no-any\nmodel, dataset, args) {\n    args = args || {};\n    const hasBatches = args.batches != null;\n    const f = model.testFunction;\n    let outs = [];\n    if (args.verbose > 0) {\n        throw new NotImplementedError('Verbose mode is not implemented yet.');\n    }\n    tfc.util.assert(!hasBatches || (args.batches > 0 && Number.isInteger(args.batches)), () => 'Test loop expects `batches` to be a positive integer, but ' +\n        `received ${JSON.stringify(args.batches)}`);\n    const dataIterator = isLazyIteratorObject(dataset) ?\n        dataset :\n        await dataset.iterator();\n    // Keeps track of number of examples used in this evaluation.\n    let numExamples = 0;\n    let batch = 0;\n    while (hasBatches ? batch < args.batches : true) {\n        const iteratorOut = await dataIterator.next();\n        outs = tfc.tidy(() => {\n            if (iteratorOut.value) {\n                // TODO(cais): Once real dataset is available, use\n                //   `map(x => standardizeDataIteratorOutput(model, x).map(f)`.\n                const { xs, ys } = standardizeDataIteratorOutput(model, iteratorOut.value);\n                const xsAndYs = xs.concat(ys);\n                const batchOuts = tfc.tidy(() => f(xsAndYs));\n                tfc.dispose(xsAndYs);\n                if (batch === 0) {\n                    for (let i = 0; i < batchOuts.length; ++i) {\n                        outs.push(scalar(0));\n                    }\n                }\n                const batchSize = xsAndYs[0].shape[0];\n                for (let i = 0; i < batchOuts.length; ++i) {\n                    const batchOut = batchOuts[i];\n                    const oldScalar = outs[i];\n                    outs[i] =\n                        tfc.tidy(() => tfc.add(outs[i], tfc.mul(batchSize, batchOut)));\n                    if (batch > 0) {\n                        tfc.dispose(oldScalar);\n                    }\n                }\n                tfc.dispose(batchOuts);\n                numExamples += batchSize;\n                ++batch;\n            }\n            return outs;\n        });\n        if (iteratorOut.done) {\n            if (hasBatches) {\n                console.warn('Your dataset iterator ran out of data during evaluateDataset(). ' +\n                    'Interrupting evalution. Make sure that your ' +\n                    'dataset can generate at least `batches` ' +\n                    `batches (in this case, ${args.batches} batches). ` +\n                    'You may need to use the repeat() function when building ' +\n                    'your dataset.');\n            }\n            break;\n        }\n    }\n    for (let i = 0; i < outs.length; ++i) {\n        const oldScalar = outs[i];\n        outs[i] = tfc.div(outs[i], numExamples);\n        tfc.dispose(oldScalar);\n    }\n    return singletonOrArray(outs);\n}\n//# sourceMappingURL=training_dataset.js.map"]},"metadata":{},"sourceType":"module"}