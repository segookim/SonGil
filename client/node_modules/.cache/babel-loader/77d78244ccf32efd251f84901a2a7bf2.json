{"ast":null,"code":"import _classCallCheck from \"/Users/kimkiwoong/SonGil/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/classCallCheck\";\nimport _createClass from \"/Users/kimkiwoong/SonGil/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/createClass\";\n\n/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { variableGrads } from '@tensorflow/tfjs-core';\nimport { getNextUniqueTensorId } from './backend/state';\nimport { getScopedTensorName, getUniqueTensorName } from './common';\nimport { NotImplementedError } from './errors';\nvar DEFAULT_VARIABLE_NAME_PREFIX = 'Variable';\n/**\n * A `tf.layers.LayerVariable` is similar to a `tf.Tensor` in that it has a\n * dtype and shape, but its value is mutable.  The value is itself represented\n * as a`tf.Tensor`, and can be read with the `read()` method and updated with\n * the `write()` method.\n */\n\nexport var LayerVariable = /*#__PURE__*/function () {\n  /**\n   * Construct Variable from a `tf.Tensor`.\n   *\n   * If not explicitly named, the Variable will be given a name with the\n   * prefix 'Variable'. Variable names are unique. In the case of name\n   * collision, suffixies '_<num>' will be added to the name.\n   *\n   * @param val Initial value of the Variable.\n   * @param name Name of the variable. If `null` or `undefined` is provided, it\n   *   will default a name with the prefix 'Variable'.\n   * @param constraint Optional, projection function to be applied to the\n   * variable after optimize updates\n   * @throws ValueError if `name` is `null` or `undefined`.\n   */\n  function LayerVariable(val) {\n    var dtype = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 'float32';\n    var name = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : DEFAULT_VARIABLE_NAME_PREFIX;\n    var trainable = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : true;\n    var constraint = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : null;\n\n    _classCallCheck(this, LayerVariable);\n\n    this.dtype = dtype == null ? 'float32' : dtype;\n    this.shape = val.shape;\n    this.id = getNextUniqueTensorId();\n    name = name == null ? DEFAULT_VARIABLE_NAME_PREFIX : name;\n    this.originalName = getScopedTensorName(name);\n    this.name = getUniqueTensorName(this.originalName);\n    this.trainable_ = trainable;\n    this.constraint = constraint;\n    this.val = tfc.variable(val, this.trainable_, this.name, this.dtype);\n  }\n  /**\n   * Get a snapshot of the Variable's value.\n   *\n   * The returned value is a snapshot of the Variable's value at the time of\n   * the invocation. Future mutations in the value of the tensor will only\n   * be reflected by future calls to this method.\n   */\n\n\n  _createClass(LayerVariable, [{\n    key: \"read\",\n    value: function read() {\n      this.assertNotDisposed();\n      return this.val;\n    }\n    /**\n     * Update the value of the Variable.\n     *\n     * @param newVal: The new value to update to. Must be consistent with the\n     *   dtype and shape of the Variable.\n     * @return This Variable.\n     */\n\n  }, {\n    key: \"write\",\n    value: function write(newVal) {\n      // TODO(cais): Once  TF.js Core supports Tensor.dtype, check dtype match.\n      this.assertNotDisposed();\n      checkShapesMatch(this.val, newVal); // Skip updating if this is the exact same tensor.\n\n      if (this.val.id !== newVal.id) {\n        this.val.assign(newVal);\n\n        if (this.constraint != null) {\n          this.val.assign(this.constraint.apply(this.val));\n        }\n      }\n\n      return this;\n    }\n    /**\n     * Dispose this LayersVariable instance from memory.\n     */\n\n  }, {\n    key: \"dispose\",\n    value: function dispose() {\n      this.assertNotDisposed();\n      this.val.dispose();\n    }\n  }, {\n    key: \"assertNotDisposed\",\n    value: function assertNotDisposed() {\n      if (this.val.isDisposed) {\n        throw new Error(\"LayersVariable \".concat(this.name, \" is already disposed.\"));\n      }\n    }\n  }, {\n    key: \"trainable\",\n    get: function get() {\n      return this.trainable_;\n    },\n    set: function set(trainable) {\n      this.trainable_ = trainable;\n      this.val.trainable = trainable;\n    }\n  }]);\n\n  return LayerVariable;\n}();\n\nfunction checkShapesMatch(x, y) {\n  if (x.shape.toString() !== y.shape.toString()) {\n    throw new Error('Shape mismatch: ' + JSON.stringify(x.shape) + ' vs. ' + JSON.stringify(y.shape));\n  }\n}\n/**\n * Create a Variable.\n * @param x The initial value of the `Variable`.\n * @param dtype optional, the type of the variable.\n * @param name optional, the name of the variable, default provided by\n * Variable.\n * @param constraint optional, a constraint to be applied after every update.\n * @return The newly instantiated `Variable`.\n */\n\n\nexport function variable(x, dtype, name, constraint) {\n  return new LayerVariable(x, dtype, name, true, constraint);\n}\n/**\n * Instantiates an all-zeros Variable and returns it.\n *\n * @param shape Shape of the tensor.\n * @param dtype DType of the tensor.\n * @param name Name of the tensor.\n * @return An all-zero Variable.\n */\n\nexport function zerosVariable(shape, dtype, name) {\n  // TODO(cais): Implement logic for dtype.\n  return new LayerVariable(tfc.zeros(shape), dtype, name);\n}\n/**\n * Instantiates an all-zeros tensor of the same shape as another tensor.\n *\n * @param x The other tensor.\n * @param dtype DType of the tensor.\n * @param name Name of the tensor.\n * @return A newly instantiated Variable.\n */\n\nexport function zerosLike(x, dtype, name) {\n  return new LayerVariable(tfc.zerosLike(x), dtype, name);\n}\n/**\n * Instantiates an all-ones tensor and returns it.\n *\n * @param shape Shape of the tensor.\n * @param dtype DType of the tensor.\n * @param name Name of the tensor.\n * @return An all-ones Variable.\n */\n\nexport function onesVariable(shape, dtype, name) {\n  // TODO(cais): Implement logic for dtype.\n  var allocated = tfc.ones(shape);\n  return new LayerVariable(allocated, dtype, name);\n}\n/**\n * Instantiates an all-ones tensor of the same shape as another tensor.\n *\n * @param x The other tensor.\n * @param dtype DType of the tensor.\n * @param name Name of the tensor.\n * @return A newly instantiated Variable.\n */\n\nexport function onesLike(x, dtype, name) {\n  var allocated = tfc.onesLike(x);\n  return new LayerVariable(allocated, dtype, name);\n}\n/**\n * Instantiate an identity matrix and returns it, as a Variable\n *\n * @param size Number of rows/columns.\n * @param dtype Data type of returned Variable.\n * @param name Name of returned Variable.\n * @return A Variable, an identity matrix.\n */\n\nexport function eyeVariable(size, dtype, name) {\n  return new LayerVariable(tfc.eye(size), dtype, name);\n}\n/**\n * Get a Variable with uniform distribution of values.\n * @param shape Shape of the tensor.\n * @param minval Lower bound of the uniform distribution.\n * @param maxval Upper bound of the uniform distribution.\n * @param dtype\n * @param seed\n * @param name Optional name.\n * @return The uniform-random Variable.\n */\n\nexport function randomUniformVariable(shape, minval, maxval, dtype, seed) {\n  var name = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : 'randomUniform';\n  return new LayerVariable(tfc.randomUniform(shape, minval, maxval, dtype), dtype, name);\n}\n/**\n * Get a Variable with truncated-normal distribution of values.\n * @param shape Shape of the tensor.\n * @param mean mean value of the normal distribution.\n * @param stddev standard deviation of the normal distribution.\n * @param dtype\n * @param seed\n * @param name Optional name.\n * @return The truncated-normal-random Variable.\n */\n\nexport function truncatedNormalVariable(shape) {\n  var mean = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0.0;\n  var stddev = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 1.0;\n  var dtype = arguments.length > 3 ? arguments[3] : undefined;\n  var seed = arguments.length > 4 ? arguments[4] : undefined;\n  var name = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : 'truncatedNormal';\n  // TODO(cais): Implement logic for dtype and seed once they are supported\n  // by deeplearn.js.\n  dtype = dtype || 'float32';\n\n  if (dtype !== 'float32' && dtype !== 'int32') {\n    throw new NotImplementedError(\"randomNormal does not support dType \".concat(dtype, \".\"));\n  }\n\n  return new LayerVariable(tfc.truncatedNormal(shape, mean, stddev, dtype, seed), dtype, name);\n}\n/**\n * Get a Variable with normal distribution of values.\n * @param shape Shape of the tensor.\n * @param mean mean value of the normal distribution.\n * @param stddev standard deviation of the normal distribution.\n * @param dtype\n * @param seed\n * @param name Optional name.\n * @return The truncated-normal-random Variable.\n */\n\nexport function randomNormalVariable(shape) {\n  var mean = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0.0;\n  var stddev = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 1.0;\n  var dtype = arguments.length > 3 ? arguments[3] : undefined;\n  var seed = arguments.length > 4 ? arguments[4] : undefined;\n  var name = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : 'randomNormal';\n  dtype = dtype || 'float32';\n\n  if (dtype !== 'float32' && dtype !== 'int32') {\n    throw new NotImplementedError(\"randomNormalVariable does not support dType \".concat(dtype, \".\"));\n  }\n\n  return new LayerVariable(tfc.randomNormal(shape, mean, stddev, dtype, seed), dtype, name);\n}\n/**\n * Update the value of a Variable.\n * @param x The Variable to be updated.\n * @param xNew The new value to update to.\n * @return The Variable updated.\n */\n\nexport function update(x, xNew) {\n  return x.write(xNew);\n}\n/**\n * Update the value of a Variable by adding an increment.\n * @param x The Variable to be updated.\n * @param increment The incrment to add to `x`.\n * @return The Variable updated.\n */\n\nexport function updateAdd(x, increment) {\n  return x.write(tfc.add(x.read(), increment));\n}\n/**\n * Update the value of a Variable by subtracting a decrement.\n * @param x The Variable to be updated.\n * @param decrement The decrement to subtract from `x`.\n * @return The Variable updated.\n */\n\nexport function updateSub(x, decrement) {\n  return x.write(tfc.sub(x.read(), decrement));\n}\n/**\n * Get the values of an array of Variables.\n *\n * @param tensors An `Array` of `Variable`s to get the values of.\n * @return The values of the inputs, as an `Array` of`tf.Tensor`s.\n */\n\nexport function batchGetValue(xs) {\n  return xs.map(function (x) {\n    return x.read();\n  });\n}\n/**\n * Update the value of multiple Variables at once.\n *\n * @param variablesAndValues An `Array`, each element is of type\n *   [Variable, Tensor]. The first item is the\n *   `Variable` of which the value is to be updated. The second item\n *   carries the new value.\n */\n\nexport function batchSetValue(variablesAndValues) {\n  variablesAndValues.forEach(function (variableAndValue) {\n    var variable = variableAndValue[0];\n    variable.write(variableAndValue[1]);\n  });\n}\n/**\n * Returns the gradients of `variables` w.r.t. the return value of `lossFn`.\n * @param lossFn A function which returns a Scalar to be used as the function\n *   value (i.e., numerator) for differentiation.\n * @param variables List of variables to be used as the independent variables\n *   (i.e., denominator) for differentiation.\n * @returns An Array of gradients tensors.\n */\n\nexport function gradients(lossFn, variables) {\n  // TODO(cais): The return type signature can be simplified if deeplearn makes\n  //   the corresponding type public.\n  var variableList = variables.map(function (variable) {\n    return variable.read();\n  });\n  var valudAndGrads = variableGrads(lossFn, variableList);\n  return variables.map(function (variable) {\n    return valudAndGrads.grads[variable.name];\n  });\n}","map":{"version":3,"sources":["../src/variables.ts"],"names":[],"mappings":";;;AAAA;;;;;;;;AAQG;AAEH,OAAO,KAAK,GAAZ,MAAqB,uBAArB;AACA,SAA0B,aAA1B,QAA8C,uBAA9C;AAEA,SAAQ,qBAAR,QAAoC,iBAApC;AACA,SAAQ,mBAAR,EAA6B,mBAA7B,QAAuD,UAAvD;AAEA,SAAQ,mBAAR,QAAkC,UAAlC;AAIA,IAAM,4BAA4B,GAAG,UAArC;AAEA;;;;;AAKG;;AACH,WAAa,aAAb;AAeE;;;;;;;;;;;;;AAaG;AACH,yBACI,GADJ,EAGiC;AAAA,QAFhB,KAEgB,uEAFE,SAEF;AAAA,QAD7B,IAC6B,uEADtB,4BACsB;AAAA,QADQ,SACR,uEADoB,IACpB;AAAA,QAA7B,UAA6B,uEAAJ,IAAI;;AAAA;;AAC/B,SAAK,KAAL,GAAa,KAAK,IAAI,IAAT,GAAgB,SAAhB,GAA4B,KAAzC;AACA,SAAK,KAAL,GAAa,GAAG,CAAC,KAAjB;AACA,SAAK,EAAL,GAAU,qBAAqB,EAA/B;AAEA,IAAA,IAAI,GAAG,IAAI,IAAI,IAAR,GAAe,4BAAf,GAA8C,IAArD;AACA,SAAK,YAAL,GAAoB,mBAAmB,CAAC,IAAD,CAAvC;AACA,SAAK,IAAL,GAAY,mBAAmB,CAAC,KAAK,YAAN,CAA/B;AAEA,SAAK,UAAL,GAAkB,SAAlB;AACA,SAAK,UAAL,GAAkB,UAAlB;AAEA,SAAK,GAAL,GAAW,GAAG,CAAC,QAAJ,CAAa,GAAb,EAAkB,KAAK,UAAvB,EAAmC,KAAK,IAAxC,EAA8C,KAAK,KAAnD,CAAX;AACD;AAED;;;;;;AAMG;;;AArDL;AAAA;AAAA,WAsDE,gBAAI;AACF,WAAK,iBAAL;AACA,aAAO,KAAK,GAAZ;AACD;AAED;;;;;;AAMG;;AAjEL;AAAA;AAAA,WAkEE,eAAM,MAAN,EAAoB;AAClB;AACA,WAAK,iBAAL;AACA,MAAA,gBAAgB,CAAC,KAAK,GAAN,EAAW,MAAX,CAAhB,CAHkB,CAIlB;;AACA,UAAI,KAAK,GAAL,CAAS,EAAT,KAAgB,MAAM,CAAC,EAA3B,EAA+B;AAC7B,aAAK,GAAL,CAAS,MAAT,CAAgB,MAAhB;;AACA,YAAI,KAAK,UAAL,IAAmB,IAAvB,EAA6B;AAC3B,eAAK,GAAL,CAAS,MAAT,CAAgB,KAAK,UAAL,CAAgB,KAAhB,CAAsB,KAAK,GAA3B,CAAhB;AACD;AACF;;AACD,aAAO,IAAP;AACD;AAED;;AAEG;;AAlFL;AAAA;AAAA,WAmFE,mBAAO;AACL,WAAK,iBAAL;AACA,WAAK,GAAL,CAAS,OAAT;AACD;AAtFH;AAAA;AAAA,WAwFY,6BAAiB;AACzB,UAAI,KAAK,GAAL,CAAS,UAAb,EAAyB;AACvB,cAAM,IAAI,KAAJ,0BAA4B,KAAK,IAAjC,2BAAN;AACD;AACF;AA5FH;AAAA;AAAA,SA8FE,eAAa;AACX,aAAO,KAAK,UAAZ;AACD,KAhGH;AAAA,SAkGE,aAAc,SAAd,EAAgC;AAC9B,WAAK,UAAL,GAAkB,SAAlB;AACA,WAAK,GAAL,CAAS,SAAT,GAAqB,SAArB;AACD;AArGH;;AAAA;AAAA;;AAwGA,SAAS,gBAAT,CAA0B,CAA1B,EAAuC,CAAvC,EAAkD;AAChD,MAAI,CAAC,CAAC,KAAF,CAAQ,QAAR,OAAuB,CAAC,CAAC,KAAF,CAAQ,QAAR,EAA3B,EAA+C;AAC7C,UAAM,IAAI,KAAJ,CACF,qBAAqB,IAAI,CAAC,SAAL,CAAe,CAAC,CAAC,KAAjB,CAArB,GAA+C,OAA/C,GACA,IAAI,CAAC,SAAL,CAAe,CAAC,CAAC,KAAjB,CAFE,CAAN;AAGD;AACF;AAED;;;;;;;;AAQG;;;AACH,OAAM,SAAU,QAAV,CACF,CADE,EACS,KADT,EAC2B,IAD3B,EAEF,UAFE,EAEqB;AACzB,SAAO,IAAI,aAAJ,CAAkB,CAAlB,EAAqB,KAArB,EAA4B,IAA5B,EAAkC,IAAlC,EAAwC,UAAxC,CAAP;AACD;AAED;;;;;;;AAOG;;AACH,OAAM,SAAU,aAAV,CACF,KADE,EACY,KADZ,EAC8B,IAD9B,EAC2C;AAC/C;AACA,SAAO,IAAI,aAAJ,CAAkB,GAAG,CAAC,KAAJ,CAAU,KAAV,CAAlB,EAAoC,KAApC,EAA2C,IAA3C,CAAP;AACD;AAED;;;;;;;AAOG;;AACH,OAAM,SAAU,SAAV,CACF,CADE,EACS,KADT,EAC2B,IAD3B,EACwC;AAC5C,SAAO,IAAI,aAAJ,CAAkB,GAAG,CAAC,SAAJ,CAAc,CAAd,CAAlB,EAAoC,KAApC,EAA2C,IAA3C,CAAP;AACD;AAED;;;;;;;AAOG;;AACH,OAAM,SAAU,YAAV,CACF,KADE,EACY,KADZ,EAC8B,IAD9B,EAC2C;AAC/C;AACA,MAAM,SAAS,GAAG,GAAG,CAAC,IAAJ,CAAS,KAAT,CAAlB;AACA,SAAO,IAAI,aAAJ,CAAkB,SAAlB,EAA6B,KAA7B,EAAoC,IAApC,CAAP;AACD;AAED;;;;;;;AAOG;;AACH,OAAM,SAAU,QAAV,CACF,CADE,EACS,KADT,EAC2B,IAD3B,EACwC;AAC5C,MAAM,SAAS,GAAG,GAAG,CAAC,QAAJ,CAAa,CAAb,CAAlB;AACA,SAAO,IAAI,aAAJ,CAAkB,SAAlB,EAA6B,KAA7B,EAAoC,IAApC,CAAP;AACD;AAED;;;;;;;AAOG;;AACH,OAAM,SAAU,WAAV,CACF,IADE,EACY,KADZ,EAC8B,IAD9B,EAC2C;AAC/C,SAAO,IAAI,aAAJ,CAAkB,GAAG,CAAC,GAAJ,CAAQ,IAAR,CAAlB,EAAiC,KAAjC,EAAwC,IAAxC,CAAP;AACD;AAED;;;;;;;;;AASG;;AACH,OAAM,SAAU,qBAAV,CACF,KADE,EACY,MADZ,EAC4B,MAD5B,EAC4C,KAD5C,EAEF,IAFE,EAEmC;AAAA,MAAtB,IAAsB,uEAAf,eAAe;AACvC,SAAO,IAAI,aAAJ,CACH,GAAG,CAAC,aAAJ,CAAkB,KAAlB,EAAyB,MAAzB,EAAiC,MAAjC,EAAyC,KAAzC,CADG,EAC8C,KAD9C,EACqD,IADrD,CAAP;AAED;AAED;;;;;;;;;AASG;;AACH,OAAM,SAAU,uBAAV,CACF,KADE,EAEsB;AAAA,MADV,IACU,uEADH,GACG;AAAA,MADE,MACF,uEADW,GACX;AAAA,MADgB,KAChB;AAAA,MADkC,IAClC;AAAA,MAAxB,IAAwB,uEAAjB,iBAAiB;AAC1B;AACA;AACA,EAAA,KAAK,GAAG,KAAK,IAAI,SAAjB;;AACA,MAAI,KAAK,KAAK,SAAV,IAAuB,KAAK,KAAK,OAArC,EAA8C;AAC5C,UAAM,IAAI,mBAAJ,+CACqC,KADrC,OAAN;AAED;;AACD,SAAO,IAAI,aAAJ,CACH,GAAG,CAAC,eAAJ,CAAoB,KAApB,EAA2B,IAA3B,EAAiC,MAAjC,EAAyC,KAAzC,EAAgD,IAAhD,CADG,EACoD,KADpD,EAC2D,IAD3D,CAAP;AAED;AACD;;;;;;;;;AASG;;AACH,OAAM,SAAU,oBAAV,CACF,KADE,EAEmB;AAAA,MADP,IACO,uEADA,GACA;AAAA,MADK,MACL,uEADc,GACd;AAAA,MADmB,KACnB;AAAA,MADqC,IACrC;AAAA,MAArB,IAAqB,uEAAd,cAAc;AACvB,EAAA,KAAK,GAAG,KAAK,IAAI,SAAjB;;AACA,MAAI,KAAK,KAAK,SAAV,IAAuB,KAAK,KAAK,OAArC,EAA8C;AAC5C,UAAM,IAAI,mBAAJ,uDAC6C,KAD7C,OAAN;AAED;;AACD,SAAO,IAAI,aAAJ,CACH,GAAG,CAAC,YAAJ,CAAiB,KAAjB,EAAwB,IAAxB,EAA8B,MAA9B,EAAsC,KAAtC,EAA6C,IAA7C,CADG,EACiD,KADjD,EACwD,IADxD,CAAP;AAED;AAED;;;;;AAKG;;AACH,OAAM,SAAU,MAAV,CAAiB,CAAjB,EAAmC,IAAnC,EAA+C;AACnD,SAAO,CAAC,CAAC,KAAF,CAAQ,IAAR,CAAP;AACD;AAED;;;;;AAKG;;AACH,OAAM,SAAU,SAAV,CAAoB,CAApB,EAAsC,SAAtC,EAAuD;AAC3D,SAAO,CAAC,CAAC,KAAF,CAAQ,GAAG,CAAC,GAAJ,CAAQ,CAAC,CAAC,IAAF,EAAR,EAAkB,SAAlB,CAAR,CAAP;AACD;AAED;;;;;AAKG;;AACH,OAAM,SAAU,SAAV,CAAoB,CAApB,EAAsC,SAAtC,EAAuD;AAC3D,SAAO,CAAC,CAAC,KAAF,CAAQ,GAAG,CAAC,GAAJ,CAAQ,CAAC,CAAC,IAAF,EAAR,EAAkB,SAAlB,CAAR,CAAP;AACD;AAED;;;;;AAKG;;AACH,OAAM,SAAU,aAAV,CAAwB,EAAxB,EAA2C;AAC/C,SAAO,EAAE,CAAC,GAAH,CAAO,UAAA,CAAC;AAAA,WAAI,CAAC,CAAC,IAAF,EAAJ;AAAA,GAAR,CAAP;AACD;AAED;;;;;;;AAOG;;AACH,OAAM,SAAU,aAAV,CACF,kBADE,EACgD;AACpD,EAAA,kBAAkB,CAAC,OAAnB,CAA2B,UAAA,gBAAgB,EAAG;AAC5C,QAAM,QAAQ,GAAkB,gBAAgB,CAAC,CAAD,CAAhD;AACA,IAAA,QAAQ,CAAC,KAAT,CAAe,gBAAgB,CAAC,CAAD,CAA/B;AACD,GAHD;AAID;AAED;;;;;;;AAOG;;AACH,OAAM,SAAU,SAAV,CACF,MADE,EACwB,SADxB,EACkD;AACtD;AACA;AACA,MAAM,YAAY,GACd,SAAS,CAAC,GAAV,CAAc,UAAA,QAAQ;AAAA,WAAI,QAAQ,CAAC,IAAT,EAAJ;AAAA,GAAtB,CADJ;AAEA,MAAM,aAAa,GAAG,aAAa,CAAC,MAAD,EAAS,YAAT,CAAnC;AACA,SAAO,SAAS,CAAC,GAAV,CAAc,UAAA,QAAQ;AAAA,WAAI,aAAa,CAAC,KAAd,CAAoB,QAAQ,CAAC,IAA7B,CAAJ;AAAA,GAAtB,CAAP;AACD","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { variableGrads } from '@tensorflow/tfjs-core';\nimport { getNextUniqueTensorId } from './backend/state';\nimport { getScopedTensorName, getUniqueTensorName } from './common';\nimport { NotImplementedError } from './errors';\nconst DEFAULT_VARIABLE_NAME_PREFIX = 'Variable';\n/**\n * A `tf.layers.LayerVariable` is similar to a `tf.Tensor` in that it has a\n * dtype and shape, but its value is mutable.  The value is itself represented\n * as a`tf.Tensor`, and can be read with the `read()` method and updated with\n * the `write()` method.\n */\nexport class LayerVariable {\n    /**\n     * Construct Variable from a `tf.Tensor`.\n     *\n     * If not explicitly named, the Variable will be given a name with the\n     * prefix 'Variable'. Variable names are unique. In the case of name\n     * collision, suffixies '_<num>' will be added to the name.\n     *\n     * @param val Initial value of the Variable.\n     * @param name Name of the variable. If `null` or `undefined` is provided, it\n     *   will default a name with the prefix 'Variable'.\n     * @param constraint Optional, projection function to be applied to the\n     * variable after optimize updates\n     * @throws ValueError if `name` is `null` or `undefined`.\n     */\n    constructor(val, dtype = 'float32', name = DEFAULT_VARIABLE_NAME_PREFIX, trainable = true, constraint = null) {\n        this.dtype = dtype == null ? 'float32' : dtype;\n        this.shape = val.shape;\n        this.id = getNextUniqueTensorId();\n        name = name == null ? DEFAULT_VARIABLE_NAME_PREFIX : name;\n        this.originalName = getScopedTensorName(name);\n        this.name = getUniqueTensorName(this.originalName);\n        this.trainable_ = trainable;\n        this.constraint = constraint;\n        this.val = tfc.variable(val, this.trainable_, this.name, this.dtype);\n    }\n    /**\n     * Get a snapshot of the Variable's value.\n     *\n     * The returned value is a snapshot of the Variable's value at the time of\n     * the invocation. Future mutations in the value of the tensor will only\n     * be reflected by future calls to this method.\n     */\n    read() {\n        this.assertNotDisposed();\n        return this.val;\n    }\n    /**\n     * Update the value of the Variable.\n     *\n     * @param newVal: The new value to update to. Must be consistent with the\n     *   dtype and shape of the Variable.\n     * @return This Variable.\n     */\n    write(newVal) {\n        // TODO(cais): Once  TF.js Core supports Tensor.dtype, check dtype match.\n        this.assertNotDisposed();\n        checkShapesMatch(this.val, newVal);\n        // Skip updating if this is the exact same tensor.\n        if (this.val.id !== newVal.id) {\n            this.val.assign(newVal);\n            if (this.constraint != null) {\n                this.val.assign(this.constraint.apply(this.val));\n            }\n        }\n        return this;\n    }\n    /**\n     * Dispose this LayersVariable instance from memory.\n     */\n    dispose() {\n        this.assertNotDisposed();\n        this.val.dispose();\n    }\n    assertNotDisposed() {\n        if (this.val.isDisposed) {\n            throw new Error(`LayersVariable ${this.name} is already disposed.`);\n        }\n    }\n    get trainable() {\n        return this.trainable_;\n    }\n    set trainable(trainable) {\n        this.trainable_ = trainable;\n        this.val.trainable = trainable;\n    }\n}\nfunction checkShapesMatch(x, y) {\n    if (x.shape.toString() !== y.shape.toString()) {\n        throw new Error('Shape mismatch: ' + JSON.stringify(x.shape) + ' vs. ' +\n            JSON.stringify(y.shape));\n    }\n}\n/**\n * Create a Variable.\n * @param x The initial value of the `Variable`.\n * @param dtype optional, the type of the variable.\n * @param name optional, the name of the variable, default provided by\n * Variable.\n * @param constraint optional, a constraint to be applied after every update.\n * @return The newly instantiated `Variable`.\n */\nexport function variable(x, dtype, name, constraint) {\n    return new LayerVariable(x, dtype, name, true, constraint);\n}\n/**\n * Instantiates an all-zeros Variable and returns it.\n *\n * @param shape Shape of the tensor.\n * @param dtype DType of the tensor.\n * @param name Name of the tensor.\n * @return An all-zero Variable.\n */\nexport function zerosVariable(shape, dtype, name) {\n    // TODO(cais): Implement logic for dtype.\n    return new LayerVariable(tfc.zeros(shape), dtype, name);\n}\n/**\n * Instantiates an all-zeros tensor of the same shape as another tensor.\n *\n * @param x The other tensor.\n * @param dtype DType of the tensor.\n * @param name Name of the tensor.\n * @return A newly instantiated Variable.\n */\nexport function zerosLike(x, dtype, name) {\n    return new LayerVariable(tfc.zerosLike(x), dtype, name);\n}\n/**\n * Instantiates an all-ones tensor and returns it.\n *\n * @param shape Shape of the tensor.\n * @param dtype DType of the tensor.\n * @param name Name of the tensor.\n * @return An all-ones Variable.\n */\nexport function onesVariable(shape, dtype, name) {\n    // TODO(cais): Implement logic for dtype.\n    const allocated = tfc.ones(shape);\n    return new LayerVariable(allocated, dtype, name);\n}\n/**\n * Instantiates an all-ones tensor of the same shape as another tensor.\n *\n * @param x The other tensor.\n * @param dtype DType of the tensor.\n * @param name Name of the tensor.\n * @return A newly instantiated Variable.\n */\nexport function onesLike(x, dtype, name) {\n    const allocated = tfc.onesLike(x);\n    return new LayerVariable(allocated, dtype, name);\n}\n/**\n * Instantiate an identity matrix and returns it, as a Variable\n *\n * @param size Number of rows/columns.\n * @param dtype Data type of returned Variable.\n * @param name Name of returned Variable.\n * @return A Variable, an identity matrix.\n */\nexport function eyeVariable(size, dtype, name) {\n    return new LayerVariable(tfc.eye(size), dtype, name);\n}\n/**\n * Get a Variable with uniform distribution of values.\n * @param shape Shape of the tensor.\n * @param minval Lower bound of the uniform distribution.\n * @param maxval Upper bound of the uniform distribution.\n * @param dtype\n * @param seed\n * @param name Optional name.\n * @return The uniform-random Variable.\n */\nexport function randomUniformVariable(shape, minval, maxval, dtype, seed, name = 'randomUniform') {\n    return new LayerVariable(tfc.randomUniform(shape, minval, maxval, dtype), dtype, name);\n}\n/**\n * Get a Variable with truncated-normal distribution of values.\n * @param shape Shape of the tensor.\n * @param mean mean value of the normal distribution.\n * @param stddev standard deviation of the normal distribution.\n * @param dtype\n * @param seed\n * @param name Optional name.\n * @return The truncated-normal-random Variable.\n */\nexport function truncatedNormalVariable(shape, mean = 0.0, stddev = 1.0, dtype, seed, name = 'truncatedNormal') {\n    // TODO(cais): Implement logic for dtype and seed once they are supported\n    // by deeplearn.js.\n    dtype = dtype || 'float32';\n    if (dtype !== 'float32' && dtype !== 'int32') {\n        throw new NotImplementedError(`randomNormal does not support dType ${dtype}.`);\n    }\n    return new LayerVariable(tfc.truncatedNormal(shape, mean, stddev, dtype, seed), dtype, name);\n}\n/**\n * Get a Variable with normal distribution of values.\n * @param shape Shape of the tensor.\n * @param mean mean value of the normal distribution.\n * @param stddev standard deviation of the normal distribution.\n * @param dtype\n * @param seed\n * @param name Optional name.\n * @return The truncated-normal-random Variable.\n */\nexport function randomNormalVariable(shape, mean = 0.0, stddev = 1.0, dtype, seed, name = 'randomNormal') {\n    dtype = dtype || 'float32';\n    if (dtype !== 'float32' && dtype !== 'int32') {\n        throw new NotImplementedError(`randomNormalVariable does not support dType ${dtype}.`);\n    }\n    return new LayerVariable(tfc.randomNormal(shape, mean, stddev, dtype, seed), dtype, name);\n}\n/**\n * Update the value of a Variable.\n * @param x The Variable to be updated.\n * @param xNew The new value to update to.\n * @return The Variable updated.\n */\nexport function update(x, xNew) {\n    return x.write(xNew);\n}\n/**\n * Update the value of a Variable by adding an increment.\n * @param x The Variable to be updated.\n * @param increment The incrment to add to `x`.\n * @return The Variable updated.\n */\nexport function updateAdd(x, increment) {\n    return x.write(tfc.add(x.read(), increment));\n}\n/**\n * Update the value of a Variable by subtracting a decrement.\n * @param x The Variable to be updated.\n * @param decrement The decrement to subtract from `x`.\n * @return The Variable updated.\n */\nexport function updateSub(x, decrement) {\n    return x.write(tfc.sub(x.read(), decrement));\n}\n/**\n * Get the values of an array of Variables.\n *\n * @param tensors An `Array` of `Variable`s to get the values of.\n * @return The values of the inputs, as an `Array` of`tf.Tensor`s.\n */\nexport function batchGetValue(xs) {\n    return xs.map(x => x.read());\n}\n/**\n * Update the value of multiple Variables at once.\n *\n * @param variablesAndValues An `Array`, each element is of type\n *   [Variable, Tensor]. The first item is the\n *   `Variable` of which the value is to be updated. The second item\n *   carries the new value.\n */\nexport function batchSetValue(variablesAndValues) {\n    variablesAndValues.forEach(variableAndValue => {\n        const variable = variableAndValue[0];\n        variable.write(variableAndValue[1]);\n    });\n}\n/**\n * Returns the gradients of `variables` w.r.t. the return value of `lossFn`.\n * @param lossFn A function which returns a Scalar to be used as the function\n *   value (i.e., numerator) for differentiation.\n * @param variables List of variables to be used as the independent variables\n *   (i.e., denominator) for differentiation.\n * @returns An Array of gradients tensors.\n */\nexport function gradients(lossFn, variables) {\n    // TODO(cais): The return type signature can be simplified if deeplearn makes\n    //   the corresponding type public.\n    const variableList = variables.map(variable => variable.read());\n    const valudAndGrads = variableGrads(lossFn, variableList);\n    return variables.map(variable => valudAndGrads.grads[variable.name]);\n}\n//# sourceMappingURL=variables.js.map"]},"metadata":{},"sourceType":"module"}