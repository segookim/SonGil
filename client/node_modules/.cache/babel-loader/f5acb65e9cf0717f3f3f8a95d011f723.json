{"ast":null,"code":"/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport * as broadcast_util from './broadcast_util';\nimport { elu } from './elu';\nimport { leakyRelu } from './leaky_relu';\nimport { mul } from './mul';\nimport { prelu } from './prelu';\nimport { relu } from './relu';\nimport { relu6 } from './relu6';\nimport { reshape } from './reshape';\nimport { step } from './step';\nimport { sum } from './sum'; // Returns gradient for fused activation.\n\nexport function getFusedDyActivation(dy, y, activation) {\n  if (activation == null || activation === 'linear') {\n    return dy;\n  }\n\n  if (activation === 'relu') {\n    return mul(dy, step(y));\n  }\n\n  throw new Error(`Cannot compute gradient for fused activation ${activation}.`);\n} // Returns gradient for fused bias.\n\nexport function getFusedBiasGradient(bias, dyActivation) {\n  let res = dyActivation;\n  const reduceAxes = broadcast_util.getReductionAxes(bias.shape, dyActivation.shape);\n\n  if (reduceAxes.length > 0) {\n    res = sum(res, reduceAxes);\n  }\n\n  return reshape(res, bias.shape);\n}\nexport function applyActivation(x, activation, preluActivationWeights, leakyreluAlpha) {\n  if (activation === 'linear') {\n    return x;\n  } else if (activation === 'relu') {\n    return relu(x);\n  } else if (activation === 'elu') {\n    return elu(x);\n  } else if (activation === 'relu6') {\n    return relu6(x);\n  } else if (activation === 'prelu') {\n    return prelu(x, preluActivationWeights);\n  } else if (activation === 'leakyrelu') {\n    return leakyRelu(x, leakyreluAlpha);\n  }\n\n  throw new Error(`Unknown fused activation ${activation}.`);\n} // Whether we should call fused ops.\n\nexport const shouldFuse = (gradientDepth, activation) => {\n  const gradientMode = gradientDepth > 0;\n  return !gradientMode || activation === 'linear';\n};","map":{"version":3,"sources":["../../src/ops/fused_util.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;AAeG;AAIH,OAAO,KAAK,cAAZ,MAAgC,kBAAhC;AACA,SAAQ,GAAR,QAAkB,OAAlB;AAEA,SAAQ,SAAR,QAAwB,cAAxB;AACA,SAAQ,GAAR,QAAkB,OAAlB;AACA,SAAQ,KAAR,QAAoB,SAApB;AACA,SAAQ,IAAR,QAAmB,QAAnB;AACA,SAAQ,KAAR,QAAoB,SAApB;AACA,SAAQ,OAAR,QAAsB,WAAtB;AACA,SAAQ,IAAR,QAAmB,QAAnB;AACA,SAAQ,GAAR,QAAkB,OAAlB,C,CAEA;;AACA,OAAM,SAAU,oBAAV,CACF,EADE,EACU,CADV,EACqB,UADrB,EAC2C;AAC/C,MAAI,UAAU,IAAI,IAAd,IAAsB,UAAU,KAAK,QAAzC,EAAmD;AACjD,WAAO,EAAP;AACD;;AACD,MAAI,UAAU,KAAK,MAAnB,EAA2B;AACzB,WAAO,GAAG,CAAC,EAAD,EAAK,IAAI,CAAC,CAAD,CAAT,CAAV;AACD;;AACD,QAAM,IAAI,KAAJ,CACF,gDAAgD,UAAU,GADxD,CAAN;AAED,C,CAED;;AACA,OAAM,SAAU,oBAAV,CACF,IADE,EACY,YADZ,EACgC;AACpC,MAAI,GAAG,GAAG,YAAV;AACA,QAAM,UAAU,GACZ,cAAc,CAAC,gBAAf,CAAgC,IAAI,CAAC,KAArC,EAA4C,YAAY,CAAC,KAAzD,CADJ;;AAEA,MAAI,UAAU,CAAC,MAAX,GAAoB,CAAxB,EAA2B;AACzB,IAAA,GAAG,GAAG,GAAG,CAAC,GAAD,EAAM,UAAN,CAAT;AACD;;AACD,SAAO,OAAO,CAAC,GAAD,EAAM,IAAI,CAAC,KAAX,CAAd;AACD;AAED,OAAM,SAAU,eAAV,CACF,CADE,EACS,UADT,EACiC,sBADjC,EAEF,cAFE,EAEqB;AACzB,MAAI,UAAU,KAAK,QAAnB,EAA6B;AAC3B,WAAO,CAAP;AACD,GAFD,MAEO,IAAI,UAAU,KAAK,MAAnB,EAA2B;AAChC,WAAO,IAAI,CAAC,CAAD,CAAX;AACD,GAFM,MAEA,IAAI,UAAU,KAAK,KAAnB,EAA0B;AAC/B,WAAO,GAAG,CAAC,CAAD,CAAV;AACD,GAFM,MAEA,IAAI,UAAU,KAAK,OAAnB,EAA4B;AACjC,WAAO,KAAK,CAAC,CAAD,CAAZ;AACD,GAFM,MAEA,IAAI,UAAU,KAAK,OAAnB,EAA4B;AACjC,WAAO,KAAK,CAAC,CAAD,EAAI,sBAAJ,CAAZ;AACD,GAFM,MAEA,IAAI,UAAU,KAAK,WAAnB,EAAgC;AACrC,WAAO,SAAS,CAAC,CAAD,EAAI,cAAJ,CAAhB;AACD;;AACD,QAAM,IAAI,KAAJ,CAAU,4BAA4B,UAAU,GAAhD,CAAN;AACD,C,CAED;;AACA,OAAO,MAAM,UAAU,GAAG,CAAC,aAAD,EAAwB,UAAxB,KAAkD;AAC1E,QAAM,YAAY,GAAG,aAAa,GAAG,CAArC;AACA,SAAO,CAAC,YAAD,IAAiB,UAAU,KAAK,QAAvC;AACD,CAHM","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport * as broadcast_util from './broadcast_util';\nimport { elu } from './elu';\nimport { leakyRelu } from './leaky_relu';\nimport { mul } from './mul';\nimport { prelu } from './prelu';\nimport { relu } from './relu';\nimport { relu6 } from './relu6';\nimport { reshape } from './reshape';\nimport { step } from './step';\nimport { sum } from './sum';\n// Returns gradient for fused activation.\nexport function getFusedDyActivation(dy, y, activation) {\n    if (activation == null || activation === 'linear') {\n        return dy;\n    }\n    if (activation === 'relu') {\n        return mul(dy, step(y));\n    }\n    throw new Error(`Cannot compute gradient for fused activation ${activation}.`);\n}\n// Returns gradient for fused bias.\nexport function getFusedBiasGradient(bias, dyActivation) {\n    let res = dyActivation;\n    const reduceAxes = broadcast_util.getReductionAxes(bias.shape, dyActivation.shape);\n    if (reduceAxes.length > 0) {\n        res = sum(res, reduceAxes);\n    }\n    return reshape(res, bias.shape);\n}\nexport function applyActivation(x, activation, preluActivationWeights, leakyreluAlpha) {\n    if (activation === 'linear') {\n        return x;\n    }\n    else if (activation === 'relu') {\n        return relu(x);\n    }\n    else if (activation === 'elu') {\n        return elu(x);\n    }\n    else if (activation === 'relu6') {\n        return relu6(x);\n    }\n    else if (activation === 'prelu') {\n        return prelu(x, preluActivationWeights);\n    }\n    else if (activation === 'leakyrelu') {\n        return leakyRelu(x, leakyreluAlpha);\n    }\n    throw new Error(`Unknown fused activation ${activation}.`);\n}\n// Whether we should call fused ops.\nexport const shouldFuse = (gradientDepth, activation) => {\n    const gradientMode = gradientDepth > 0;\n    return !gradientMode || activation === 'linear';\n};\n//# sourceMappingURL=fused_util.js.map"]},"metadata":{},"sourceType":"module"}