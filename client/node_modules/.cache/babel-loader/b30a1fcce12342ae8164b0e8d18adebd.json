{"ast":null,"code":"import _toConsumableArray from \"/Users/kimkiwoong/SonGil/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/toConsumableArray\";\n\n/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from './engine';\nimport { Tensor, Variable } from './tensor';\nimport { convertToTensor, convertToTensorArray } from './tensor_util_env';\nimport * as util from './util';\n/**\n * Provided `f(x)`, returns another function `g(x, dy?)`, which gives the\n * gradient of `f(x)` with respect to `x`.\n *\n * If `dy` is provided, the gradient of `f(x).mul(dy).sum()` with respect to\n * `x` is computed instead. `f(x)` must take a single tensor `x` and return a\n * single tensor `y`. If `f()` takes multiple inputs, use `tf.grads` instead.\n *\n * ```js\n * // f(x) = x ^ 2\n * const f = x => x.square();\n * // f'(x) = 2x\n * const g = tf.grad(f);\n *\n * const x = tf.tensor1d([2, 3]);\n * g(x).print();\n * ```\n *\n * ```js\n * // f(x) = x ^ 3\n * const f = x => x.pow(tf.scalar(3, 'int32'));\n * // f'(x) = 3x ^ 2\n * const g = tf.grad(f);\n * // f''(x) = 6x\n * const gg = tf.grad(g);\n *\n * const x = tf.tensor1d([2, 3]);\n * gg(x).print();\n * ```\n *\n * @param f The function f(x), to compute gradient for.\n *\n * @doc {heading: 'Training', subheading: 'Gradients'}\n */\n\nfunction grad(f) {\n  util.assert(util.isFunction(f), function () {\n    return 'The f passed in grad(f) must be a function';\n  });\n  return function (x, dy) {\n    // x can be of any dtype, thus null as the last argument.\n    var $x = convertToTensor(x, 'x', 'tf.grad', 'string_or_numeric');\n    var $dy = dy != null ? convertToTensor(dy, 'dy', 'tf.grad') : null;\n    return ENGINE.tidy(function () {\n      var _ENGINE$gradients = ENGINE.gradients(function () {\n        return f($x);\n      }, [$x], $dy),\n          value = _ENGINE$gradients.value,\n          grads = _ENGINE$gradients.grads;\n\n      if ($dy != null) {\n        util.assertShapesMatch(value.shape, $dy.shape, 'The shape of dy passed in grad(f)(x, dy) must match the shape ' + 'returned by f(x)');\n      }\n\n      checkGrads(grads);\n      return grads[0];\n    });\n  };\n}\n/**\n * Provided `f(x1, x2,...)`, returns another function `g([x1, x2,...], dy?)`,\n * which gives an array of gradients of `f()` with respect to each input\n * [`x1`,`x2`,...].\n *\n * If `dy` is passed when calling `g()`, the gradient of\n * `f(x1,...).mul(dy).sum()` with respect to each input is computed instead.\n * The provided `f` must take one or more tensors and return a single tensor\n * `y`. If `f()` takes a single input, we recommend using `tf.grad` instead.\n *\n * ```js\n * // f(a, b) = a * b\n * const f = (a, b) => a.mul(b);\n * // df / da = b, df / db = a\n * const g = tf.grads(f);\n *\n * const a = tf.tensor1d([2, 3]);\n * const b = tf.tensor1d([-2, -3]);\n * const [da, db] = g([a, b]);\n * console.log('da');\n * da.print();\n * console.log('db');\n * db.print();\n * ```\n *\n * @param f The function `f(x1, x2,...)` to compute gradients for.\n *\n * @doc {heading: 'Training', subheading: 'Gradients'}\n */\n\n\nfunction grads(f) {\n  util.assert(util.isFunction(f), function () {\n    return 'The f passed in grads(f) must be a function';\n  });\n  return function (args, dy) {\n    util.assert(Array.isArray(args), function () {\n      return 'The args passed in grads(f)(args) must be an array ' + 'of `Tensor`s or `TensorLike`s';\n    }); // args can be of any dtype, thus null as the last argument.\n\n    var $args = convertToTensorArray(args, 'args', 'tf.grads', 'string_or_numeric');\n    var $dy = dy != null ? convertToTensor(dy, 'dy', 'tf.grads') : null;\n    return ENGINE.tidy(function () {\n      var _ENGINE$gradients2 = ENGINE.gradients(function () {\n        return f.apply(void 0, _toConsumableArray($args));\n      }, $args, $dy),\n          value = _ENGINE$gradients2.value,\n          grads = _ENGINE$gradients2.grads;\n\n      if ($dy != null) {\n        util.assertShapesMatch(value.shape, $dy.shape, 'The shape of dy passed in grads(f)([x1,...], dy) must ' + 'match the shape returned by f([x1,...])');\n      }\n\n      checkGrads(grads);\n      return grads;\n    });\n  };\n}\n/**\n * Like `tf.grad`, but also returns the value of `f()`. Useful when `f()`\n * returns a metric you want to show.\n *\n * The result is a rich object with the following properties:\n * - grad: The gradient of `f(x)` w.r.t `x` (result of `tf.grad`).\n * - value: The value returned by `f(x)`.\n *\n * ```js\n * // f(x) = x ^ 2\n * const f = x => x.square();\n * // f'(x) = 2x\n * const g = tf.valueAndGrad(f);\n *\n * const x = tf.tensor1d([2, 3]);\n * const {value, grad} = g(x);\n *\n * console.log('value');\n * value.print();\n * console.log('grad');\n * grad.print();\n * ```\n *\n * @doc {heading: 'Training', subheading: 'Gradients'}\n */\n\n\nfunction valueAndGrad(f) {\n  util.assert(util.isFunction(f), function () {\n    return 'The f passed in valueAndGrad(f) must be a function';\n  });\n  return function (x, dy) {\n    util.assert(x instanceof Tensor, function () {\n      return 'The x passed in valueAndGrad(f)(x) must be a tensor';\n    });\n    util.assert(dy == null || dy instanceof Tensor, function () {\n      return 'The dy passed in valueAndGrad(f)(x, dy) must be a tensor';\n    });\n\n    var _ENGINE$gradients3 = ENGINE.gradients(function () {\n      return f(x);\n    }, [x], dy),\n        grads = _ENGINE$gradients3.grads,\n        value = _ENGINE$gradients3.value;\n\n    checkGrads(grads);\n    return {\n      grad: grads[0],\n      value: value\n    };\n  };\n}\n/**\n * Like `tf.grads`, but returns also the value of `f()`. Useful when `f()`\n * returns a metric you want to show.\n *\n * The result is a rich object with the following properties:\n * - grads: The gradients of `f()` w.r.t each input (result of `tf.grads`).\n * - value: The value returned by `f(x)`.\n *\n * ```js\n * // f(a, b) = a * b\n * const f = (a, b) => a.mul(b);\n * // df/da = b, df/db = a\n * const g = tf.valueAndGrads(f);\n *\n * const a = tf.tensor1d([2, 3]);\n * const b = tf.tensor1d([-2, -3]);\n * const {value, grads} = g([a, b]);\n *\n * const [da, db] = grads;\n *\n * console.log('value');\n * value.print();\n *\n * console.log('da');\n * da.print();\n * console.log('db');\n * db.print();\n * ```\n *\n * @doc {heading: 'Training', subheading: 'Gradients'}\n */\n\n\nfunction valueAndGrads(f) {\n  util.assert(util.isFunction(f), function () {\n    return 'The f passed in valueAndGrads(f) must be a function';\n  });\n  return function (args, dy) {\n    util.assert(Array.isArray(args) && args.every(function (arg) {\n      return arg instanceof Tensor;\n    }), function () {\n      return 'The args passed in valueAndGrads(f)(args) must be array of ' + 'tensors';\n    });\n    util.assert(dy == null || dy instanceof Tensor, function () {\n      return 'The dy passed in valueAndGrads(f)(args, dy) must be a tensor';\n    });\n    var res = ENGINE.gradients(function () {\n      return f.apply(void 0, _toConsumableArray(args));\n    }, args, dy);\n\n    if (dy != null) {\n      util.assertShapesMatch(res.value.shape, dy.shape, 'The shape of dy passed in valueAndGrads(f)([x1,...], dy) must ' + 'match the shape returned by f([x1,...])');\n    }\n\n    checkGrads(res.grads);\n    return res;\n  };\n}\n/**\n * Computes and returns the gradient of f(x) with respect to the list of\n * trainable variables provided by `varList`. If no list is provided, it\n * defaults to all trainable variables.\n *\n * ```js\n * const a = tf.variable(tf.tensor1d([3, 4]));\n * const b = tf.variable(tf.tensor1d([5, 6]));\n * const x = tf.tensor1d([1, 2]);\n *\n * // f(a, b) = a * x ^ 2 + b * x\n * const f = () => a.mul(x.square()).add(b.mul(x)).sum();\n * // df/da = x ^ 2, df/db = x\n * const {value, grads} = tf.variableGrads(f);\n *\n * Object.keys(grads).forEach(varName => grads[varName].print());\n * ```\n *\n * @param f The function to execute. f() should return a scalar.\n * @param varList The list of variables to compute the gradients with respect\n *     to. Defaults to all trainable variables.\n * @returns An object with the following keys and values:\n *   - `value`: The value of the function `f`.\n *   - `grads`: A map from the names of the variables to the gradients.\n *     If the `varList` argument is provided explicitly and contains a subset of\n *     non-trainable variables, this map in the return value will contain keys\n *     that map the names of the non-trainable variables to `null`.\n *\n * @doc {heading: 'Training', subheading: 'Gradients'}\n */\n\n\nfunction variableGrads(f, varList) {\n  util.assert(util.isFunction(f), function () {\n    return 'The f passed in variableGrads(f) must be a function';\n  });\n  util.assert(varList == null || Array.isArray(varList) && varList.every(function (v) {\n    return v instanceof Variable;\n  }), function () {\n    return 'The varList passed in variableGrads(f, varList) must be an array ' + 'of variables';\n  });\n  var specifiedVarList = varList != null;\n\n  if (!specifiedVarList) {\n    // Get all of the trainable variables.\n    varList = [];\n\n    for (var varName in ENGINE.registeredVariables) {\n      varList.push(ENGINE.registeredVariables[varName]);\n    }\n  }\n\n  var specifiedNonTrainable = specifiedVarList ? varList.filter(function (variable) {\n    return !variable.trainable;\n  }) : null; // Prune non-trainable variables.\n\n  var originalVarCount = varList.length;\n  varList = varList.filter(function (variable) {\n    return variable.trainable;\n  });\n  util.assert(varList.length > 0, function () {\n    return \"variableGrads() expects at least one of the input variables to \" + \"be trainable, but none of the \".concat(originalVarCount, \" variables is \") + \"trainable.\";\n  });\n  var allowNoGradients = true;\n\n  var _ENGINE$gradients4 = ENGINE.gradients(f, varList, null, allowNoGradients),\n      value = _ENGINE$gradients4.value,\n      grads = _ENGINE$gradients4.grads;\n\n  util.assert(grads.some(function (g) {\n    return g != null;\n  }), function () {\n    return 'Cannot find a connection between any variable and the result of ' + 'the loss function y=f(x). Please make sure the operations that ' + 'use variables are inside the function f passed to minimize().';\n  });\n  util.assert(value.rank === 0, function () {\n    return \"The f passed in variableGrads(f) must return a scalar, but it \" + \"returned a rank-\".concat(value.rank, \" tensor\");\n  });\n  var namedGrads = {};\n  varList.forEach(function (v, i) {\n    if (grads[i] != null) {\n      namedGrads[v.name] = grads[i];\n    }\n  });\n\n  if (specifiedNonTrainable != null) {\n    // If varList is explicitly provided and contains non-trainable values,\n    // add them to the returned gradients with `null` values.\n    specifiedNonTrainable.forEach(function (v) {\n      return namedGrads[v.name] = null;\n    });\n  }\n\n  return {\n    value: value,\n    grads: namedGrads\n  };\n}\n/**\n * Overrides the gradient computation of a function `f`.\n *\n * Takes a function\n * `f(...inputs, save) => {value: Tensor, gradFunc: (dy, saved) => Tensor[]}`\n * and returns another function `g(...inputs)` which takes the same inputs as\n * `f`. When called, `g` returns `f().value`. In backward mode, custom gradients\n * with respect to each input of `f` are computed using `f().gradFunc`.\n *\n * The `save` function passsed to `f` should be used for saving tensors needed\n * in the gradient. And the `saved` passed to the `gradFunc` is a\n * `NamedTensorMap`, which contains those saved tensor.\n *\n * ```js\n * const customOp = tf.customGrad((x, save) => {\n *   // Save x to make sure it's available later for the gradient.\n *   save([x]);\n *   // Override gradient of our custom x ^ 2 op to be dy * abs(x);\n *   return {\n *     value: x.square(),\n *     // Note `saved.x` which points to the `x` we saved earlier.\n *     gradFunc: (dy, saved) => [dy.mul(saved[0].abs())]\n *   };\n * });\n *\n * const x = tf.tensor1d([-1, -2, 3]);\n * const dx = tf.grad(x => customOp(x));\n *\n * console.log(`f(x):`);\n * customOp(x).print();\n * console.log(`f'(x):`);\n * dx(x).print();\n * ```\n *\n * @param f The function to evaluate in forward mode, which should return\n *     `{value: Tensor, gradFunc: (dy, saved) => Tensor[]}`, where `gradFunc`\n *     returns the custom gradients of `f` with respect to its inputs.\n *\n * @doc {heading: 'Training', subheading: 'Gradients'}\n */\n\n\nfunction customGrad(f) {\n  return ENGINE.customGrad(f);\n}\n\nfunction checkGrads(grads) {\n  var numNullGradients = grads.filter(function (g) {\n    return g == null;\n  }).length;\n\n  if (numNullGradients > 0) {\n    throw new Error(\"Cannot compute gradient of y=f(x) with respect to x. Make sure that\\n    the f you passed encloses all operations that lead from x to y.\");\n  }\n}\n\nexport { customGrad, variableGrads, valueAndGrad, valueAndGrads, grad, grads };","map":{"version":3,"sources":["../src/gradients.ts"],"names":[],"mappings":";;AAAA;;;;;;;;;;;;;;;AAeG;AAEH,SAA4B,MAA5B,QAAyC,UAAzC;AACA,SAAgB,MAAhB,EAAwB,QAAxB,QAAuC,UAAvC;AAEA,SAAQ,eAAR,EAAyB,oBAAzB,QAAoD,mBAApD;AAEA,OAAO,KAAK,IAAZ,MAAsB,QAAtB;AAEA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAiCG;;AACH,SAAS,IAAT,CAAc,CAAd,EAAsC;AAEpC,EAAA,IAAI,CAAC,MAAL,CACI,IAAI,CAAC,UAAL,CAAgB,CAAhB,CADJ,EACwB;AAAA,WAAM,4CAAN;AAAA,GADxB;AAEA,SAAO,UAAC,CAAD,EAAuB,EAAvB,EAAyD;AAC9D;AACA,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,SAAT,EAAoB,mBAApB,CAA1B;AACA,QAAM,GAAG,GACJ,EAAE,IAAI,IAAP,GAAe,eAAe,CAAC,EAAD,EAAK,IAAL,EAAW,SAAX,CAA9B,GAAsD,IAD1D;AAEA,WAAO,MAAM,CAAC,IAAP,CAAY,YAAK;AAAA,8BACC,MAAM,CAAC,SAAP,CAAiB;AAAA,eAAM,CAAC,CAAC,EAAD,CAAP;AAAA,OAAjB,EAA8B,CAAC,EAAD,CAA9B,EAAoC,GAApC,CADD;AAAA,UACf,KADe,qBACf,KADe;AAAA,UACR,KADQ,qBACR,KADQ;;AAEtB,UAAI,GAAG,IAAI,IAAX,EAAiB;AACf,QAAA,IAAI,CAAC,iBAAL,CACI,KAAK,CAAC,KADV,EACiB,GAAG,CAAC,KADrB,EAEI,mEACI,kBAHR;AAID;;AACD,MAAA,UAAU,CAAC,KAAD,CAAV;AACA,aAAO,KAAK,CAAC,CAAD,CAAZ;AACD,KAVM,CAAP;AAWD,GAhBD;AAiBD;AAED;;;;;;;;;;;;;;;;;;;;;;;;;;;;AA4BG;;;AACH,SAAS,KAAT,CAAe,CAAf,EAA+C;AAE7C,EAAA,IAAI,CAAC,MAAL,CACI,IAAI,CAAC,UAAL,CAAgB,CAAhB,CADJ,EACwB;AAAA,WAAM,6CAAN;AAAA,GADxB;AAEA,SAAO,UAAC,IAAD,EAAiC,EAAjC,EAAqE;AAC1E,IAAA,IAAI,CAAC,MAAL,CACI,KAAK,CAAC,OAAN,CAAc,IAAd,CADJ,EAEI;AAAA,aAAM,wDACF,+BADJ;AAAA,KAFJ,EAD0E,CAK1E;;AACA,QAAM,KAAK,GACP,oBAAoB,CAAC,IAAD,EAAO,MAAP,EAAe,UAAf,EAA2B,mBAA3B,CADxB;AAEA,QAAM,GAAG,GACJ,EAAE,IAAI,IAAP,GAAe,eAAe,CAAC,EAAD,EAAK,IAAL,EAAW,UAAX,CAA9B,GAAuD,IAD3D;AAEA,WAAO,MAAM,CAAC,IAAP,CAAY,YAAK;AAAA,+BACC,MAAM,CAAC,SAAP,CAAiB;AAAA,eAAM,CAAC,MAAD,4BAAK,KAAL,EAAN;AAAA,OAAjB,EAAoC,KAApC,EAA2C,GAA3C,CADD;AAAA,UACf,KADe,sBACf,KADe;AAAA,UACR,KADQ,sBACR,KADQ;;AAEtB,UAAI,GAAG,IAAI,IAAX,EAAiB;AACf,QAAA,IAAI,CAAC,iBAAL,CACI,KAAK,CAAC,KADV,EACiB,GAAG,CAAC,KADrB,EAEI,2DACI,yCAHR;AAID;;AACD,MAAA,UAAU,CAAC,KAAD,CAAV;AACA,aAAO,KAAP;AACD,KAVM,CAAP;AAWD,GArBD;AAsBD;AAED;;;;;;;;;;;;;;;;;;;;;;;;AAwBG;;;AACH,SAAS,YAAT,CAA0D,CAA1D,EAAwE;AAKtE,EAAA,IAAI,CAAC,MAAL,CACI,IAAI,CAAC,UAAL,CAAgB,CAAhB,CADJ,EAEI;AAAA,WAAM,oDAAN;AAAA,GAFJ;AAGA,SAAO,UAAC,CAAD,EAAO,EAAP,EAAiB;AACtB,IAAA,IAAI,CAAC,MAAL,CACI,CAAC,YAAY,MADjB,EAEI;AAAA,aAAM,qDAAN;AAAA,KAFJ;AAGA,IAAA,IAAI,CAAC,MAAL,CACI,EAAE,IAAI,IAAN,IAAc,EAAE,YAAY,MADhC,EAEI;AAAA,aAAM,0DAAN;AAAA,KAFJ;;AAJsB,6BAOC,MAAM,CAAC,SAAP,CAAiB;AAAA,aAAM,CAAC,CAAC,CAAD,CAAP;AAAA,KAAjB,EAA6B,CAAC,CAAD,CAA7B,EAAkC,EAAlC,CAPD;AAAA,QAOf,KAPe,sBAOf,KAPe;AAAA,QAOR,KAPQ,sBAOR,KAPQ;;AAQtB,IAAA,UAAU,CAAC,KAAD,CAAV;AACA,WAAO;AAAC,MAAA,IAAI,EAAE,KAAK,CAAC,CAAD,CAAZ;AAAsB,MAAA,KAAK,EAAL;AAAtB,KAAP;AACD,GAVD;AAWD;AAED;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AA8BG;;;AACH,SAAS,aAAT,CAAyC,CAAzC,EAAoE;AAKlE,EAAA,IAAI,CAAC,MAAL,CACI,IAAI,CAAC,UAAL,CAAgB,CAAhB,CADJ,EAEI;AAAA,WAAM,qDAAN;AAAA,GAFJ;AAGA,SAAO,UAAC,IAAD,EAAiB,EAAjB,EAA2B;AAChC,IAAA,IAAI,CAAC,MAAL,CACI,KAAK,CAAC,OAAN,CAAc,IAAd,KAAuB,IAAI,CAAC,KAAL,CAAW,UAAA,GAAG;AAAA,aAAI,GAAG,YAAY,MAAnB;AAAA,KAAd,CAD3B,EAEI;AAAA,aAAM,gEACF,SADJ;AAAA,KAFJ;AAIA,IAAA,IAAI,CAAC,MAAL,CACI,EAAE,IAAI,IAAN,IAAc,EAAE,YAAY,MADhC,EAEI;AAAA,aAAM,8DAAN;AAAA,KAFJ;AAGA,QAAM,GAAG,GAAG,MAAM,CAAC,SAAP,CAAiB;AAAA,aAAM,CAAC,MAAD,4BAAK,IAAL,EAAN;AAAA,KAAjB,EAAmC,IAAnC,EAAyC,EAAzC,CAAZ;;AACA,QAAI,EAAE,IAAI,IAAV,EAAgB;AACd,MAAA,IAAI,CAAC,iBAAL,CACI,GAAG,CAAC,KAAJ,CAAU,KADd,EACqB,EAAE,CAAC,KADxB,EAEI,mEACI,yCAHR;AAID;;AACD,IAAA,UAAU,CAAC,GAAG,CAAC,KAAL,CAAV;AACA,WAAO,GAAP;AACD,GAjBD;AAkBD;AAED;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AA6BG;;;AACH,SAAS,aAAT,CAAuB,CAAvB,EAAwC,OAAxC,EAA4D;AAE1D,EAAA,IAAI,CAAC,MAAL,CACI,IAAI,CAAC,UAAL,CAAgB,CAAhB,CADJ,EAEI;AAAA,WAAM,qDAAN;AAAA,GAFJ;AAGA,EAAA,IAAI,CAAC,MAAL,CACI,OAAO,IAAI,IAAX,IACI,KAAK,CAAC,OAAN,CAAc,OAAd,KAA0B,OAAO,CAAC,KAAR,CAAc,UAAA,CAAC;AAAA,WAAI,CAAC,YAAY,QAAjB;AAAA,GAAf,CAFlC,EAGI;AAAA,WACI,sEACA,cAFJ;AAAA,GAHJ;AAOA,MAAM,gBAAgB,GAAG,OAAO,IAAI,IAApC;;AACA,MAAI,CAAC,gBAAL,EAAuB;AACrB;AACA,IAAA,OAAO,GAAG,EAAV;;AACA,SAAK,IAAM,OAAX,IAAsB,MAAM,CAAC,mBAA7B,EAAkD;AAChD,MAAA,OAAO,CAAC,IAAR,CAAa,MAAM,CAAC,mBAAP,CAA2B,OAA3B,CAAb;AACD;AACF;;AAED,MAAM,qBAAqB,GACvB,gBAAgB,GAAG,OAAO,CAAC,MAAR,CAAe,UAAA,QAAQ;AAAA,WAAI,CAAC,QAAQ,CAAC,SAAd;AAAA,GAAvB,CAAH,GAAqD,IADzE,CArB0D,CAwB1D;;AACA,MAAM,gBAAgB,GAAG,OAAO,CAAC,MAAjC;AACA,EAAA,OAAO,GAAG,OAAO,CAAC,MAAR,CAAe,UAAA,QAAQ;AAAA,WAAI,QAAQ,CAAC,SAAb;AAAA,GAAvB,CAAV;AACA,EAAA,IAAI,CAAC,MAAL,CACI,OAAO,CAAC,MAAR,GAAiB,CADrB,EAEI;AAAA,WAAM,4GAC+B,gBAD/B,kCAAN;AAAA,GAFJ;AAMA,MAAM,gBAAgB,GAAG,IAAzB;;AAjC0D,2BAkCnC,MAAM,CAAC,SAAP,CAAiB,CAAjB,EAAoB,OAApB,EAA6B,IAA7B,EAAmC,gBAAnC,CAlCmC;AAAA,MAkCnD,KAlCmD,sBAkCnD,KAlCmD;AAAA,MAkC5C,KAlC4C,sBAkC5C,KAlC4C;;AAoC1D,EAAA,IAAI,CAAC,MAAL,CACI,KAAK,CAAC,IAAN,CAAW,UAAA,CAAC;AAAA,WAAI,CAAC,IAAI,IAAT;AAAA,GAAZ,CADJ,EAEI;AAAA,WAAM,qEACF,iEADE,GAEF,+DAFJ;AAAA,GAFJ;AAKA,EAAA,IAAI,CAAC,MAAL,CACI,KAAK,CAAC,IAAN,KAAe,CADnB,EAEI;AAAA,WAAM,6FACiB,KAAK,CAAC,IADvB,YAAN;AAAA,GAFJ;AAKA,MAAM,UAAU,GAAmB,EAAnC;AACA,EAAA,OAAO,CAAC,OAAR,CAAgB,UAAC,CAAD,EAAI,CAAJ,EAAS;AACvB,QAAI,KAAK,CAAC,CAAD,CAAL,IAAY,IAAhB,EAAsB;AACpB,MAAA,UAAU,CAAC,CAAC,CAAC,IAAH,CAAV,GAAqB,KAAK,CAAC,CAAD,CAA1B;AACD;AACF,GAJD;;AAKA,MAAI,qBAAqB,IAAI,IAA7B,EAAmC;AACjC;AACA;AACA,IAAA,qBAAqB,CAAC,OAAtB,CAA8B,UAAA,CAAC;AAAA,aAAI,UAAU,CAAC,CAAC,CAAC,IAAH,CAAV,GAAqB,IAAzB;AAAA,KAA/B;AACD;;AACD,SAAO;AAAC,IAAA,KAAK,EAAL,KAAD;AAAQ,IAAA,KAAK,EAAE;AAAf,GAAP;AACD;AAED;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAuCG;;;AACH,SAAS,UAAT,CAAsC,CAAtC,EAA8D;AAE5D,SAAO,MAAM,CAAC,UAAP,CAAkB,CAAlB,CAAP;AACD;;AAED,SAAS,UAAT,CAAoB,KAApB,EAAmC;AACjC,MAAM,gBAAgB,GAAG,KAAK,CAAC,MAAN,CAAa,UAAA,CAAC;AAAA,WAAI,CAAC,IAAI,IAAT;AAAA,GAAd,EAA6B,MAAtD;;AACA,MAAI,gBAAgB,GAAG,CAAvB,EAA0B;AACxB,UAAM,IAAI,KAAJ,4IAAN;AAGD;AACF;;AAED,SACE,UADF,EAEE,aAFF,EAGE,YAHF,EAIE,aAJF,EAKE,IALF,EAME,KANF","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from './engine';\nimport { Tensor, Variable } from './tensor';\nimport { convertToTensor, convertToTensorArray } from './tensor_util_env';\nimport * as util from './util';\n/**\n * Provided `f(x)`, returns another function `g(x, dy?)`, which gives the\n * gradient of `f(x)` with respect to `x`.\n *\n * If `dy` is provided, the gradient of `f(x).mul(dy).sum()` with respect to\n * `x` is computed instead. `f(x)` must take a single tensor `x` and return a\n * single tensor `y`. If `f()` takes multiple inputs, use `tf.grads` instead.\n *\n * ```js\n * // f(x) = x ^ 2\n * const f = x => x.square();\n * // f'(x) = 2x\n * const g = tf.grad(f);\n *\n * const x = tf.tensor1d([2, 3]);\n * g(x).print();\n * ```\n *\n * ```js\n * // f(x) = x ^ 3\n * const f = x => x.pow(tf.scalar(3, 'int32'));\n * // f'(x) = 3x ^ 2\n * const g = tf.grad(f);\n * // f''(x) = 6x\n * const gg = tf.grad(g);\n *\n * const x = tf.tensor1d([2, 3]);\n * gg(x).print();\n * ```\n *\n * @param f The function f(x), to compute gradient for.\n *\n * @doc {heading: 'Training', subheading: 'Gradients'}\n */\nfunction grad(f) {\n    util.assert(util.isFunction(f), () => 'The f passed in grad(f) must be a function');\n    return (x, dy) => {\n        // x can be of any dtype, thus null as the last argument.\n        const $x = convertToTensor(x, 'x', 'tf.grad', 'string_or_numeric');\n        const $dy = (dy != null) ? convertToTensor(dy, 'dy', 'tf.grad') : null;\n        return ENGINE.tidy(() => {\n            const { value, grads } = ENGINE.gradients(() => f($x), [$x], $dy);\n            if ($dy != null) {\n                util.assertShapesMatch(value.shape, $dy.shape, 'The shape of dy passed in grad(f)(x, dy) must match the shape ' +\n                    'returned by f(x)');\n            }\n            checkGrads(grads);\n            return grads[0];\n        });\n    };\n}\n/**\n * Provided `f(x1, x2,...)`, returns another function `g([x1, x2,...], dy?)`,\n * which gives an array of gradients of `f()` with respect to each input\n * [`x1`,`x2`,...].\n *\n * If `dy` is passed when calling `g()`, the gradient of\n * `f(x1,...).mul(dy).sum()` with respect to each input is computed instead.\n * The provided `f` must take one or more tensors and return a single tensor\n * `y`. If `f()` takes a single input, we recommend using `tf.grad` instead.\n *\n * ```js\n * // f(a, b) = a * b\n * const f = (a, b) => a.mul(b);\n * // df / da = b, df / db = a\n * const g = tf.grads(f);\n *\n * const a = tf.tensor1d([2, 3]);\n * const b = tf.tensor1d([-2, -3]);\n * const [da, db] = g([a, b]);\n * console.log('da');\n * da.print();\n * console.log('db');\n * db.print();\n * ```\n *\n * @param f The function `f(x1, x2,...)` to compute gradients for.\n *\n * @doc {heading: 'Training', subheading: 'Gradients'}\n */\nfunction grads(f) {\n    util.assert(util.isFunction(f), () => 'The f passed in grads(f) must be a function');\n    return (args, dy) => {\n        util.assert(Array.isArray(args), () => 'The args passed in grads(f)(args) must be an array ' +\n            'of `Tensor`s or `TensorLike`s');\n        // args can be of any dtype, thus null as the last argument.\n        const $args = convertToTensorArray(args, 'args', 'tf.grads', 'string_or_numeric');\n        const $dy = (dy != null) ? convertToTensor(dy, 'dy', 'tf.grads') : null;\n        return ENGINE.tidy(() => {\n            const { value, grads } = ENGINE.gradients(() => f(...$args), $args, $dy);\n            if ($dy != null) {\n                util.assertShapesMatch(value.shape, $dy.shape, 'The shape of dy passed in grads(f)([x1,...], dy) must ' +\n                    'match the shape returned by f([x1,...])');\n            }\n            checkGrads(grads);\n            return grads;\n        });\n    };\n}\n/**\n * Like `tf.grad`, but also returns the value of `f()`. Useful when `f()`\n * returns a metric you want to show.\n *\n * The result is a rich object with the following properties:\n * - grad: The gradient of `f(x)` w.r.t `x` (result of `tf.grad`).\n * - value: The value returned by `f(x)`.\n *\n * ```js\n * // f(x) = x ^ 2\n * const f = x => x.square();\n * // f'(x) = 2x\n * const g = tf.valueAndGrad(f);\n *\n * const x = tf.tensor1d([2, 3]);\n * const {value, grad} = g(x);\n *\n * console.log('value');\n * value.print();\n * console.log('grad');\n * grad.print();\n * ```\n *\n * @doc {heading: 'Training', subheading: 'Gradients'}\n */\nfunction valueAndGrad(f) {\n    util.assert(util.isFunction(f), () => 'The f passed in valueAndGrad(f) must be a function');\n    return (x, dy) => {\n        util.assert(x instanceof Tensor, () => 'The x passed in valueAndGrad(f)(x) must be a tensor');\n        util.assert(dy == null || dy instanceof Tensor, () => 'The dy passed in valueAndGrad(f)(x, dy) must be a tensor');\n        const { grads, value } = ENGINE.gradients(() => f(x), [x], dy);\n        checkGrads(grads);\n        return { grad: grads[0], value };\n    };\n}\n/**\n * Like `tf.grads`, but returns also the value of `f()`. Useful when `f()`\n * returns a metric you want to show.\n *\n * The result is a rich object with the following properties:\n * - grads: The gradients of `f()` w.r.t each input (result of `tf.grads`).\n * - value: The value returned by `f(x)`.\n *\n * ```js\n * // f(a, b) = a * b\n * const f = (a, b) => a.mul(b);\n * // df/da = b, df/db = a\n * const g = tf.valueAndGrads(f);\n *\n * const a = tf.tensor1d([2, 3]);\n * const b = tf.tensor1d([-2, -3]);\n * const {value, grads} = g([a, b]);\n *\n * const [da, db] = grads;\n *\n * console.log('value');\n * value.print();\n *\n * console.log('da');\n * da.print();\n * console.log('db');\n * db.print();\n * ```\n *\n * @doc {heading: 'Training', subheading: 'Gradients'}\n */\nfunction valueAndGrads(f) {\n    util.assert(util.isFunction(f), () => 'The f passed in valueAndGrads(f) must be a function');\n    return (args, dy) => {\n        util.assert(Array.isArray(args) && args.every(arg => arg instanceof Tensor), () => 'The args passed in valueAndGrads(f)(args) must be array of ' +\n            'tensors');\n        util.assert(dy == null || dy instanceof Tensor, () => 'The dy passed in valueAndGrads(f)(args, dy) must be a tensor');\n        const res = ENGINE.gradients(() => f(...args), args, dy);\n        if (dy != null) {\n            util.assertShapesMatch(res.value.shape, dy.shape, 'The shape of dy passed in valueAndGrads(f)([x1,...], dy) must ' +\n                'match the shape returned by f([x1,...])');\n        }\n        checkGrads(res.grads);\n        return res;\n    };\n}\n/**\n * Computes and returns the gradient of f(x) with respect to the list of\n * trainable variables provided by `varList`. If no list is provided, it\n * defaults to all trainable variables.\n *\n * ```js\n * const a = tf.variable(tf.tensor1d([3, 4]));\n * const b = tf.variable(tf.tensor1d([5, 6]));\n * const x = tf.tensor1d([1, 2]);\n *\n * // f(a, b) = a * x ^ 2 + b * x\n * const f = () => a.mul(x.square()).add(b.mul(x)).sum();\n * // df/da = x ^ 2, df/db = x\n * const {value, grads} = tf.variableGrads(f);\n *\n * Object.keys(grads).forEach(varName => grads[varName].print());\n * ```\n *\n * @param f The function to execute. f() should return a scalar.\n * @param varList The list of variables to compute the gradients with respect\n *     to. Defaults to all trainable variables.\n * @returns An object with the following keys and values:\n *   - `value`: The value of the function `f`.\n *   - `grads`: A map from the names of the variables to the gradients.\n *     If the `varList` argument is provided explicitly and contains a subset of\n *     non-trainable variables, this map in the return value will contain keys\n *     that map the names of the non-trainable variables to `null`.\n *\n * @doc {heading: 'Training', subheading: 'Gradients'}\n */\nfunction variableGrads(f, varList) {\n    util.assert(util.isFunction(f), () => 'The f passed in variableGrads(f) must be a function');\n    util.assert(varList == null ||\n        Array.isArray(varList) && varList.every(v => v instanceof Variable), () => 'The varList passed in variableGrads(f, varList) must be an array ' +\n        'of variables');\n    const specifiedVarList = varList != null;\n    if (!specifiedVarList) {\n        // Get all of the trainable variables.\n        varList = [];\n        for (const varName in ENGINE.registeredVariables) {\n            varList.push(ENGINE.registeredVariables[varName]);\n        }\n    }\n    const specifiedNonTrainable = specifiedVarList ? varList.filter(variable => !variable.trainable) : null;\n    // Prune non-trainable variables.\n    const originalVarCount = varList.length;\n    varList = varList.filter(variable => variable.trainable);\n    util.assert(varList.length > 0, () => `variableGrads() expects at least one of the input variables to ` +\n        `be trainable, but none of the ${originalVarCount} variables is ` +\n        `trainable.`);\n    const allowNoGradients = true;\n    const { value, grads } = ENGINE.gradients(f, varList, null, allowNoGradients);\n    util.assert(grads.some(g => g != null), () => 'Cannot find a connection between any variable and the result of ' +\n        'the loss function y=f(x). Please make sure the operations that ' +\n        'use variables are inside the function f passed to minimize().');\n    util.assert(value.rank === 0, () => `The f passed in variableGrads(f) must return a scalar, but it ` +\n        `returned a rank-${value.rank} tensor`);\n    const namedGrads = {};\n    varList.forEach((v, i) => {\n        if (grads[i] != null) {\n            namedGrads[v.name] = grads[i];\n        }\n    });\n    if (specifiedNonTrainable != null) {\n        // If varList is explicitly provided and contains non-trainable values,\n        // add them to the returned gradients with `null` values.\n        specifiedNonTrainable.forEach(v => namedGrads[v.name] = null);\n    }\n    return { value, grads: namedGrads };\n}\n/**\n * Overrides the gradient computation of a function `f`.\n *\n * Takes a function\n * `f(...inputs, save) => {value: Tensor, gradFunc: (dy, saved) => Tensor[]}`\n * and returns another function `g(...inputs)` which takes the same inputs as\n * `f`. When called, `g` returns `f().value`. In backward mode, custom gradients\n * with respect to each input of `f` are computed using `f().gradFunc`.\n *\n * The `save` function passsed to `f` should be used for saving tensors needed\n * in the gradient. And the `saved` passed to the `gradFunc` is a\n * `NamedTensorMap`, which contains those saved tensor.\n *\n * ```js\n * const customOp = tf.customGrad((x, save) => {\n *   // Save x to make sure it's available later for the gradient.\n *   save([x]);\n *   // Override gradient of our custom x ^ 2 op to be dy * abs(x);\n *   return {\n *     value: x.square(),\n *     // Note `saved.x` which points to the `x` we saved earlier.\n *     gradFunc: (dy, saved) => [dy.mul(saved[0].abs())]\n *   };\n * });\n *\n * const x = tf.tensor1d([-1, -2, 3]);\n * const dx = tf.grad(x => customOp(x));\n *\n * console.log(`f(x):`);\n * customOp(x).print();\n * console.log(`f'(x):`);\n * dx(x).print();\n * ```\n *\n * @param f The function to evaluate in forward mode, which should return\n *     `{value: Tensor, gradFunc: (dy, saved) => Tensor[]}`, where `gradFunc`\n *     returns the custom gradients of `f` with respect to its inputs.\n *\n * @doc {heading: 'Training', subheading: 'Gradients'}\n */\nfunction customGrad(f) {\n    return ENGINE.customGrad(f);\n}\nfunction checkGrads(grads) {\n    const numNullGradients = grads.filter(g => g == null).length;\n    if (numNullGradients > 0) {\n        throw new Error(`Cannot compute gradient of y=f(x) with respect to x. Make sure that\n    the f you passed encloses all operations that lead from x to y.`);\n    }\n}\nexport { customGrad, variableGrads, valueAndGrad, valueAndGrads, grad, grads, };\n//# sourceMappingURL=gradients.js.map"]},"metadata":{},"sourceType":"module"}