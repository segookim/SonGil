{"ast":null,"code":"import _toConsumableArray from \"/Users/kimkiwoong/SonGil/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/toConsumableArray\";\nimport _classCallCheck from \"/Users/kimkiwoong/SonGil/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/classCallCheck\";\nimport _createClass from \"/Users/kimkiwoong/SonGil/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/createClass\";\nimport _get from \"/Users/kimkiwoong/SonGil/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/get\";\nimport _getPrototypeOf from \"/Users/kimkiwoong/SonGil/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/getPrototypeOf\";\nimport _inherits from \"/Users/kimkiwoong/SonGil/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/inherits\";\nimport _createSuper from \"/Users/kimkiwoong/SonGil/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/createSuper\";\n\n/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Layers that augment the functionality of a base layer.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { serialization, tidy } from '@tensorflow/tfjs-core';\nimport * as K from '../backend/tfjs_backend';\nimport { nameScope } from '../common';\nimport { InputSpec, Layer, SymbolicTensor } from '../engine/topology';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { VALID_BIDIRECTIONAL_MERGE_MODES } from '../keras_format/common';\nimport * as generic_utils from '../utils/generic_utils';\nimport { getExactlyOneShape, getExactlyOneTensor } from '../utils/types_utils';\nimport { rnn, standardizeArgs } from './recurrent';\nimport { deserialize } from './serialization';\n/**\n * Abstract wrapper base class.\n *\n * Wrappers take another layer and augment it in various ways.\n * Do not use this class as a layer, it is only an abstract base class.\n * Two usable wrappers are the `TimeDistributed` and `Bidirectional` wrappers.\n */\n\nexport var Wrapper = /*#__PURE__*/function (_Layer) {\n  _inherits(Wrapper, _Layer);\n\n  var _super = _createSuper(Wrapper);\n\n  function Wrapper(args) {\n    var _this;\n\n    _classCallCheck(this, Wrapper);\n\n    // Porting Note: In PyKeras, `self.layer` is set prior to the calling\n    //   `super()`. But we can't do that here due to TypeScript's restriction.\n    //   See: https://github.com/Microsoft/TypeScript/issues/8277\n    //   As a result, we have to add checks in `get trainable()` and\n    //   `set trainable()` below in order to prevent using `this.layer` when\n    //   its value is `undefined`. The super constructor does use the getter\n    //   and the setter of `this.layer`.\n    _this = _super.call(this, args);\n    _this.layer = args.layer;\n    return _this;\n  }\n\n  _createClass(Wrapper, [{\n    key: \"build\",\n    value: function build(inputShape) {\n      this.built = true;\n    } // TODO(cais): Implement activityRegularizer getter.\n\n  }, {\n    key: \"trainable\",\n    get: function get() {\n      // Porting Note: the check of `this.layer` here is necessary due to the\n      //   way the `constructor` of this class is written (see Porting Note\n      //   above).\n      if (this.layer != null) {\n        return this.layer.trainable;\n      } else {\n        return false;\n      }\n    },\n    set: function set(value) {\n      // Porting Note: the check of `this.layer` here is necessary due to the\n      //   way the `constructor` of this class is written (see Porting Note\n      //   above).\n      if (this.layer != null) {\n        this.layer.trainable = value;\n      }\n    }\n  }, {\n    key: \"trainableWeights\",\n    get: function get() {\n      return this.layer.trainableWeights;\n    } // TODO(cais): Implement setter for trainableWeights.\n\n  }, {\n    key: \"nonTrainableWeights\",\n    get: function get() {\n      return this.layer.nonTrainableWeights;\n    } // TODO(cais): Implement setter for nonTrainableWeights.\n\n  }, {\n    key: \"updates\",\n    get: function get() {\n      // tslint:disable-next-line:no-any\n      return this.layer._updates;\n    } // TODO(cais): Implement getUpdatesFor().\n\n  }, {\n    key: \"losses\",\n    get: function get() {\n      return this.layer.losses;\n    } // TODO(cais): Implement getLossesFor().\n\n  }, {\n    key: \"getWeights\",\n    value: function getWeights() {\n      return this.layer.getWeights();\n    }\n  }, {\n    key: \"setWeights\",\n    value: function setWeights(weights) {\n      this.layer.setWeights(weights);\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var config = {\n        'layer': {\n          'className': this.layer.getClassName(),\n          'config': this.layer.getConfig()\n        }\n      };\n\n      var baseConfig = _get(_getPrototypeOf(Wrapper.prototype), \"getConfig\", this).call(this);\n\n      Object.assign(config, baseConfig);\n      return config;\n    }\n  }, {\n    key: \"setFastWeightInitDuringBuild\",\n    value: function setFastWeightInitDuringBuild(value) {\n      _get(_getPrototypeOf(Wrapper.prototype), \"setFastWeightInitDuringBuild\", this).call(this, value);\n\n      if (this.layer != null) {\n        this.layer.setFastWeightInitDuringBuild(value);\n      }\n    }\n    /** @nocollapse */\n\n  }], [{\n    key: \"fromConfig\",\n    value: function fromConfig(cls, config) {\n      var customObjects = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};\n      var layerConfig = config['layer'];\n      var layer = deserialize(layerConfig, customObjects);\n      delete config['layer'];\n      var newConfig = {\n        layer: layer\n      };\n      Object.assign(newConfig, config);\n      return new cls(newConfig);\n    }\n  }]);\n\n  return Wrapper;\n}(Layer);\nexport var TimeDistributed = /*#__PURE__*/function (_Wrapper) {\n  _inherits(TimeDistributed, _Wrapper);\n\n  var _super2 = _createSuper(TimeDistributed);\n\n  function TimeDistributed(args) {\n    var _this2;\n\n    _classCallCheck(this, TimeDistributed);\n\n    _this2 = _super2.call(this, args);\n    _this2.supportsMasking = true;\n    return _this2;\n  }\n\n  _createClass(TimeDistributed, [{\n    key: \"build\",\n    value: function build(inputShape) {\n      inputShape = getExactlyOneShape(inputShape);\n\n      if (inputShape.length < 3) {\n        throw new ValueError(\"TimeDistributed layer expects an input shape >= 3D, but received \" + \"input shape \".concat(JSON.stringify(inputShape)));\n      }\n\n      this.inputSpec = [{\n        shape: inputShape\n      }];\n      var childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n\n      if (!this.layer.built) {\n        this.layer.build(childInputShape);\n        this.layer.built = true;\n      }\n\n      _get(_getPrototypeOf(TimeDistributed.prototype), \"build\", this).call(this, inputShape);\n    }\n  }, {\n    key: \"computeOutputShape\",\n    value: function computeOutputShape(inputShape) {\n      inputShape = getExactlyOneShape(inputShape);\n      var childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n      var childOutputShape = this.layer.computeOutputShape(childInputShape);\n      var timesteps = inputShape[1];\n      return [childOutputShape[0], timesteps].concat(childOutputShape.slice(1));\n    }\n  }, {\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this3 = this;\n\n      return tidy(function () {\n        // TODO(cais): Add 'training' and 'useLearningPhase' to kwargs.\n        inputs = getExactlyOneTensor(inputs); // Porting Note: In tfjs-layers, `inputs` are always concrete tensor\n        // values. Hence the inputs can't have an undetermined first (batch)\n        // dimension, which is why we always use the K.rnn approach here.\n\n        var step = function step(inputs, states) {\n          // TODO(cais): Add useLearningPhase.\n          // NOTE(cais): `layer.call` may return a length-1 array of Tensor in\n          //   some cases (e.g., `layer` is a `Sequential` instance), which is\n          //   why `getExactlyOneTensor` is used below.\n          var output = getExactlyOneTensor(_this3.layer.call(inputs, kwargs));\n          return [output, []];\n        };\n\n        var rnnOutputs = rnn(step, inputs, [], false\n        /* goBackwards */\n        , null\n        /* mask */\n        , null\n        /* constants */\n        , false\n        /* unroll */\n        , true\n        /* needPerStepOutputs */\n        );\n        var y = rnnOutputs[1]; // TODO(cais): Add activity regularization.\n        // TODO(cais): Add useLearningPhase.\n\n        return y;\n      });\n    }\n  }]);\n\n  return TimeDistributed;\n}(Wrapper);\n/** @nocollapse */\n\nTimeDistributed.className = 'TimeDistributed';\nserialization.registerClass(TimeDistributed);\nexport function checkBidirectionalMergeMode(value) {\n  generic_utils.checkStringTypeUnionValue(VALID_BIDIRECTIONAL_MERGE_MODES, 'BidirectionalMergeMode', value);\n}\nvar DEFAULT_BIDIRECTIONAL_MERGE_MODE = 'concat';\nexport var Bidirectional = /*#__PURE__*/function (_Wrapper2) {\n  _inherits(Bidirectional, _Wrapper2);\n\n  var _super3 = _createSuper(Bidirectional);\n\n  function Bidirectional(args) {\n    var _this4;\n\n    _classCallCheck(this, Bidirectional);\n\n    _this4 = _super3.call(this, args); // Note: When creating `this.forwardLayer`, the original Layer object\n    //   (`config.layer`) ought to be cloned. This is why we call\n    //   `getConfig()` followed by `deserialize()`. Without this cloning,\n    //   the layer names saved during serialization will incorrectly contain\n    //   the 'forward_' prefix. In Python Keras, this is done using\n    //   `copy.copy` (shallow copy), which does not have a simple equivalent\n    //   in JavaScript. JavaScript's `Object.assign()` does not copy\n    //   methods.\n\n    var layerConfig = args.layer.getConfig();\n    var forwDict = {};\n    forwDict['className'] = args.layer.getClassName();\n    forwDict['config'] = layerConfig;\n    _this4.forwardLayer = deserialize(forwDict);\n    layerConfig['goBackwards'] = layerConfig['goBackwards'] === true ? false : true;\n    var backDict = {};\n    backDict['className'] = args.layer.getClassName();\n    backDict['config'] = layerConfig;\n    _this4.backwardLayer = deserialize(backDict);\n    _this4.forwardLayer.name = 'forward_' + _this4.forwardLayer.name;\n    _this4.backwardLayer.name = 'backward_' + _this4.backwardLayer.name;\n    _this4.mergeMode = args.mergeMode === undefined ? DEFAULT_BIDIRECTIONAL_MERGE_MODE : args.mergeMode;\n    checkBidirectionalMergeMode(_this4.mergeMode);\n\n    if (args.weights) {\n      throw new NotImplementedError('weights support is not implemented for Bidirectional layer yet.');\n    }\n\n    _this4._stateful = args.layer.stateful;\n    _this4.returnSequences = args.layer.returnSequences;\n    _this4.returnState = args.layer.returnState;\n    _this4.supportsMasking = true;\n    _this4._trainable = true;\n    _this4.inputSpec = args.layer.inputSpec;\n    _this4.numConstants = null;\n    return _this4;\n  }\n\n  _createClass(Bidirectional, [{\n    key: \"trainable\",\n    get: function get() {\n      return this._trainable;\n    },\n    set: function set(value) {\n      // Porting Note: the check of `this.layer` here is necessary due to the\n      //   way the `constructor` of this class is written (see Porting Note\n      //   above).\n      this._trainable = value;\n\n      if (this.forwardLayer != null) {\n        this.forwardLayer.trainable = value;\n      }\n\n      if (this.backwardLayer != null) {\n        this.backwardLayer.trainable = value;\n      }\n    }\n  }, {\n    key: \"getWeights\",\n    value: function getWeights() {\n      return this.forwardLayer.getWeights().concat(this.backwardLayer.getWeights());\n    }\n  }, {\n    key: \"setWeights\",\n    value: function setWeights(weights) {\n      var numWeights = weights.length;\n      var numeightsOver2 = Math.floor(numWeights / 2);\n      this.forwardLayer.setWeights(weights.slice(0, numeightsOver2));\n      this.backwardLayer.setWeights(weights.slice(numeightsOver2));\n    }\n  }, {\n    key: \"computeOutputShape\",\n    value: function computeOutputShape(inputShape) {\n      var layerShapes = this.forwardLayer.computeOutputShape(inputShape);\n\n      if (!(Array.isArray(layerShapes) && Array.isArray(layerShapes[0]))) {\n        layerShapes = [layerShapes];\n      }\n\n      layerShapes = layerShapes;\n      var outputShape;\n      var outputShapes;\n      var stateShape;\n\n      if (this.returnState) {\n        stateShape = layerShapes.slice(1);\n        outputShape = layerShapes[0];\n      } else {\n        outputShape = layerShapes[0];\n      }\n\n      outputShape = outputShape;\n\n      if (this.mergeMode === 'concat') {\n        outputShape[outputShape.length - 1] *= 2;\n        outputShapes = [outputShape];\n      } else if (this.mergeMode == null) {\n        outputShapes = [outputShape, outputShape.slice()];\n      } else {\n        outputShapes = [outputShape];\n      }\n\n      if (this.returnState) {\n        if (this.mergeMode == null) {\n          return outputShapes.concat(stateShape).concat(stateShape.slice());\n        }\n\n        return [outputShape].concat(stateShape).concat(stateShape.slice());\n      }\n\n      return generic_utils.singletonOrArray(outputShapes);\n    }\n  }, {\n    key: \"apply\",\n    value: function apply(inputs, kwargs) {\n      var initialState = kwargs == null ? null : kwargs['initialState'];\n      var constants = kwargs == null ? null : kwargs['constants'];\n\n      if (kwargs == null) {\n        kwargs = {};\n      }\n\n      var standardized = standardizeArgs(inputs, initialState, constants, this.numConstants);\n      inputs = standardized.inputs;\n      initialState = standardized.initialState;\n      constants = standardized.constants;\n\n      if (Array.isArray(inputs)) {\n        initialState = inputs.slice(1);\n        inputs = inputs[0];\n      }\n\n      if ((initialState == null || initialState.length === 0) && constants == null) {\n        return _get(_getPrototypeOf(Bidirectional.prototype), \"apply\", this).call(this, inputs, kwargs);\n      }\n\n      var additionalInputs = [];\n      var additionalSpecs = [];\n\n      if (initialState != null) {\n        var numStates = initialState.length;\n\n        if (numStates % 2 > 0) {\n          throw new ValueError('When passing `initialState` to a Bidrectional RNN, ' + 'the state should be an Array containing the states of ' + 'the underlying RNNs.');\n        }\n\n        kwargs['initialState'] = initialState;\n        additionalInputs.push.apply(additionalInputs, _toConsumableArray(initialState));\n        var stateSpecs = initialState.map(function (state) {\n          return new InputSpec({\n            shape: state.shape\n          });\n        });\n        this.forwardLayer.stateSpec = stateSpecs.slice(0, numStates / 2);\n        this.backwardLayer.stateSpec = stateSpecs.slice(numStates / 2);\n        additionalSpecs.push.apply(additionalSpecs, _toConsumableArray(stateSpecs));\n      }\n\n      if (constants != null) {\n        throw new NotImplementedError('Support for constants in Bidirectional layers is not ' + 'implemented yet.');\n      }\n\n      var isSymbolicTensor = additionalInputs[0] instanceof SymbolicTensor;\n\n      for (var _i = 0, _additionalInputs = additionalInputs; _i < _additionalInputs.length; _i++) {\n        var tensor = _additionalInputs[_i];\n\n        if (tensor instanceof SymbolicTensor !== isSymbolicTensor) {\n          throw new ValueError('The initial state of a Bidirectional layer cannot be ' + 'specified as a mix of symbolic and non-symbolic tensors');\n        }\n      }\n\n      if (isSymbolicTensor) {\n        // Compute the full input and specs, including the states.\n        var fullInput = [inputs].concat(additionalInputs);\n        var fullInputSpec = this.inputSpec.concat(additionalSpecs); // Perform the call temporarily and replace inputSpec.\n        // Note: with initial states symbolic calls and non-symbolic calls to\n        // this method differ in how the initial states are passed. For\n        // symbolic calls, the initial states are passed in the first arg, as\n        // an Array of SymbolicTensors; for non-symbolic calls, they are\n        // passed in the second arg as a part of the kwargs. Hence the need to\n        // temporarily modify inputSpec here.\n        // TODO(cais): Make refactoring so that this hacky code below is no\n        // longer needed.\n\n        var originalInputSpec = this.inputSpec;\n        this.inputSpec = fullInputSpec;\n\n        var output = _get(_getPrototypeOf(Bidirectional.prototype), \"apply\", this).call(this, fullInput, kwargs);\n\n        this.inputSpec = originalInputSpec;\n        return output;\n      } else {\n        return _get(_getPrototypeOf(Bidirectional.prototype), \"apply\", this).call(this, inputs, kwargs);\n      }\n    }\n  }, {\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this5 = this;\n\n      return tidy(function () {\n        var initialState = kwargs['initialState'];\n        var y;\n        var yRev;\n\n        if (initialState == null) {\n          y = _this5.forwardLayer.call(inputs, kwargs);\n          yRev = _this5.backwardLayer.call(inputs, kwargs);\n        } else {\n          var forwardState = initialState.slice(0, initialState.length / 2);\n          var backwardState = initialState.slice(initialState.length / 2);\n          y = _this5.forwardLayer.call(inputs, Object.assign(kwargs, {\n            initialState: forwardState\n          }));\n          yRev = _this5.backwardLayer.call(inputs, Object.assign(kwargs, {\n            initialState: backwardState\n          }));\n        }\n\n        var states;\n\n        if (_this5.returnState) {\n          if (Array.isArray(y)) {\n            states = y.slice(1).concat(yRev.slice(1));\n          } else {}\n\n          y = y[0];\n          yRev = yRev[0];\n        }\n\n        if (_this5.returnSequences) {\n          yRev = tfc.reverse(yRev, 1);\n        }\n\n        var output;\n\n        if (_this5.mergeMode === 'concat') {\n          output = K.concatenate([y, yRev]);\n        } else if (_this5.mergeMode === 'sum') {\n          output = tfc.add(y, yRev);\n        } else if (_this5.mergeMode === 'ave') {\n          output = tfc.mul(.5, tfc.add(y, yRev));\n        } else if (_this5.mergeMode === 'mul') {\n          output = tfc.mul(y, yRev);\n        } else if (_this5.mergeMode == null) {\n          output = [y, yRev];\n        } // TODO(cais): Properly set learning phase.\n\n\n        if (_this5.returnState) {\n          if (_this5.mergeMode == null) {\n            return output.concat(states);\n          }\n\n          return [output].concat(states);\n        }\n\n        return output;\n      });\n    }\n  }, {\n    key: \"resetStates\",\n    value: function resetStates(states) {\n      this.forwardLayer.resetStates();\n      this.backwardLayer.resetStates();\n    }\n  }, {\n    key: \"build\",\n    value: function build(inputShape) {\n      var _this6 = this;\n\n      nameScope(this.forwardLayer.name, function () {\n        _this6.forwardLayer.build(inputShape);\n      });\n      nameScope(this.backwardLayer.name, function () {\n        _this6.backwardLayer.build(inputShape);\n      });\n      this.built = true;\n    }\n  }, {\n    key: \"computeMask\",\n    value: function computeMask(inputs, mask) {\n      if (Array.isArray(mask)) {\n        mask = mask[0];\n      }\n\n      var outputMask;\n\n      if (this.returnSequences) {\n        if (this.mergeMode == null) {\n          outputMask = [mask, mask];\n        } else {\n          outputMask = mask;\n        }\n      } else {\n        if (this.mergeMode == null) {\n          outputMask = [null, null];\n        } else {\n          outputMask = null;\n        }\n      }\n\n      if (this.returnState) {\n        var states = this.forwardLayer.states;\n        var stateMask = states.map(function (state) {\n          return null;\n        });\n\n        if (Array.isArray(outputMask)) {\n          return outputMask.concat(stateMask).concat(stateMask);\n        } else {\n          return [outputMask].concat(stateMask).concat(stateMask);\n        }\n      } else {\n        return outputMask;\n      }\n    }\n  }, {\n    key: \"trainableWeights\",\n    get: function get() {\n      return this.forwardLayer.trainableWeights.concat(this.backwardLayer.trainableWeights);\n    }\n  }, {\n    key: \"nonTrainableWeights\",\n    get: function get() {\n      return this.forwardLayer.nonTrainableWeights.concat(this.backwardLayer.nonTrainableWeights);\n    } // TODO(cais): Implement constraints().\n\n  }, {\n    key: \"setFastWeightInitDuringBuild\",\n    value: function setFastWeightInitDuringBuild(value) {\n      _get(_getPrototypeOf(Bidirectional.prototype), \"setFastWeightInitDuringBuild\", this).call(this, value);\n\n      if (this.forwardLayer != null) {\n        this.forwardLayer.setFastWeightInitDuringBuild(value);\n      }\n\n      if (this.backwardLayer != null) {\n        this.backwardLayer.setFastWeightInitDuringBuild(value);\n      }\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var config = {\n        'mergeMode': this.mergeMode\n      }; // TODO(cais): Add logic for `numConstants` once the property is added.\n\n      var baseConfig = _get(_getPrototypeOf(Bidirectional.prototype), \"getConfig\", this).call(this);\n\n      Object.assign(config, baseConfig);\n      return config;\n    }\n    /** @nocollapse */\n\n  }], [{\n    key: \"fromConfig\",\n    value: function fromConfig(cls, config) {\n      var rnnLayer = deserialize(config['layer']);\n      delete config['layer']; // TODO(cais): Add logic for `numConstants` once the property is added.\n\n      if (config['numConstants'] != null) {\n        throw new NotImplementedError(\"Deserialization of a Bidirectional layer with numConstants \" + \"present is not supported yet.\");\n      } // tslint:disable-next-line:no-any\n\n\n      var newConfig = config;\n      newConfig['layer'] = rnnLayer;\n      return new cls(newConfig);\n    }\n  }]);\n\n  return Bidirectional;\n}(Wrapper);\n/** @nocollapse */\n\nBidirectional.className = 'Bidirectional';\nserialization.registerClass(Bidirectional);","map":{"version":3,"sources":["../../src/layers/wrappers.ts"],"names":[],"mappings":";;;;;;;;AAAA;;;;;;;;AAQG;;AAEH;;AAEG;AAEH,OAAO,KAAK,GAAZ,MAAqB,uBAArB;AACA,SAAQ,aAAR,EAA+B,IAA/B,QAA0C,uBAA1C;AACA,OAAO,KAAK,CAAZ,MAAmB,yBAAnB;AACA,SAAQ,SAAR,QAAwB,WAAxB;AACA,SAAQ,SAAR,EAAmB,KAAnB,EAAqC,cAArC,QAA0D,oBAA1D;AACA,SAAQ,mBAAR,EAA6B,UAA7B,QAA8C,WAA9C;AACA,SAAuC,+BAAvC,QAA6E,wBAA7E;AAGA,OAAO,KAAK,aAAZ,MAA+B,wBAA/B;AACA,SAAQ,kBAAR,EAA4B,mBAA5B,QAAsD,sBAAtD;AAGA,SAAQ,GAAR,EAAkB,eAAlB,QAAwC,aAAxC;AACA,SAAQ,WAAR,QAA0B,iBAA1B;AASA;;;;;;AAMG;;AACH,WAAsB,OAAtB;AAAA;;AAAA;;AAGE,mBAAY,IAAZ,EAAkC;AAAA;;AAAA;;AAChC;AACA;AACA;AACA;AACA;AACA;AACA;AACA,8BAAM,IAAN;AACA,UAAK,KAAL,GAAa,IAAI,CAAC,KAAlB;AATgC;AAUjC;;AAbH;AAAA;AAAA,WAeE,eAAM,UAAN,EAA+B;AAC7B,WAAK,KAAL,GAAa,IAAb;AACD,KAjBH,CAmBE;;AAnBF;AAAA;AAAA,SAqBE,eAAa;AACX;AACA;AACA;AACA,UAAI,KAAK,KAAL,IAAc,IAAlB,EAAwB;AACtB,eAAO,KAAK,KAAL,CAAW,SAAlB;AACD,OAFD,MAEO;AACL,eAAO,KAAP;AACD;AACF,KA9BH;AAAA,SAgCE,aAAc,KAAd,EAA4B;AAC1B;AACA;AACA;AACA,UAAI,KAAK,KAAL,IAAc,IAAlB,EAAwB;AACtB,aAAK,KAAL,CAAW,SAAX,GAAuB,KAAvB;AACD;AACF;AAvCH;AAAA;AAAA,SAyCE,eAAoB;AAClB,aAAO,KAAK,KAAL,CAAW,gBAAlB;AACD,KA3CH,CA4CE;;AA5CF;AAAA;AAAA,SA8CE,eAAuB;AACrB,aAAO,KAAK,KAAL,CAAW,mBAAlB;AACD,KAhDH,CAiDE;;AAjDF;AAAA;AAAA,SAmDE,eAAW;AACT;AACA,aAAQ,KAAK,KAAL,CAAmB,QAA3B;AACD,KAtDH,CAwDE;;AAxDF;AAAA;AAAA,SA0DE,eAAU;AACR,aAAO,KAAK,KAAL,CAAW,MAAlB;AACD,KA5DH,CA8DE;;AA9DF;AAAA;AAAA,WAgEE,sBAAU;AACR,aAAO,KAAK,KAAL,CAAW,UAAX,EAAP;AACD;AAlEH;AAAA;AAAA,WAoEE,oBAAW,OAAX,EAA4B;AAC1B,WAAK,KAAL,CAAW,UAAX,CAAsB,OAAtB;AACD;AAtEH;AAAA;AAAA,WAwEE,qBAAS;AACP,UAAM,MAAM,GAA6B;AACvC,iBAAS;AACP,uBAAa,KAAK,KAAL,CAAW,YAAX,EADN;AAEP,oBAAU,KAAK,KAAL,CAAW,SAAX;AAFH;AAD8B,OAAzC;;AAMA,UAAM,UAAU,yEAAhB;;AACA,MAAA,MAAM,CAAC,MAAP,CAAc,MAAd,EAAsB,UAAtB;AACA,aAAO,MAAP;AACD;AAlFH;AAAA;AAAA,WAoFE,sCAA6B,KAA7B,EAA2C;AACzC,gGAAmC,KAAnC;;AACA,UAAI,KAAK,KAAL,IAAc,IAAlB,EAAwB;AACtB,aAAK,KAAL,CAAW,4BAAX,CAAwC,KAAxC;AACD;AACF;AAED;;AA3FF;AAAA;AAAA,WA4FE,oBACI,GADJ,EAEI,MAFJ,EAGkD;AAAA,UAA9C,aAA8C,uEAA9B,EAA8B;AAChD,UAAM,WAAW,GAAG,MAAM,CAAC,OAAD,CAA1B;AACA,UAAM,KAAK,GAAG,WAAW,CAAC,WAAD,EAAc,aAAd,CAAzB;AACA,aAAO,MAAM,CAAC,OAAD,CAAb;AACA,UAAM,SAAS,GAAG;AAAC,QAAA,KAAK,EAAL;AAAD,OAAlB;AACA,MAAA,MAAM,CAAC,MAAP,CAAc,SAAd,EAAyB,MAAzB;AACA,aAAO,IAAI,GAAJ,CAAQ,SAAR,CAAP;AACD;AAtGH;;AAAA;AAAA,EAAsC,KAAtC;AAyGA,WAAa,eAAb;AAAA;;AAAA;;AAGE,2BAAY,IAAZ,EAAkC;AAAA;;AAAA;;AAChC,gCAAM,IAAN;AACA,WAAK,eAAL,GAAuB,IAAvB;AAFgC;AAGjC;;AANH;AAAA;AAAA,WAQE,eAAM,UAAN,EAA+B;AAC7B,MAAA,UAAU,GAAG,kBAAkB,CAAC,UAAD,CAA/B;;AACA,UAAI,UAAU,CAAC,MAAX,GAAoB,CAAxB,EAA2B;AACzB,cAAM,IAAI,UAAJ,CACF,4FACe,IAAI,CAAC,SAAL,CAAe,UAAf,CADf,CADE,CAAN;AAGD;;AACD,WAAK,SAAL,GAAiB,CAAC;AAAC,QAAA,KAAK,EAAE;AAAR,OAAD,CAAjB;AACA,UAAM,eAAe,GAAG,CAAC,UAAU,CAAC,CAAD,CAAX,EAAgB,MAAhB,CAAuB,UAAU,CAAC,KAAX,CAAiB,CAAjB,CAAvB,CAAxB;;AACA,UAAI,CAAC,KAAK,KAAL,CAAW,KAAhB,EAAuB;AACrB,aAAK,KAAL,CAAW,KAAX,CAAiB,eAAjB;AACA,aAAK,KAAL,CAAW,KAAX,GAAmB,IAAnB;AACD;;AACD,iFAAY,UAAZ;AACD;AAtBH;AAAA;AAAA,WAwBE,4BAAmB,UAAnB,EAA4C;AAC1C,MAAA,UAAU,GAAG,kBAAkB,CAAC,UAAD,CAA/B;AACA,UAAM,eAAe,GAAG,CAAC,UAAU,CAAC,CAAD,CAAX,EAAgB,MAAhB,CAAuB,UAAU,CAAC,KAAX,CAAiB,CAAjB,CAAvB,CAAxB;AACA,UAAM,gBAAgB,GAClB,KAAK,KAAL,CAAW,kBAAX,CAA8B,eAA9B,CADJ;AAEA,UAAM,SAAS,GAAG,UAAU,CAAC,CAAD,CAA5B;AACA,aAAO,CAAC,gBAAgB,CAAC,CAAD,CAAjB,EAAsB,SAAtB,EAAiC,MAAjC,CAAwC,gBAAgB,CAAC,KAAjB,CAAuB,CAAvB,CAAxC,CAAP;AACD;AA/BH;AAAA;AAAA,WAiCE,cAAK,MAAL,EAA8B,MAA9B,EAA4C;AAAA;;AAC1C,aAAO,IAAI,CAAC,YAAK;AACf;AACA,QAAA,MAAM,GAAG,mBAAmB,CAAC,MAAD,CAA5B,CAFe,CAGf;AACA;AACA;;AACA,YAAM,IAAI,GAAoB,SAAxB,IAAwB,CAAC,MAAD,EAAiB,MAAjB,EAAqC;AACjE;AACA;AACA;AACA;AACA,cAAM,MAAM,GAAG,mBAAmB,CAAC,MAAI,CAAC,KAAL,CAAW,IAAX,CAAgB,MAAhB,EAAwB,MAAxB,CAAD,CAAlC;AACA,iBAAO,CAAC,MAAD,EAAS,EAAT,CAAP;AACD,SAPD;;AAQA,YAAM,UAAU,GACZ,GAAG,CAAC,IAAD,EAAO,MAAP,EAAe,EAAf,EAAmB;AAAM;AAAzB,UAA4C;AAAK;AAAjD,UACC;AAAK;AADN,UACuB;AAAM;AAD7B,UAEC;AAAK;AAFN,SADP;AAIA,YAAM,CAAC,GAAG,UAAU,CAAC,CAAD,CAApB,CAlBe,CAmBf;AACA;;AACA,eAAO,CAAP;AACD,OAtBU,CAAX;AAuBD;AAzDH;;AAAA;AAAA,EAAqC,OAArC;AACE;;AACO,eAAA,CAAA,SAAA,GAAY,iBAAZ;AA2DT,aAAa,CAAC,aAAd,CAA4B,eAA5B;AAEA,OAAM,SAAU,2BAAV,CAAsC,KAAtC,EAAoD;AACxD,EAAA,aAAa,CAAC,yBAAd,CACI,+BADJ,EACqC,wBADrC,EAC+D,KAD/D;AAED;AAkBD,IAAM,gCAAgC,GAA2B,QAAjE;AAEA,WAAa,aAAb;AAAA;;AAAA;;AAWE,yBAAY,IAAZ,EAAwC;AAAA;;AAAA;;AACtC,gCAAM,IAAN,EADsC,CAGtC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA,QAAM,WAAW,GAAG,IAAI,CAAC,KAAL,CAAW,SAAX,EAApB;AACA,QAAM,QAAQ,GAA6B,EAA3C;AACA,IAAA,QAAQ,CAAC,WAAD,CAAR,GAAwB,IAAI,CAAC,KAAL,CAAW,YAAX,EAAxB;AACA,IAAA,QAAQ,CAAC,QAAD,CAAR,GAAqB,WAArB;AACA,WAAK,YAAL,GAAoB,WAAW,CAAC,QAAD,CAA/B;AACA,IAAA,WAAW,CAAC,aAAD,CAAX,GACI,WAAW,CAAC,aAAD,CAAX,KAA+B,IAA/B,GAAsC,KAAtC,GAA8C,IADlD;AAEA,QAAM,QAAQ,GAA6B,EAA3C;AACA,IAAA,QAAQ,CAAC,WAAD,CAAR,GAAwB,IAAI,CAAC,KAAL,CAAW,YAAX,EAAxB;AACA,IAAA,QAAQ,CAAC,QAAD,CAAR,GAAqB,WAArB;AACA,WAAK,aAAL,GAAqB,WAAW,CAAC,QAAD,CAAhC;AACA,WAAK,YAAL,CAAkB,IAAlB,GAAyB,aAAa,OAAK,YAAL,CAAkB,IAAxD;AACA,WAAK,aAAL,CAAmB,IAAnB,GAA0B,cAAc,OAAK,aAAL,CAAmB,IAA3D;AAEA,WAAK,SAAL,GAAiB,IAAI,CAAC,SAAL,KAAmB,SAAnB,GACb,gCADa,GAEb,IAAI,CAAC,SAFT;AAGA,IAAA,2BAA2B,CAAC,OAAK,SAAN,CAA3B;;AACA,QAAI,IAAI,CAAC,OAAT,EAAkB;AAChB,YAAM,IAAI,mBAAJ,CACF,iEADE,CAAN;AAED;;AACD,WAAK,SAAL,GAAiB,IAAI,CAAC,KAAL,CAAW,QAA5B;AACA,WAAK,eAAL,GAAuB,IAAI,CAAC,KAAL,CAAW,eAAlC;AACA,WAAK,WAAL,GAAmB,IAAI,CAAC,KAAL,CAAW,WAA9B;AACA,WAAK,eAAL,GAAuB,IAAvB;AACA,WAAK,UAAL,GAAkB,IAAlB;AACA,WAAK,SAAL,GAAiB,IAAI,CAAC,KAAL,CAAW,SAA5B;AACA,WAAK,YAAL,GAAoB,IAApB;AAvCsC;AAwCvC;;AAnDH;AAAA;AAAA,SAqDE,eAAa;AACX,aAAO,KAAK,UAAZ;AACD,KAvDH;AAAA,SAyDE,aAAc,KAAd,EAA4B;AAC1B;AACA;AACA;AACA,WAAK,UAAL,GAAkB,KAAlB;;AACA,UAAI,KAAK,YAAL,IAAqB,IAAzB,EAA+B;AAC7B,aAAK,YAAL,CAAkB,SAAlB,GAA8B,KAA9B;AACD;;AACD,UAAI,KAAK,aAAL,IAAsB,IAA1B,EAAgC;AAC9B,aAAK,aAAL,CAAmB,SAAnB,GAA+B,KAA/B;AACD;AACF;AApEH;AAAA;AAAA,WAsEE,sBAAU;AACR,aAAO,KAAK,YAAL,CAAkB,UAAlB,GAA+B,MAA/B,CACH,KAAK,aAAL,CAAmB,UAAnB,EADG,CAAP;AAED;AAzEH;AAAA;AAAA,WA2EE,oBAAW,OAAX,EAA4B;AAC1B,UAAM,UAAU,GAAG,OAAO,CAAC,MAA3B;AACA,UAAM,cAAc,GAAG,IAAI,CAAC,KAAL,CAAW,UAAU,GAAG,CAAxB,CAAvB;AACA,WAAK,YAAL,CAAkB,UAAlB,CAA6B,OAAO,CAAC,KAAR,CAAc,CAAd,EAAiB,cAAjB,CAA7B;AACA,WAAK,aAAL,CAAmB,UAAnB,CAA8B,OAAO,CAAC,KAAR,CAAc,cAAd,CAA9B;AACD;AAhFH;AAAA;AAAA,WAkFE,4BAAmB,UAAnB,EAA4C;AAC1C,UAAI,WAAW,GACX,KAAK,YAAL,CAAkB,kBAAlB,CAAqC,UAArC,CADJ;;AAEA,UAAI,EAAE,KAAK,CAAC,OAAN,CAAc,WAAd,KAA8B,KAAK,CAAC,OAAN,CAAc,WAAW,CAAC,CAAD,CAAzB,CAAhC,CAAJ,EAAoE;AAClE,QAAA,WAAW,GAAG,CAAC,WAAD,CAAd;AACD;;AACD,MAAA,WAAW,GAAG,WAAd;AAEA,UAAI,WAAJ;AACA,UAAI,YAAJ;AACA,UAAI,UAAJ;;AACA,UAAI,KAAK,WAAT,EAAsB;AACpB,QAAA,UAAU,GAAG,WAAW,CAAC,KAAZ,CAAkB,CAAlB,CAAb;AACA,QAAA,WAAW,GAAG,WAAW,CAAC,CAAD,CAAzB;AACD,OAHD,MAGO;AACL,QAAA,WAAW,GAAG,WAAW,CAAC,CAAD,CAAzB;AACD;;AACD,MAAA,WAAW,GAAG,WAAd;;AACA,UAAI,KAAK,SAAL,KAAmB,QAAvB,EAAiC;AAC/B,QAAA,WAAW,CAAC,WAAW,CAAC,MAAZ,GAAqB,CAAtB,CAAX,IAAuC,CAAvC;AACA,QAAA,YAAY,GAAG,CAAC,WAAD,CAAf;AACD,OAHD,MAGO,IAAI,KAAK,SAAL,IAAkB,IAAtB,EAA4B;AACjC,QAAA,YAAY,GAAG,CAAC,WAAD,EAAc,WAAW,CAAC,KAAZ,EAAd,CAAf;AACD,OAFM,MAEA;AACL,QAAA,YAAY,GAAG,CAAC,WAAD,CAAf;AACD;;AAED,UAAI,KAAK,WAAT,EAAsB;AACpB,YAAI,KAAK,SAAL,IAAkB,IAAtB,EAA4B;AAC1B,iBAAO,YAAY,CAAC,MAAb,CAAoB,UAApB,EAAgC,MAAhC,CAAuC,UAAU,CAAC,KAAX,EAAvC,CAAP;AACD;;AACD,eAAO,CAAC,WAAD,EAAc,MAAd,CAAqB,UAArB,EAAiC,MAAjC,CAAwC,UAAU,CAAC,KAAX,EAAxC,CAAP;AACD;;AACD,aAAO,aAAa,CAAC,gBAAd,CAA+B,YAA/B,CAAP;AACD;AApHH;AAAA;AAAA,WAsHE,eACI,MADJ,EAEI,MAFJ,EAEmB;AACjB,UAAI,YAAY,GACZ,MAAM,IAAI,IAAV,GAAiB,IAAjB,GAAwB,MAAM,CAAC,cAAD,CADlC;AAEA,UAAI,SAAS,GACT,MAAM,IAAI,IAAV,GAAiB,IAAjB,GAAwB,MAAM,CAAC,WAAD,CADlC;;AAEA,UAAI,MAAM,IAAI,IAAd,EAAoB;AAClB,QAAA,MAAM,GAAG,EAAT;AACD;;AACD,UAAM,YAAY,GACd,eAAe,CAAC,MAAD,EAAS,YAAT,EAAuB,SAAvB,EAAkC,KAAK,YAAvC,CADnB;AAEA,MAAA,MAAM,GAAG,YAAY,CAAC,MAAtB;AACA,MAAA,YAAY,GAAG,YAAY,CAAC,YAA5B;AACA,MAAA,SAAS,GAAG,YAAY,CAAC,SAAzB;;AAEA,UAAI,KAAK,CAAC,OAAN,CAAc,MAAd,CAAJ,EAA2B;AACzB,QAAA,YAAY,GAAI,MAAsC,CAAC,KAAvC,CAA6C,CAA7C,CAAhB;AACA,QAAA,MAAM,GAAI,MAAsC,CAAC,CAAD,CAAhD;AACD;;AAED,UAAI,CAAC,YAAY,IAAI,IAAhB,IAAwB,YAAY,CAAC,MAAb,KAAwB,CAAjD,KACA,SAAS,IAAI,IADjB,EACuB;AACrB,wFAAmB,MAAnB,EAA2B,MAA3B;AACD;;AACD,UAAM,gBAAgB,GAAiC,EAAvD;AACA,UAAM,eAAe,GAAgB,EAArC;;AACA,UAAI,YAAY,IAAI,IAApB,EAA0B;AACxB,YAAM,SAAS,GAAG,YAAY,CAAC,MAA/B;;AACA,YAAI,SAAS,GAAG,CAAZ,GAAgB,CAApB,EAAuB;AACrB,gBAAM,IAAI,UAAJ,CACF,wDACA,wDADA,GAEA,sBAHE,CAAN;AAID;;AACD,QAAA,MAAM,CAAC,cAAD,CAAN,GAAyB,YAAzB;AACA,QAAA,gBAAgB,CAAC,IAAjB,OAAA,gBAAgB,qBAAS,YAAT,EAAhB;AACA,YAAM,UAAU,GAAI,YAA6C,CACzC,GADJ,CACQ,UAAA,KAAK;AAAA,iBAAI,IAAI,SAAJ,CAAc;AAAC,YAAA,KAAK,EAAE,KAAK,CAAC;AAAd,WAAd,CAAJ;AAAA,SADb,CAApB;AAEA,aAAK,YAAL,CAAkB,SAAlB,GAA8B,UAAU,CAAC,KAAX,CAAiB,CAAjB,EAAoB,SAAS,GAAG,CAAhC,CAA9B;AACA,aAAK,aAAL,CAAmB,SAAnB,GAA+B,UAAU,CAAC,KAAX,CAAiB,SAAS,GAAG,CAA7B,CAA/B;AACA,QAAA,eAAe,CAAC,IAAhB,OAAA,eAAe,qBAAS,UAAT,EAAf;AACD;;AACD,UAAI,SAAS,IAAI,IAAjB,EAAuB;AACrB,cAAM,IAAI,mBAAJ,CACF,0DACA,kBAFE,CAAN;AAGD;;AAED,UAAM,gBAAgB,GAAG,gBAAgB,CAAC,CAAD,CAAhB,YAA+B,cAAxD;;AACA,2CAAqB,gBAArB,uCAAuC;AAAlC,YAAM,MAAM,wBAAZ;;AACH,YAAI,MAAM,YAAY,cAAlB,KAAqC,gBAAzC,EAA2D;AACzD,gBAAM,IAAI,UAAJ,CACF,0DACA,yDAFE,CAAN;AAGD;AACF;;AAED,UAAI,gBAAJ,EAAsB;AACpB;AACA,YAAM,SAAS,GAAG,CAAC,MAAD,EAAS,MAAT,CAAgB,gBAAhB,CAAlB;AACA,YAAM,aAAa,GAAG,KAAK,SAAL,CAAe,MAAf,CAAsB,eAAtB,CAAtB,CAHoB,CAIpB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA,YAAM,iBAAiB,GAAG,KAAK,SAA/B;AACA,aAAK,SAAL,GAAiB,aAAjB;;AACA,YAAM,MAAM,4EACI,SADJ,EAC8C,MAD9C,CAAZ;;AAEA,aAAK,SAAL,GAAiB,iBAAjB;AACA,eAAO,MAAP;AACD,OAnBD,MAmBO;AACL,wFAAmB,MAAnB,EAA2B,MAA3B;AACD;AACF;AAtMH;AAAA;AAAA,WAwME,cAAK,MAAL,EAA8B,MAA9B,EAA4C;AAAA;;AAC1C,aAAO,IAAI,CAAC,YAAK;AACf,YAAM,YAAY,GAAG,MAAM,CAAC,cAAD,CAA3B;AAEA,YAAI,CAAJ;AACA,YAAI,IAAJ;;AACA,YAAI,YAAY,IAAI,IAApB,EAA0B;AACxB,UAAA,CAAC,GAAG,MAAI,CAAC,YAAL,CAAkB,IAAlB,CAAuB,MAAvB,EAA+B,MAA/B,CAAJ;AACA,UAAA,IAAI,GAAG,MAAI,CAAC,aAAL,CAAmB,IAAnB,CAAwB,MAAxB,EAAgC,MAAhC,CAAP;AACD,SAHD,MAGO;AACL,cAAM,YAAY,GAAG,YAAY,CAAC,KAAb,CAAmB,CAAnB,EAAsB,YAAY,CAAC,MAAb,GAAsB,CAA5C,CAArB;AACA,cAAM,aAAa,GAAG,YAAY,CAAC,KAAb,CAAmB,YAAY,CAAC,MAAb,GAAsB,CAAzC,CAAtB;AACA,UAAA,CAAC,GAAG,MAAI,CAAC,YAAL,CAAkB,IAAlB,CACA,MADA,EACQ,MAAM,CAAC,MAAP,CAAc,MAAd,EAAsB;AAAC,YAAA,YAAY,EAAE;AAAf,WAAtB,CADR,CAAJ;AAEA,UAAA,IAAI,GAAG,MAAI,CAAC,aAAL,CAAmB,IAAnB,CACH,MADG,EACK,MAAM,CAAC,MAAP,CAAc,MAAd,EAAsB;AAAC,YAAA,YAAY,EAAE;AAAf,WAAtB,CADL,CAAP;AAED;;AAED,YAAI,MAAJ;;AACA,YAAI,MAAI,CAAC,WAAT,EAAsB;AACpB,cAAI,KAAK,CAAC,OAAN,CAAc,CAAd,CAAJ,EAAsB;AACpB,YAAA,MAAM,GAAG,CAAC,CAAC,KAAF,CAAQ,CAAR,EAAW,MAAX,CAAmB,IAAiB,CAAC,KAAlB,CAAwB,CAAxB,CAAnB,CAAT;AACD,WAFD,MAEO,CACN;;AACD,UAAA,CAAC,GAAI,CAAc,CAAC,CAAD,CAAnB;AACA,UAAA,IAAI,GAAI,IAAiB,CAAC,CAAD,CAAzB;AACD;;AAED,YAAI,MAAI,CAAC,eAAT,EAA0B;AACxB,UAAA,IAAI,GAAG,GAAG,CAAC,OAAJ,CAAY,IAAZ,EAA4B,CAA5B,CAAP;AACD;;AAED,YAAI,MAAJ;;AACA,YAAI,MAAI,CAAC,SAAL,KAAmB,QAAvB,EAAiC;AAC/B,UAAA,MAAM,GAAG,CAAC,CAAC,WAAF,CAAc,CAAC,CAAD,EAAc,IAAd,CAAd,CAAT;AACD,SAFD,MAEO,IAAI,MAAI,CAAC,SAAL,KAAmB,KAAvB,EAA8B;AACnC,UAAA,MAAM,GAAG,GAAG,CAAC,GAAJ,CAAQ,CAAR,EAAqB,IAArB,CAAT;AACD,SAFM,MAEA,IAAI,MAAI,CAAC,SAAL,KAAmB,KAAvB,EAA8B;AACnC,UAAA,MAAM,GAAG,GAAG,CAAC,GAAJ,CAAQ,EAAR,EAAY,GAAG,CAAC,GAAJ,CAAQ,CAAR,EAAqB,IAArB,CAAZ,CAAT;AACD,SAFM,MAEA,IAAI,MAAI,CAAC,SAAL,KAAmB,KAAvB,EAA8B;AACnC,UAAA,MAAM,GAAG,GAAG,CAAC,GAAJ,CAAQ,CAAR,EAAqB,IAArB,CAAT;AACD,SAFM,MAEA,IAAI,MAAI,CAAC,SAAL,IAAkB,IAAtB,EAA4B;AACjC,UAAA,MAAM,GAAG,CAAC,CAAD,EAAc,IAAd,CAAT;AACD,SA1Cc,CA4Cf;;;AACA,YAAI,MAAI,CAAC,WAAT,EAAsB;AACpB,cAAI,MAAI,CAAC,SAAL,IAAkB,IAAtB,EAA4B;AAC1B,mBAAQ,MAAmB,CAAC,MAApB,CAA2B,MAA3B,CAAR;AACD;;AACD,iBAAO,CAAC,MAAD,EAAmB,MAAnB,CAA0B,MAA1B,CAAP;AACD;;AACD,eAAO,MAAP;AACD,OApDU,CAAX;AAqDD;AA9PH;AAAA;AAAA,WAgQE,qBAAY,MAAZ,EAAoC;AAClC,WAAK,YAAL,CAAkB,WAAlB;AACA,WAAK,aAAL,CAAmB,WAAnB;AACD;AAnQH;AAAA;AAAA,WAqQE,eAAM,UAAN,EAA+B;AAAA;;AAC7B,MAAA,SAAS,CAAC,KAAK,YAAL,CAAkB,IAAnB,EAAyB,YAAK;AACrC,QAAA,MAAI,CAAC,YAAL,CAAkB,KAAlB,CAAwB,UAAxB;AACD,OAFQ,CAAT;AAGA,MAAA,SAAS,CAAC,KAAK,aAAL,CAAmB,IAApB,EAA0B,YAAK;AACtC,QAAA,MAAI,CAAC,aAAL,CAAmB,KAAnB,CAAyB,UAAzB;AACD,OAFQ,CAAT;AAGA,WAAK,KAAL,GAAa,IAAb;AACD;AA7QH;AAAA;AAAA,WA+QE,qBAAY,MAAZ,EAAqC,IAArC,EAA2D;AAEzD,UAAI,KAAK,CAAC,OAAN,CAAc,IAAd,CAAJ,EAAyB;AACvB,QAAA,IAAI,GAAG,IAAI,CAAC,CAAD,CAAX;AACD;;AACD,UAAI,UAAJ;;AACA,UAAI,KAAK,eAAT,EAA0B;AACxB,YAAI,KAAK,SAAL,IAAkB,IAAtB,EAA4B;AAC1B,UAAA,UAAU,GAAG,CAAC,IAAD,EAAO,IAAP,CAAb;AACD,SAFD,MAEO;AACL,UAAA,UAAU,GAAG,IAAb;AACD;AACF,OAND,MAMO;AACL,YAAI,KAAK,SAAL,IAAkB,IAAtB,EAA4B;AAC1B,UAAA,UAAU,GAAG,CAAC,IAAD,EAAO,IAAP,CAAb;AACD,SAFD,MAEO;AACL,UAAA,UAAU,GAAG,IAAb;AACD;AACF;;AACD,UAAI,KAAK,WAAT,EAAsB;AACpB,YAAM,MAAM,GAAG,KAAK,YAAL,CAAkB,MAAjC;AACA,YAAM,SAAS,GAAa,MAAM,CAAC,GAAP,CAAW,UAAA,KAAK;AAAA,iBAAI,IAAJ;AAAA,SAAhB,CAA5B;;AACA,YAAI,KAAK,CAAC,OAAN,CAAc,UAAd,CAAJ,EAA+B;AAC7B,iBAAO,UAAU,CAAC,MAAX,CAAkB,SAAlB,EAA6B,MAA7B,CAAoC,SAApC,CAAP;AACD,SAFD,MAEO;AACL,iBAAO,CAAC,UAAD,EAAa,MAAb,CAAoB,SAApB,EAA+B,MAA/B,CAAsC,SAAtC,CAAP;AACD;AACF,OARD,MAQO;AACL,eAAO,UAAP;AACD;AACF;AA7SH;AAAA;AAAA,SA+SE,eAAoB;AAClB,aAAO,KAAK,YAAL,CAAkB,gBAAlB,CAAmC,MAAnC,CACH,KAAK,aAAL,CAAmB,gBADhB,CAAP;AAED;AAlTH;AAAA;AAAA,SAoTE,eAAuB;AACrB,aAAO,KAAK,YAAL,CAAkB,mBAAlB,CAAsC,MAAtC,CACH,KAAK,aAAL,CAAmB,mBADhB,CAAP;AAED,KAvTH,CAyTE;;AAzTF;AAAA;AAAA,WA2TE,sCAA6B,KAA7B,EAA2C;AACzC,sGAAmC,KAAnC;;AACA,UAAI,KAAK,YAAL,IAAqB,IAAzB,EAA+B;AAC7B,aAAK,YAAL,CAAkB,4BAAlB,CAA+C,KAA/C;AACD;;AACD,UAAI,KAAK,aAAL,IAAsB,IAA1B,EAAgC;AAC9B,aAAK,aAAL,CAAmB,4BAAnB,CAAgD,KAAhD;AACD;AACF;AAnUH;AAAA;AAAA,WAqUE,qBAAS;AACP,UAAM,MAAM,GAA6B;AACvC,qBAAa,KAAK;AADqB,OAAzC,CADO,CAIP;;AACA,UAAM,UAAU,+EAAhB;;AACA,MAAA,MAAM,CAAC,MAAP,CAAc,MAAd,EAAsB,UAAtB;AACA,aAAO,MAAP;AACD;AAED;;AA/UF;AAAA;AAAA,WAgVE,oBACI,GADJ,EAEI,MAFJ,EAEoC;AAClC,UAAM,QAAQ,GACV,WAAW,CAAC,MAAM,CAAC,OAAD,CAAP,CADf;AAEA,aAAO,MAAM,CAAC,OAAD,CAAb,CAHkC,CAIlC;;AACA,UAAI,MAAM,CAAC,cAAD,CAAN,IAA0B,IAA9B,EAAoC;AAClC,cAAM,IAAI,mBAAJ,CACF,+FADE,CAAN;AAGD,OATiC,CAUlC;;;AACA,UAAM,SAAS,GAAyB,MAAxC;AACA,MAAA,SAAS,CAAC,OAAD,CAAT,GAAqB,QAArB;AACA,aAAO,IAAI,GAAJ,CAAQ,SAAR,CAAP;AACD;AAhWH;;AAAA;AAAA,EAAmC,OAAnC;AACE;;AACO,aAAA,CAAA,SAAA,GAAY,eAAZ;AAgWT,aAAa,CAAC,aAAd,CAA4B,aAA5B","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * Layers that augment the functionality of a base layer.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { serialization, tidy } from '@tensorflow/tfjs-core';\nimport * as K from '../backend/tfjs_backend';\nimport { nameScope } from '../common';\nimport { InputSpec, Layer, SymbolicTensor } from '../engine/topology';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { VALID_BIDIRECTIONAL_MERGE_MODES } from '../keras_format/common';\nimport * as generic_utils from '../utils/generic_utils';\nimport { getExactlyOneShape, getExactlyOneTensor } from '../utils/types_utils';\nimport { rnn, standardizeArgs } from './recurrent';\nimport { deserialize } from './serialization';\n/**\n * Abstract wrapper base class.\n *\n * Wrappers take another layer and augment it in various ways.\n * Do not use this class as a layer, it is only an abstract base class.\n * Two usable wrappers are the `TimeDistributed` and `Bidirectional` wrappers.\n */\nexport class Wrapper extends Layer {\n    constructor(args) {\n        // Porting Note: In PyKeras, `self.layer` is set prior to the calling\n        //   `super()`. But we can't do that here due to TypeScript's restriction.\n        //   See: https://github.com/Microsoft/TypeScript/issues/8277\n        //   As a result, we have to add checks in `get trainable()` and\n        //   `set trainable()` below in order to prevent using `this.layer` when\n        //   its value is `undefined`. The super constructor does use the getter\n        //   and the setter of `this.layer`.\n        super(args);\n        this.layer = args.layer;\n    }\n    build(inputShape) {\n        this.built = true;\n    }\n    // TODO(cais): Implement activityRegularizer getter.\n    get trainable() {\n        // Porting Note: the check of `this.layer` here is necessary due to the\n        //   way the `constructor` of this class is written (see Porting Note\n        //   above).\n        if (this.layer != null) {\n            return this.layer.trainable;\n        }\n        else {\n            return false;\n        }\n    }\n    set trainable(value) {\n        // Porting Note: the check of `this.layer` here is necessary due to the\n        //   way the `constructor` of this class is written (see Porting Note\n        //   above).\n        if (this.layer != null) {\n            this.layer.trainable = value;\n        }\n    }\n    get trainableWeights() {\n        return this.layer.trainableWeights;\n    }\n    // TODO(cais): Implement setter for trainableWeights.\n    get nonTrainableWeights() {\n        return this.layer.nonTrainableWeights;\n    }\n    // TODO(cais): Implement setter for nonTrainableWeights.\n    get updates() {\n        // tslint:disable-next-line:no-any\n        return this.layer._updates;\n    }\n    // TODO(cais): Implement getUpdatesFor().\n    get losses() {\n        return this.layer.losses;\n    }\n    // TODO(cais): Implement getLossesFor().\n    getWeights() {\n        return this.layer.getWeights();\n    }\n    setWeights(weights) {\n        this.layer.setWeights(weights);\n    }\n    getConfig() {\n        const config = {\n            'layer': {\n                'className': this.layer.getClassName(),\n                'config': this.layer.getConfig(),\n            }\n        };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n    setFastWeightInitDuringBuild(value) {\n        super.setFastWeightInitDuringBuild(value);\n        if (this.layer != null) {\n            this.layer.setFastWeightInitDuringBuild(value);\n        }\n    }\n    /** @nocollapse */\n    static fromConfig(cls, config, customObjects = {}) {\n        const layerConfig = config['layer'];\n        const layer = deserialize(layerConfig, customObjects);\n        delete config['layer'];\n        const newConfig = { layer };\n        Object.assign(newConfig, config);\n        return new cls(newConfig);\n    }\n}\nexport class TimeDistributed extends Wrapper {\n    constructor(args) {\n        super(args);\n        this.supportsMasking = true;\n    }\n    build(inputShape) {\n        inputShape = getExactlyOneShape(inputShape);\n        if (inputShape.length < 3) {\n            throw new ValueError(`TimeDistributed layer expects an input shape >= 3D, but received ` +\n                `input shape ${JSON.stringify(inputShape)}`);\n        }\n        this.inputSpec = [{ shape: inputShape }];\n        const childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n        if (!this.layer.built) {\n            this.layer.build(childInputShape);\n            this.layer.built = true;\n        }\n        super.build(inputShape);\n    }\n    computeOutputShape(inputShape) {\n        inputShape = getExactlyOneShape(inputShape);\n        const childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n        const childOutputShape = this.layer.computeOutputShape(childInputShape);\n        const timesteps = inputShape[1];\n        return [childOutputShape[0], timesteps].concat(childOutputShape.slice(1));\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            // TODO(cais): Add 'training' and 'useLearningPhase' to kwargs.\n            inputs = getExactlyOneTensor(inputs);\n            // Porting Note: In tfjs-layers, `inputs` are always concrete tensor\n            // values. Hence the inputs can't have an undetermined first (batch)\n            // dimension, which is why we always use the K.rnn approach here.\n            const step = (inputs, states) => {\n                // TODO(cais): Add useLearningPhase.\n                // NOTE(cais): `layer.call` may return a length-1 array of Tensor in\n                //   some cases (e.g., `layer` is a `Sequential` instance), which is\n                //   why `getExactlyOneTensor` is used below.\n                const output = getExactlyOneTensor(this.layer.call(inputs, kwargs));\n                return [output, []];\n            };\n            const rnnOutputs = rnn(step, inputs, [], false /* goBackwards */, null /* mask */, null /* constants */, false /* unroll */, true /* needPerStepOutputs */);\n            const y = rnnOutputs[1];\n            // TODO(cais): Add activity regularization.\n            // TODO(cais): Add useLearningPhase.\n            return y;\n        });\n    }\n}\n/** @nocollapse */\nTimeDistributed.className = 'TimeDistributed';\nserialization.registerClass(TimeDistributed);\nexport function checkBidirectionalMergeMode(value) {\n    generic_utils.checkStringTypeUnionValue(VALID_BIDIRECTIONAL_MERGE_MODES, 'BidirectionalMergeMode', value);\n}\nconst DEFAULT_BIDIRECTIONAL_MERGE_MODE = 'concat';\nexport class Bidirectional extends Wrapper {\n    constructor(args) {\n        super(args);\n        // Note: When creating `this.forwardLayer`, the original Layer object\n        //   (`config.layer`) ought to be cloned. This is why we call\n        //   `getConfig()` followed by `deserialize()`. Without this cloning,\n        //   the layer names saved during serialization will incorrectly contain\n        //   the 'forward_' prefix. In Python Keras, this is done using\n        //   `copy.copy` (shallow copy), which does not have a simple equivalent\n        //   in JavaScript. JavaScript's `Object.assign()` does not copy\n        //   methods.\n        const layerConfig = args.layer.getConfig();\n        const forwDict = {};\n        forwDict['className'] = args.layer.getClassName();\n        forwDict['config'] = layerConfig;\n        this.forwardLayer = deserialize(forwDict);\n        layerConfig['goBackwards'] =\n            layerConfig['goBackwards'] === true ? false : true;\n        const backDict = {};\n        backDict['className'] = args.layer.getClassName();\n        backDict['config'] = layerConfig;\n        this.backwardLayer = deserialize(backDict);\n        this.forwardLayer.name = 'forward_' + this.forwardLayer.name;\n        this.backwardLayer.name = 'backward_' + this.backwardLayer.name;\n        this.mergeMode = args.mergeMode === undefined ?\n            DEFAULT_BIDIRECTIONAL_MERGE_MODE :\n            args.mergeMode;\n        checkBidirectionalMergeMode(this.mergeMode);\n        if (args.weights) {\n            throw new NotImplementedError('weights support is not implemented for Bidirectional layer yet.');\n        }\n        this._stateful = args.layer.stateful;\n        this.returnSequences = args.layer.returnSequences;\n        this.returnState = args.layer.returnState;\n        this.supportsMasking = true;\n        this._trainable = true;\n        this.inputSpec = args.layer.inputSpec;\n        this.numConstants = null;\n    }\n    get trainable() {\n        return this._trainable;\n    }\n    set trainable(value) {\n        // Porting Note: the check of `this.layer` here is necessary due to the\n        //   way the `constructor` of this class is written (see Porting Note\n        //   above).\n        this._trainable = value;\n        if (this.forwardLayer != null) {\n            this.forwardLayer.trainable = value;\n        }\n        if (this.backwardLayer != null) {\n            this.backwardLayer.trainable = value;\n        }\n    }\n    getWeights() {\n        return this.forwardLayer.getWeights().concat(this.backwardLayer.getWeights());\n    }\n    setWeights(weights) {\n        const numWeights = weights.length;\n        const numeightsOver2 = Math.floor(numWeights / 2);\n        this.forwardLayer.setWeights(weights.slice(0, numeightsOver2));\n        this.backwardLayer.setWeights(weights.slice(numeightsOver2));\n    }\n    computeOutputShape(inputShape) {\n        let layerShapes = this.forwardLayer.computeOutputShape(inputShape);\n        if (!(Array.isArray(layerShapes) && Array.isArray(layerShapes[0]))) {\n            layerShapes = [layerShapes];\n        }\n        layerShapes = layerShapes;\n        let outputShape;\n        let outputShapes;\n        let stateShape;\n        if (this.returnState) {\n            stateShape = layerShapes.slice(1);\n            outputShape = layerShapes[0];\n        }\n        else {\n            outputShape = layerShapes[0];\n        }\n        outputShape = outputShape;\n        if (this.mergeMode === 'concat') {\n            outputShape[outputShape.length - 1] *= 2;\n            outputShapes = [outputShape];\n        }\n        else if (this.mergeMode == null) {\n            outputShapes = [outputShape, outputShape.slice()];\n        }\n        else {\n            outputShapes = [outputShape];\n        }\n        if (this.returnState) {\n            if (this.mergeMode == null) {\n                return outputShapes.concat(stateShape).concat(stateShape.slice());\n            }\n            return [outputShape].concat(stateShape).concat(stateShape.slice());\n        }\n        return generic_utils.singletonOrArray(outputShapes);\n    }\n    apply(inputs, kwargs) {\n        let initialState = kwargs == null ? null : kwargs['initialState'];\n        let constants = kwargs == null ? null : kwargs['constants'];\n        if (kwargs == null) {\n            kwargs = {};\n        }\n        const standardized = standardizeArgs(inputs, initialState, constants, this.numConstants);\n        inputs = standardized.inputs;\n        initialState = standardized.initialState;\n        constants = standardized.constants;\n        if (Array.isArray(inputs)) {\n            initialState = inputs.slice(1);\n            inputs = inputs[0];\n        }\n        if ((initialState == null || initialState.length === 0) &&\n            constants == null) {\n            return super.apply(inputs, kwargs);\n        }\n        const additionalInputs = [];\n        const additionalSpecs = [];\n        if (initialState != null) {\n            const numStates = initialState.length;\n            if (numStates % 2 > 0) {\n                throw new ValueError('When passing `initialState` to a Bidrectional RNN, ' +\n                    'the state should be an Array containing the states of ' +\n                    'the underlying RNNs.');\n            }\n            kwargs['initialState'] = initialState;\n            additionalInputs.push(...initialState);\n            const stateSpecs = initialState\n                .map(state => new InputSpec({ shape: state.shape }));\n            this.forwardLayer.stateSpec = stateSpecs.slice(0, numStates / 2);\n            this.backwardLayer.stateSpec = stateSpecs.slice(numStates / 2);\n            additionalSpecs.push(...stateSpecs);\n        }\n        if (constants != null) {\n            throw new NotImplementedError('Support for constants in Bidirectional layers is not ' +\n                'implemented yet.');\n        }\n        const isSymbolicTensor = additionalInputs[0] instanceof SymbolicTensor;\n        for (const tensor of additionalInputs) {\n            if (tensor instanceof SymbolicTensor !== isSymbolicTensor) {\n                throw new ValueError('The initial state of a Bidirectional layer cannot be ' +\n                    'specified as a mix of symbolic and non-symbolic tensors');\n            }\n        }\n        if (isSymbolicTensor) {\n            // Compute the full input and specs, including the states.\n            const fullInput = [inputs].concat(additionalInputs);\n            const fullInputSpec = this.inputSpec.concat(additionalSpecs);\n            // Perform the call temporarily and replace inputSpec.\n            // Note: with initial states symbolic calls and non-symbolic calls to\n            // this method differ in how the initial states are passed. For\n            // symbolic calls, the initial states are passed in the first arg, as\n            // an Array of SymbolicTensors; for non-symbolic calls, they are\n            // passed in the second arg as a part of the kwargs. Hence the need to\n            // temporarily modify inputSpec here.\n            // TODO(cais): Make refactoring so that this hacky code below is no\n            // longer needed.\n            const originalInputSpec = this.inputSpec;\n            this.inputSpec = fullInputSpec;\n            const output = super.apply(fullInput, kwargs);\n            this.inputSpec = originalInputSpec;\n            return output;\n        }\n        else {\n            return super.apply(inputs, kwargs);\n        }\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            const initialState = kwargs['initialState'];\n            let y;\n            let yRev;\n            if (initialState == null) {\n                y = this.forwardLayer.call(inputs, kwargs);\n                yRev = this.backwardLayer.call(inputs, kwargs);\n            }\n            else {\n                const forwardState = initialState.slice(0, initialState.length / 2);\n                const backwardState = initialState.slice(initialState.length / 2);\n                y = this.forwardLayer.call(inputs, Object.assign(kwargs, { initialState: forwardState }));\n                yRev = this.backwardLayer.call(inputs, Object.assign(kwargs, { initialState: backwardState }));\n            }\n            let states;\n            if (this.returnState) {\n                if (Array.isArray(y)) {\n                    states = y.slice(1).concat(yRev.slice(1));\n                }\n                else {\n                }\n                y = y[0];\n                yRev = yRev[0];\n            }\n            if (this.returnSequences) {\n                yRev = tfc.reverse(yRev, 1);\n            }\n            let output;\n            if (this.mergeMode === 'concat') {\n                output = K.concatenate([y, yRev]);\n            }\n            else if (this.mergeMode === 'sum') {\n                output = tfc.add(y, yRev);\n            }\n            else if (this.mergeMode === 'ave') {\n                output = tfc.mul(.5, tfc.add(y, yRev));\n            }\n            else if (this.mergeMode === 'mul') {\n                output = tfc.mul(y, yRev);\n            }\n            else if (this.mergeMode == null) {\n                output = [y, yRev];\n            }\n            // TODO(cais): Properly set learning phase.\n            if (this.returnState) {\n                if (this.mergeMode == null) {\n                    return output.concat(states);\n                }\n                return [output].concat(states);\n            }\n            return output;\n        });\n    }\n    resetStates(states) {\n        this.forwardLayer.resetStates();\n        this.backwardLayer.resetStates();\n    }\n    build(inputShape) {\n        nameScope(this.forwardLayer.name, () => {\n            this.forwardLayer.build(inputShape);\n        });\n        nameScope(this.backwardLayer.name, () => {\n            this.backwardLayer.build(inputShape);\n        });\n        this.built = true;\n    }\n    computeMask(inputs, mask) {\n        if (Array.isArray(mask)) {\n            mask = mask[0];\n        }\n        let outputMask;\n        if (this.returnSequences) {\n            if (this.mergeMode == null) {\n                outputMask = [mask, mask];\n            }\n            else {\n                outputMask = mask;\n            }\n        }\n        else {\n            if (this.mergeMode == null) {\n                outputMask = [null, null];\n            }\n            else {\n                outputMask = null;\n            }\n        }\n        if (this.returnState) {\n            const states = this.forwardLayer.states;\n            const stateMask = states.map(state => null);\n            if (Array.isArray(outputMask)) {\n                return outputMask.concat(stateMask).concat(stateMask);\n            }\n            else {\n                return [outputMask].concat(stateMask).concat(stateMask);\n            }\n        }\n        else {\n            return outputMask;\n        }\n    }\n    get trainableWeights() {\n        return this.forwardLayer.trainableWeights.concat(this.backwardLayer.trainableWeights);\n    }\n    get nonTrainableWeights() {\n        return this.forwardLayer.nonTrainableWeights.concat(this.backwardLayer.nonTrainableWeights);\n    }\n    // TODO(cais): Implement constraints().\n    setFastWeightInitDuringBuild(value) {\n        super.setFastWeightInitDuringBuild(value);\n        if (this.forwardLayer != null) {\n            this.forwardLayer.setFastWeightInitDuringBuild(value);\n        }\n        if (this.backwardLayer != null) {\n            this.backwardLayer.setFastWeightInitDuringBuild(value);\n        }\n    }\n    getConfig() {\n        const config = {\n            'mergeMode': this.mergeMode,\n        };\n        // TODO(cais): Add logic for `numConstants` once the property is added.\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n    /** @nocollapse */\n    static fromConfig(cls, config) {\n        const rnnLayer = deserialize(config['layer']);\n        delete config['layer'];\n        // TODO(cais): Add logic for `numConstants` once the property is added.\n        if (config['numConstants'] != null) {\n            throw new NotImplementedError(`Deserialization of a Bidirectional layer with numConstants ` +\n                `present is not supported yet.`);\n        }\n        // tslint:disable-next-line:no-any\n        const newConfig = config;\n        newConfig['layer'] = rnnLayer;\n        return new cls(newConfig);\n    }\n}\n/** @nocollapse */\nBidirectional.className = 'Bidirectional';\nserialization.registerClass(Bidirectional);\n//# sourceMappingURL=wrappers.js.map"]},"metadata":{},"sourceType":"module"}