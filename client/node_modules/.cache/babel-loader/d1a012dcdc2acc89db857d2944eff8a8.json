{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Interfaces and methods for training models using TensorFlow.js datasets.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { scalar } from '@tensorflow/tfjs-core';\nimport { configureCallbacks, standardizeCallbacks } from '../base_callbacks';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { disposeTensorsInLogs } from '../logs';\nimport { singletonOrArray, toList } from '../utils/generic_utils';\nimport { standardizeClassWeights, standardizeWeights } from './training_utils'; // Default batch size used during tensor-based validation.\n\nconst DEFAULT_VALIDATION_BATCH_SIZE = 32;\n/**\n * Standardize the output of a dataset iterator for use by\n * LayersModel.fitDataset().\n *\n * @param model: A `tf.LayersModel` object.\n * @param iteratorOut The output of a dataset iterator. It is required to be\n *   an object of the form `{xs: TensorOrArrayOrMap, ys:\n * TensorOrArrayOrMap}`, where `TensorOrArrayOrMap` is a single `tf.Tensor`,\n * a `tf.Tensor[]`, or a flat map from string names to `tf.Tensor`s.\n * @returns A flat array of `tf.Tensor` objects: the input `tf.Tensor`s\n *   followed by the target `tf.Tensor`s.  When `tf.Tensor`s are provided\n *   as a map, the order in the resulting array is taken from the `inputNames`\n *   and `outputNames` of the model.\n */\n\nfunction standardizeDataIteratorOutput( // Type `model` as `any` here to avoid circular dependency w/\n// training.ts.\n// tslint:disable-next-line:no-any\nmodel, iteratorOut) {\n  let xs;\n  let ys;\n  const iteratorOutObj = iteratorOut;\n  xs = iteratorOutObj['xs'];\n  ys = iteratorOutObj['ys'];\n  tfc.util.assert(xs != null && ys != null, () => 'A Dataset iterator for fitDataset() is expected to generate ' + 'objects of the form `{xs: xVal, ys: yVal}`, where the two ' + 'values may be `tf.Tensor`, an array of Tensors, or a map of ' + 'string to Tensor.  The provided Dataset instead generates ' + `${iteratorOut}`);\n  const flattenedXs = flattenTensorOrArrayOrMap('input', model.inputNames, xs);\n  const flattenedYs = flattenTensorOrArrayOrMap('output', model.outputNames, ys);\n  const batchSize = flattenedXs[0].shape[0];\n  tfc.util.assert(flattenedXs.length === model.inputs.length, () => `LayersModel has ${model.inputs.length} inputs, but the dataset ` + `provides ${flattenedXs.length} inputs.  (Expected input keys: ` + `${JSON.stringify(model.inputNames)})`);\n  tfc.util.assert(flattenedYs.length === model.outputs.length, () => `LayersModel has ${model.outputs.length} outputs, but the dataset ` + `provides ${flattenedYs.length} outputs.  (Expected output keys: ` + `${JSON.stringify(model.outputNames)})`);\n\n  for (let xIndex = 0; xIndex < flattenedXs.length; xIndex++) {\n    tfc.util.assert(flattenedXs[xIndex].shape[0] === batchSize, () => `Batch size mismatch: input ` + `${model.inputNames[xIndex]} has ${flattenedXs[xIndex].shape[0]}; ` + `expected  ${batchSize} based on input ${model.inputNames[0]}.`);\n  }\n\n  for (let yIndex = 0; yIndex < flattenedYs.length; yIndex++) {\n    tfc.util.assert(flattenedYs[yIndex].shape[0] === batchSize, () => `Batch size mismatch: output ` + `${model.outputNames[yIndex]} has ${flattenedYs[yIndex].shape[0]}; ` + `expected  ${batchSize} based on input ${model.inputNames[0]}.`);\n  }\n\n  return {\n    xs: flattenedXs,\n    ys: flattenedYs\n  };\n}\n\nfunction flattenTensorOrArrayOrMap(inputOrOutput, names, values) {\n  if (values instanceof tfc.Tensor) {\n    return [values];\n  } else if (Array.isArray(values)) {\n    tfc.util.assert(values.length === names.length, () => `Received an array of ${values.length} Tensors, but expected ${names.length} to match the ${inputOrOutput} keys ${names}.`);\n    return values;\n  } else {\n    const result = []; // Check that all the required keys are available.\n\n    for (const name of names) {\n      if (values[name] == null) {\n        throw new ValueError(`The feature data generated by the dataset lacks the required ` + `${inputOrOutput} key '${name}'.`);\n      }\n\n      result.push(values[name]);\n    }\n\n    return result;\n  }\n}\n\nfunction standardizeTensorValidationData(data) {\n  if (data.length === 3) {\n    throw new NotImplementedError('Validation with sample weights is not implemented yet.');\n  }\n\n  return {\n    xs: data[0],\n    ys: data[1]\n  };\n}\n\nexport async function fitDataset( // Type `model` as `any` here to avoid circular dependency w/\n// training.ts.\n// tslint:disable-next-line:no-any\nmodel, dataset, args) {\n  const hasBatchesPerEpoch = args.batchesPerEpoch != null;\n  tfc.util.assert(model.optimizer != null, () => 'You must compile a model before training/testing. Use ' + 'LayersModel.compile(modelCompileConfig).');\n  tfc.util.assert(args != null, () => `For fitDataset(), the 2nd argument (config) is required, ` + `but it is not provided in this call.`);\n  tfc.util.assert(args.epochs != null && args.epochs > 0 && Number.isInteger(args.epochs), () => `For fitDataset(), config.epochs is expected to be a positive ` + `integer, but got ${args.epochs}`);\n  tfc.util.assert(!hasBatchesPerEpoch || args.batchesPerEpoch > 0 && Number.isInteger(args.batchesPerEpoch), () => `For fitDataset(), config.batchesPerEpoch is expected to be a ` + `positive integer if specified, but got ${args.batchesPerEpoch}`);\n  tfc.util.assert( // tslint:disable-next-line:no-any\n  args['validationSplit'] == null, () => '`validationSplit` is not supported by `fitDataset()`. ' + 'Use validationData instead.');\n\n  if (model.isTraining) {\n    throw new Error('Cannot start training because another fit() call is ongoing.');\n  }\n\n  model.isTraining = true;\n\n  try {\n    const doValidation = args.validationData != null;\n    let valXs;\n    let valYs;\n\n    if (doValidation) {\n      if (isDatasetObject(args.validationData)) {\n        tfc.util.assert(args.validationBatches == null || args.validationBatches > 0 && Number.isInteger(args.validationBatches), () => `For fitDataset() with dataset-based validation, ` + `config.validationBatches is expected not to be provided, ` + `or to be a positive integer, ` + `but got ${args.validationBatches}`);\n      } else {\n        const validationData = standardizeTensorValidationData(args.validationData);\n        valXs = validationData.xs;\n        valYs = validationData.ys;\n      }\n    }\n\n    const trainFunction = model.makeTrainFunction();\n    const outLabels = model.getDedupedMetricsNames();\n    let callbackMetrics;\n\n    if (doValidation) {\n      callbackMetrics = outLabels.slice().concat(outLabels.map(n => 'val_' + n));\n    } else {\n      callbackMetrics = outLabels.slice();\n    }\n\n    const callbacks = standardizeCallbacks(args.callbacks, args.yieldEvery);\n    const verbose = args.verbose == null ? 1 : args.verbose;\n    const {\n      callbackList,\n      history\n    } = configureCallbacks(callbacks, verbose, args.epochs, null, null, getStepsPerEpoch(dataset, args), null, // Batch size determined by the dataset itself.\n    doValidation, callbackMetrics);\n    callbackList.setModel(model);\n    model.history = history;\n    await callbackList.onTrainBegin();\n    model.stopTraining_ = false;\n    let epoch = args.initialEpoch == null ? 0 : args.initialEpoch;\n    let dataIterator = await dataset.iterator();\n\n    while (epoch < args.epochs) {\n      const epochLogs = {};\n      await callbackList.onEpochBegin(epoch);\n      let stepsDone = 0;\n      let batchIndex = 0;\n\n      if (!hasBatchesPerEpoch) {\n        dataIterator = await dataset.iterator();\n      }\n\n      while (hasBatchesPerEpoch ? stepsDone < args.batchesPerEpoch : true) {\n        const iteratorOut = await dataIterator.next(); // If `batchesPerEpoch` is specified, the dataset should not be\n        // exhausted until all epoches are done.\n\n        if (hasBatchesPerEpoch && iteratorOut.done) {\n          console.warn('You provided `batchesPerEpoch` as ' + `${args.batchesPerEpoch}, ` + 'but your dataset iterator ran out of data after ' + `${stepsDone} batches; ` + 'interrupting training. Make sure that your ' + 'dataset can generate at least `batchesPerEpoch * epochs` ' + 'batches (in this case, ' + `${args.batchesPerEpoch * args.epochs} batches). ` + 'You may need to use the repeat() function when building ' + 'your dataset.');\n          break;\n        }\n\n        if (iteratorOut.value != null) {\n          const {\n            xs,\n            ys\n          } = standardizeDataIteratorOutput(model, iteratorOut.value);\n          const batchLogs = {};\n          batchLogs['batch'] = batchIndex;\n          batchLogs['size'] = xs[0].shape[0];\n          await callbackList.onBatchBegin(batchIndex, batchLogs);\n          const sampleWeights = [];\n\n          if (args.classWeight != null) {\n            const standardClassWeights = standardizeClassWeights(args.classWeight, model.outputNames);\n\n            for (let i = 0; i < standardClassWeights.length; ++i) {\n              sampleWeights.push(await standardizeWeights(ys[i], null, standardClassWeights[i]));\n            }\n          } // Train on batch.\n\n\n          const ins = xs.concat(ys).concat(sampleWeights);\n          const outs = trainFunction(ins);\n          tfc.dispose(ins);\n\n          for (let i = 0; i < outLabels.length; ++i) {\n            const label = outLabels[i];\n            const out = outs[i];\n            batchLogs[label] = out;\n            tfc.keep(out);\n          }\n\n          await callbackList.onBatchEnd(batchIndex, batchLogs);\n          disposeTensorsInLogs(batchLogs);\n          batchIndex++;\n          stepsDone++;\n        }\n\n        if (hasBatchesPerEpoch ? stepsDone >= args.batchesPerEpoch : iteratorOut.done) {\n          // Epoch finished. Perform validation.\n          if (doValidation) {\n            let valOuts;\n\n            if (isDatasetObject(args.validationData)) {\n              valOuts = toList(await model.evaluateDataset(args.validationData, {\n                batches: args.validationBatches\n              }));\n            } else {\n              valOuts = toList(model.evaluate(valXs, valYs, {\n                batchSize: args.validationBatchSize == null ? DEFAULT_VALIDATION_BATCH_SIZE : args.validationBatchSize,\n                verbose: 0\n              }));\n            }\n\n            for (let i = 0; i < model.metricsNames.length; ++i) {\n              epochLogs[`val_${model.metricsNames[i]}`] = valOuts[i];\n            }\n          } // Call `break` to exit one epoch lopp after validation is done. If\n          // config.batchesPerEpoch is specified, an epoch while loop will\n          // stop when `stepsDone >= config.batchesPerEpoch`. When\n          // config.batchesPerEpoch is not provided, the following `break` is\n          // required to exit the while lopp after dataset is exhausted.\n\n\n          break;\n        }\n\n        if (model.stopTraining_) {\n          break;\n        }\n      }\n\n      await callbackList.onEpochEnd(epoch, epochLogs);\n      epoch++;\n\n      if (model.stopTraining_) {\n        break;\n      }\n    }\n\n    await callbackList.onTrainEnd();\n    await model.history.syncData();\n    return model.history;\n  } finally {\n    model.isTraining = false;\n  }\n}\n/** Helper function that determines number of steps (batches) per epoch. */\n\nfunction getStepsPerEpoch(dataset, args) {\n  // Attempt to determine # of batches in an epoch.\n  let stepsPerEpoch = null;\n\n  if (args.batchesPerEpoch != null) {\n    stepsPerEpoch = args.batchesPerEpoch;\n  } else if (Number.isFinite(dataset.size)) {\n    stepsPerEpoch = dataset.size;\n  }\n\n  return stepsPerEpoch;\n} // Check if provided object is a Dataset object by checking its .iterator\n// element.\n\n\nfunction isDatasetObject(dataset) {\n  return typeof dataset.iterator === 'function';\n} // Check if provided object is a LazyIterator object by checking it's .next\n// element.\n\n\nfunction isLazyIteratorObject(iterator) {\n  return typeof iterator.next === 'function';\n}\n\nexport async function evaluateDataset( // Type `model` as `any` here to avoid circular dependency w/\n// training.ts.\n// tslint:disable-next-line:no-any\nmodel, dataset, args) {\n  args = args || {};\n  const hasBatches = args.batches != null;\n  const f = model.testFunction;\n  let outs = [];\n\n  if (args.verbose > 0) {\n    throw new NotImplementedError('Verbose mode is not implemented yet.');\n  }\n\n  tfc.util.assert(!hasBatches || args.batches > 0 && Number.isInteger(args.batches), () => 'Test loop expects `batches` to be a positive integer, but ' + `received ${JSON.stringify(args.batches)}`);\n  const dataIterator = isLazyIteratorObject(dataset) ? dataset : await dataset.iterator(); // Keeps track of number of examples used in this evaluation.\n\n  let numExamples = 0;\n  let batch = 0;\n\n  while (hasBatches ? batch < args.batches : true) {\n    const iteratorOut = await dataIterator.next();\n    outs = tfc.tidy(() => {\n      if (iteratorOut.value) {\n        // TODO(cais): Once real dataset is available, use\n        //   `map(x => standardizeDataIteratorOutput(model, x).map(f)`.\n        const {\n          xs,\n          ys\n        } = standardizeDataIteratorOutput(model, iteratorOut.value);\n        const xsAndYs = xs.concat(ys);\n        const batchOuts = tfc.tidy(() => f(xsAndYs));\n        tfc.dispose(xsAndYs);\n\n        if (batch === 0) {\n          for (let i = 0; i < batchOuts.length; ++i) {\n            outs.push(scalar(0));\n          }\n        }\n\n        const batchSize = xsAndYs[0].shape[0];\n\n        for (let i = 0; i < batchOuts.length; ++i) {\n          const batchOut = batchOuts[i];\n          const oldScalar = outs[i];\n          outs[i] = tfc.tidy(() => tfc.add(outs[i], tfc.mul(batchSize, batchOut)));\n\n          if (batch > 0) {\n            tfc.dispose(oldScalar);\n          }\n        }\n\n        tfc.dispose(batchOuts);\n        numExamples += batchSize;\n        ++batch;\n      }\n\n      return outs;\n    });\n\n    if (iteratorOut.done) {\n      if (hasBatches) {\n        console.warn('Your dataset iterator ran out of data during evaluateDataset(). ' + 'Interrupting evalution. Make sure that your ' + 'dataset can generate at least `batches` ' + `batches (in this case, ${args.batches} batches). ` + 'You may need to use the repeat() function when building ' + 'your dataset.');\n      }\n\n      break;\n    }\n  }\n\n  for (let i = 0; i < outs.length; ++i) {\n    const oldScalar = outs[i];\n    outs[i] = tfc.div(outs[i], numExamples);\n    tfc.dispose(oldScalar);\n  }\n\n  return singletonOrArray(outs);\n}","map":{"version":3,"sources":["../../src/engine/training_dataset.ts"],"names":[],"mappings":"AAAA;;;;;;;;AAQG;;AAEH;;AAEG;AAEH,OAAO,KAAK,GAAZ,MAAqB,uBAArB;AACA,SAAQ,MAAR,QAAqB,uBAArB;AACA,SAAsB,kBAAtB,EAA8F,oBAA9F,QAA4I,mBAA5I;AACA,SAAQ,mBAAR,EAA6B,UAA7B,QAA8C,WAA9C;AACA,SAAQ,oBAAR,QAAmD,SAAnD;AAEA,SAAQ,gBAAR,EAA0B,MAA1B,QAAuC,wBAAvC;AAGA,SAAqC,uBAArC,EAA8D,kBAA9D,QAAuF,kBAAvF,C,CAiKA;;AACA,MAAM,6BAA6B,GAAG,EAAtC;AAEA;;;;;;;;;;;;;AAaG;;AACH,SAAS,6BAAT,EACI;AACA;AACA;AACA,KAJJ,EAIgB,WAJhB,EAI+B;AAC7B,MAAI,EAAJ;AACA,MAAI,EAAJ;AAEA,QAAM,cAAc,GAAG,WAAvB;AACA,EAAA,EAAE,GAAG,cAAc,CAAC,IAAD,CAAnB;AACA,EAAA,EAAE,GAAG,cAAc,CAAC,IAAD,CAAnB;AACA,EAAA,GAAG,CAAC,IAAJ,CAAS,MAAT,CACI,EAAE,IAAI,IAAN,IAAc,EAAE,IAAI,IADxB,EAEI,MAAM,iEACF,4DADE,GAEF,8DAFE,GAGF,4DAHE,GAIF,GAAG,WAAW,EANtB;AAQA,QAAM,WAAW,GACb,yBAAyB,CAAC,OAAD,EAAU,KAAK,CAAC,UAAhB,EAA4B,EAA5B,CAD7B;AAEA,QAAM,WAAW,GACb,yBAAyB,CAAC,QAAD,EAAW,KAAK,CAAC,WAAjB,EAA8B,EAA9B,CAD7B;AAGA,QAAM,SAAS,GAAW,WAAW,CAAC,CAAD,CAAX,CAAe,KAAf,CAAqB,CAArB,CAA1B;AAEA,EAAA,GAAG,CAAC,IAAJ,CAAS,MAAT,CACI,WAAW,CAAC,MAAZ,KAAuB,KAAK,CAAC,MAAN,CAAa,MADxC,EAEI,MAAM,mBAAmB,KAAK,CAAC,MAAN,CAAa,MAAM,2BAAtC,GACF,YAAY,WAAW,CAAC,MAAM,kCAD5B,GAEF,GAAG,IAAI,CAAC,SAAL,CAAe,KAAK,CAAC,UAArB,CAAgC,GAJ3C;AAMA,EAAA,GAAG,CAAC,IAAJ,CAAS,MAAT,CACI,WAAW,CAAC,MAAZ,KAAuB,KAAK,CAAC,OAAN,CAAc,MADzC,EAEI,MACI,mBAAmB,KAAK,CAAC,OAAN,CAAc,MAAM,4BAAvC,GACA,YAAY,WAAW,CAAC,MAAM,oCAD9B,GAEA,GAAG,IAAI,CAAC,SAAL,CAAe,KAAK,CAAC,WAArB,CAAiC,GAL5C;;AAOA,OAAK,IAAI,MAAM,GAAG,CAAlB,EAAqB,MAAM,GAAG,WAAW,CAAC,MAA1C,EAAkD,MAAM,EAAxD,EAA4D;AAC1D,IAAA,GAAG,CAAC,IAAJ,CAAS,MAAT,CACI,WAAW,CAAC,MAAD,CAAX,CAAoB,KAApB,CAA0B,CAA1B,MAAiC,SADrC,EAEI,MAAM,6BAAA,GACF,GAAG,KAAK,CAAC,UAAN,CAAiB,MAAjB,CAAwB,QACrB,WAAW,CAAC,MAAD,CAAX,CAAoB,KAApB,CAA0B,CAA1B,CAA4B,IAFhC,GAGF,aAAa,SAAS,mBAAmB,KAAK,CAAC,UAAN,CAAiB,CAAjB,CAAmB,GALpE;AAMD;;AAED,OAAK,IAAI,MAAM,GAAG,CAAlB,EAAqB,MAAM,GAAG,WAAW,CAAC,MAA1C,EAAkD,MAAM,EAAxD,EAA4D;AAC1D,IAAA,GAAG,CAAC,IAAJ,CAAS,MAAT,CACI,WAAW,CAAC,MAAD,CAAX,CAAoB,KAApB,CAA0B,CAA1B,MAAiC,SADrC,EAEI,MAAM,8BAAA,GACF,GAAG,KAAK,CAAC,WAAN,CAAkB,MAAlB,CAAyB,QACtB,WAAW,CAAC,MAAD,CAAX,CAAoB,KAApB,CAA0B,CAA1B,CAA4B,IAFhC,GAGF,aAAa,SAAS,mBAAmB,KAAK,CAAC,UAAN,CAAiB,CAAjB,CAAmB,GALpE;AAMD;;AAED,SAAO;AAAC,IAAA,EAAE,EAAE,WAAL;AAAkB,IAAA,EAAE,EAAE;AAAtB,GAAP;AACD;;AAED,SAAS,yBAAT,CACI,aADJ,EAC2B,KAD3B,EAC4C,MAD5C,EACsE;AACpE,MAAI,MAAM,YAAY,GAAG,CAAC,MAA1B,EAAkC;AAChC,WAAO,CAAC,MAAD,CAAP;AACD,GAFD,MAEO,IAAI,KAAK,CAAC,OAAN,CAAc,MAAd,CAAJ,EAA2B;AAChC,IAAA,GAAG,CAAC,IAAJ,CAAS,MAAT,CACI,MAAM,CAAC,MAAP,KAAkB,KAAK,CAAC,MAD5B,EAEI,MAAM,wBAAwB,MAAM,CAAC,MAAM,0BACvC,KAAK,CAAC,MAAM,iBAAiB,aAAa,SAAS,KAAK,GAHhE;AAIA,WAAO,MAAP;AACD,GANM,MAMA;AACL,UAAM,MAAM,GAAiB,EAA7B,CADK,CAEL;;AACA,SAAK,MAAM,IAAX,IAAmB,KAAnB,EAA0B;AACxB,UAAI,MAAM,CAAC,IAAD,CAAN,IAAgB,IAApB,EAA0B;AACxB,cAAM,IAAI,UAAJ,CACF,+DAAA,GACA,GAAG,aAAa,SAAS,IAAI,IAF3B,CAAN;AAGD;;AACD,MAAA,MAAM,CAAC,IAAP,CAAY,MAAM,CAAC,IAAD,CAAlB;AACD;;AACD,WAAO,MAAP;AACD;AACF;;AAED,SAAS,+BAAT,CACI,IADJ,EAKqC;AAEnC,MAAI,IAAI,CAAC,MAAL,KAAgB,CAApB,EAAuB;AACrB,UAAM,IAAI,mBAAJ,CACF,wDADE,CAAN;AAED;;AACD,SAAO;AAAC,IAAA,EAAE,EAAE,IAAI,CAAC,CAAD,CAAT;AAAc,IAAA,EAAE,EAAE,IAAI,CAAC,CAAD;AAAtB,GAAP;AACD;;AAED,OAAO,eAAe,UAAf,EACH;AACA;AACA;AACA,KAJG,EAIS,OAJT,EAKH,IALG,EAKyB;AAC9B,QAAM,kBAAkB,GAAG,IAAI,CAAC,eAAL,IAAwB,IAAnD;AACA,EAAA,GAAG,CAAC,IAAJ,CAAS,MAAT,CACI,KAAK,CAAC,SAAN,IAAmB,IADvB,EAEI,MAAM,2DACF,0CAHR;AAKA,EAAA,GAAG,CAAC,IAAJ,CAAS,MAAT,CACI,IAAI,IAAI,IADZ,EAEI,MAAM,2DAAA,GACF,sCAHR;AAIA,EAAA,GAAG,CAAC,IAAJ,CAAS,MAAT,CACI,IAAI,CAAC,MAAL,IAAe,IAAf,IAAuB,IAAI,CAAC,MAAL,GAAc,CAArC,IAA0C,MAAM,CAAC,SAAP,CAAiB,IAAI,CAAC,MAAtB,CAD9C,EAEI,MAAM,+DAAA,GACF,oBAAoB,IAAI,CAAC,MAAM,EAHvC;AAIA,EAAA,GAAG,CAAC,IAAJ,CAAS,MAAT,CACI,CAAC,kBAAD,IACK,IAAI,CAAC,eAAL,GAAuB,CAAvB,IAA4B,MAAM,CAAC,SAAP,CAAiB,IAAI,CAAC,eAAtB,CAFrC,EAGI,MAAM,+DAAA,GACF,0CAA0C,IAAI,CAAC,eAAe,EAJtE;AAKA,EAAA,GAAG,CAAC,IAAJ,CAAS,MAAT,EACI;AACC,EAAA,IAAY,CAAC,iBAAD,CAAZ,IAAmC,IAFxC,EAGI,MAAM,2DACF,6BAJR;;AAMA,MAAI,KAAK,CAAC,UAAV,EAAsB;AACpB,UAAM,IAAI,KAAJ,CACF,8DADE,CAAN;AAED;;AACD,EAAA,KAAK,CAAC,UAAN,GAAmB,IAAnB;;AAEA,MAAI;AACF,UAAM,YAAY,GAAG,IAAI,CAAC,cAAL,IAAuB,IAA5C;AACA,QAAI,KAAJ;AACA,QAAI,KAAJ;;AACA,QAAI,YAAJ,EAAkB;AAChB,UAAI,eAAe,CAAC,IAAI,CAAC,cAAN,CAAnB,EAA0C;AACxC,QAAA,GAAG,CAAC,IAAJ,CAAS,MAAT,CACI,IAAI,CAAC,iBAAL,IAA0B,IAA1B,IACK,IAAI,CAAC,iBAAL,GAAyB,CAAzB,IACA,MAAM,CAAC,SAAP,CAAiB,IAAI,CAAC,iBAAtB,CAHT,EAII,MAAM,kDAAA,GACF,2DADE,GAEF,+BAFE,GAGF,WAAW,IAAI,CAAC,iBAAiB,EAPzC;AAQD,OATD,MASO;AACL,cAAM,cAAc,GAAG,+BAA+B,CAClD,IAAI,CAAC,cAD6C,CAAtD;AAOA,QAAA,KAAK,GAAG,cAAc,CAAC,EAAvB;AACA,QAAA,KAAK,GAAG,cAAc,CAAC,EAAvB;AACD;AACF;;AAED,UAAM,aAAa,GAAG,KAAK,CAAC,iBAAN,EAAtB;AACA,UAAM,SAAS,GAAG,KAAK,CAAC,sBAAN,EAAlB;AAEA,QAAI,eAAJ;;AACA,QAAI,YAAJ,EAAkB;AAChB,MAAA,eAAe,GACX,SAAS,CAAC,KAAV,GAAkB,MAAlB,CAAyB,SAAS,CAAC,GAAV,CAAc,CAAC,IAAI,SAAS,CAA5B,CAAzB,CADJ;AAED,KAHD,MAGO;AACL,MAAA,eAAe,GAAG,SAAS,CAAC,KAAV,EAAlB;AACD;;AAED,UAAM,SAAS,GAAG,oBAAoB,CAAC,IAAI,CAAC,SAAN,EAAiB,IAAI,CAAC,UAAtB,CAAtC;AACA,UAAM,OAAO,GAAG,IAAI,CAAC,OAAL,IAAgB,IAAhB,GAAuB,CAAvB,GAA2B,IAAI,CAAC,OAAhD;AACA,UAAM;AAAC,MAAA,YAAD;AAAe,MAAA;AAAf,QAA0B,kBAAkB,CAC9C,SAD8C,EACnC,OADmC,EAC1B,IAAI,CAAC,MADqB,EACb,IADa,EACP,IADO,EAE9C,gBAAgB,CAAC,OAAD,EAAU,IAAV,CAF8B,EAG9C,IAH8C,EAGvC;AACP,IAAA,YAJ8C,EAIhC,eAJgC,CAAlD;AAKA,IAAA,YAAY,CAAC,QAAb,CAAsB,KAAtB;AACA,IAAA,KAAK,CAAC,OAAN,GAAgB,OAAhB;AAEA,UAAM,YAAY,CAAC,YAAb,EAAN;AACA,IAAA,KAAK,CAAC,aAAN,GAAsB,KAAtB;AACA,QAAI,KAAK,GAAG,IAAI,CAAC,YAAL,IAAqB,IAArB,GAA4B,CAA5B,GAAgC,IAAI,CAAC,YAAjD;AAEA,QAAI,YAAY,GAAG,MAAM,OAAO,CAAC,QAAR,EAAzB;;AACA,WAAO,KAAK,GAAG,IAAI,CAAC,MAApB,EAA4B;AAC1B,YAAM,SAAS,GAAmB,EAAlC;AACA,YAAM,YAAY,CAAC,YAAb,CAA0B,KAA1B,CAAN;AACA,UAAI,SAAS,GAAG,CAAhB;AACA,UAAI,UAAU,GAAG,CAAjB;;AACA,UAAI,CAAC,kBAAL,EAAyB;AACvB,QAAA,YAAY,GAAG,MAAM,OAAO,CAAC,QAAR,EAArB;AACD;;AACD,aAAO,kBAAkB,GAAG,SAAS,GAAG,IAAI,CAAC,eAApB,GAAsC,IAA/D,EAAqE;AACnE,cAAM,WAAW,GAAG,MAAM,YAAY,CAAC,IAAb,EAA1B,CADmE,CAGnE;AACA;;AACA,YAAI,kBAAkB,IAAI,WAAW,CAAC,IAAtC,EAA4C;AAC1C,UAAA,OAAO,CAAC,IAAR,CACI,uCACA,GAAG,IAAI,CAAC,eAAe,IADvB,GAEA,kDAFA,GAGA,GAAG,SAAS,YAHZ,GAIA,6CAJA,GAKA,2DALA,GAMA,yBANA,GAOA,GAAG,IAAI,CAAC,eAAL,GAAuB,IAAI,CAAC,MAAM,aAPrC,GAQA,0DARA,GASA,eAVJ;AAWA;AACD;;AAED,YAAI,WAAW,CAAC,KAAZ,IAAqB,IAAzB,EAA+B;AAC7B,gBAAM;AAAC,YAAA,EAAD;AAAK,YAAA;AAAL,cACF,6BAA6B,CAAC,KAAD,EAAQ,WAAW,CAAC,KAApB,CADjC;AAEA,gBAAM,SAAS,GAAmB,EAAlC;AACA,UAAA,SAAS,CAAC,OAAD,CAAT,GAAqB,UAArB;AACA,UAAA,SAAS,CAAC,MAAD,CAAT,GAAoB,EAAE,CAAC,CAAD,CAAF,CAAM,KAAN,CAAY,CAAZ,CAApB;AAEA,gBAAM,YAAY,CAAC,YAAb,CAA0B,UAA1B,EAAsC,SAAtC,CAAN;AAEA,gBAAM,aAAa,GAAiB,EAApC;;AACA,cAAI,IAAI,CAAC,WAAL,IAAoB,IAAxB,EAA8B;AAC5B,kBAAM,oBAAoB,GACtB,uBAAuB,CAAC,IAAI,CAAC,WAAN,EAAmB,KAAK,CAAC,WAAzB,CAD3B;;AAEA,iBAAK,IAAI,CAAC,GAAG,CAAb,EAAgB,CAAC,GAAG,oBAAoB,CAAC,MAAzC,EAAiD,EAAE,CAAnD,EAAsD;AACpD,cAAA,aAAa,CAAC,IAAd,CAAmB,MAAM,kBAAkB,CACvC,EAAE,CAAC,CAAD,CADqC,EAChC,IADgC,EAC1B,oBAAoB,CAAC,CAAD,CADM,CAA3C;AAED;AACF,WAjB4B,CAmB7B;;;AACA,gBAAM,GAAG,GAAG,EAAE,CAAC,MAAH,CAAU,EAAV,EAAc,MAAd,CAAqB,aAArB,CAAZ;AACA,gBAAM,IAAI,GAAG,aAAa,CAAC,GAAD,CAA1B;AACA,UAAA,GAAG,CAAC,OAAJ,CAAY,GAAZ;;AACA,eAAK,IAAI,CAAC,GAAG,CAAb,EAAgB,CAAC,GAAG,SAAS,CAAC,MAA9B,EAAsC,EAAE,CAAxC,EAA2C;AACzC,kBAAM,KAAK,GAAG,SAAS,CAAC,CAAD,CAAvB;AACA,kBAAM,GAAG,GAAG,IAAI,CAAC,CAAD,CAAhB;AACA,YAAA,SAAS,CAAC,KAAD,CAAT,GAAmB,GAAnB;AACA,YAAA,GAAG,CAAC,IAAJ,CAAS,GAAT;AACD;;AAED,gBAAM,YAAY,CAAC,UAAb,CAAwB,UAAxB,EAAoC,SAApC,CAAN;AACA,UAAA,oBAAoB,CAAC,SAAD,CAApB;AAEA,UAAA,UAAU;AACV,UAAA,SAAS;AACV;;AAED,YAAI,kBAAkB,GAAG,SAAS,IAAI,IAAI,CAAC,eAArB,GACG,WAAW,CAAC,IADrC,EAC2C;AACzC;AACA,cAAI,YAAJ,EAAkB;AAChB,gBAAI,OAAJ;;AACA,gBAAI,eAAe,CAAC,IAAI,CAAC,cAAN,CAAnB,EAA0C;AACxC,cAAA,OAAO,GAAG,MAAM,CAAC,MAAM,KAAK,CAAC,eAAN,CACnB,IAAI,CAAC,cADc,EACE;AAAC,gBAAA,OAAO,EAAE,IAAI,CAAC;AAAf,eADF,CAAP,CAAhB;AAED,aAHD,MAGO;AACL,cAAA,OAAO,GAAG,MAAM,CAAC,KAAK,CAAC,QAAN,CAAe,KAAf,EAAsB,KAAtB,EAA6B;AAC5C,gBAAA,SAAS,EAAE,IAAI,CAAC,mBAAL,IAA4B,IAA5B,GACP,6BADO,GAEP,IAAI,CAAC,mBAHmC;AAI5C,gBAAA,OAAO,EAAE;AAJmC,eAA7B,CAAD,CAAhB;AAMD;;AACD,iBAAK,IAAI,CAAC,GAAG,CAAb,EAAgB,CAAC,GAAG,KAAK,CAAC,YAAN,CAAmB,MAAvC,EAA+C,EAAE,CAAjD,EAAoD;AAClD,cAAA,SAAS,CAAC,OAAO,KAAK,CAAC,YAAN,CAAmB,CAAnB,CAAqB,EAA7B,CAAT,GAA4C,OAAO,CAAC,CAAD,CAAnD;AACD;AACF,WAlBwC,CAmBzC;AACA;AACA;AACA;AACA;;;AACA;AACD;;AAED,YAAI,KAAK,CAAC,aAAV,EAAyB;AACvB;AACD;AACF;;AACD,YAAM,YAAY,CAAC,UAAb,CAAwB,KAAxB,EAA+B,SAA/B,CAAN;AACA,MAAA,KAAK;;AACL,UAAI,KAAK,CAAC,aAAV,EAAyB;AACvB;AACD;AACF;;AACD,UAAM,YAAY,CAAC,UAAb,EAAN;AACA,UAAM,KAAK,CAAC,OAAN,CAAc,QAAd,EAAN;AACA,WAAO,KAAK,CAAC,OAAb;AACD,GA/JD,SA+JU;AACR,IAAA,KAAK,CAAC,UAAN,GAAmB,KAAnB;AACD;AACF;AAED;;AACA,SAAS,gBAAT,CACI,OADJ,EACyB,IADzB,EACqD;AACnD;AACA,MAAI,aAAa,GAAW,IAA5B;;AACA,MAAI,IAAI,CAAC,eAAL,IAAwB,IAA5B,EAAkC;AAChC,IAAA,aAAa,GAAG,IAAI,CAAC,eAArB;AACD,GAFD,MAEO,IAAI,MAAM,CAAC,QAAP,CAAgB,OAAO,CAAC,IAAxB,CAAJ,EAAmC;AACxC,IAAA,aAAa,GAAG,OAAO,CAAC,IAAxB;AACD;;AACD,SAAO,aAAP;AACD,C,CAED;AACA;;;AACA,SAAS,eAAT,CACI,OADJ,EAKc;AACZ,SAAQ,OAAQ,OAAsB,CAAC,QAA/B,KAA4C,UAApD;AACD,C,CAED;AACA;;;AACA,SAAS,oBAAT,CAAiC,QAAjC,EACgD;AAC9C,SAAQ,OAAQ,QAA4B,CAAC,IAArC,KAA8C,UAAtD;AACD;;AAED,OAAO,eAAe,eAAf,EACH;AACA;AACA;AACA,KAJG,EAIS,OAJT,EAKH,IALG,EAK2B;AAChC,EAAA,IAAI,GAAG,IAAI,IAAI,EAAf;AACA,QAAM,UAAU,GAAG,IAAI,CAAC,OAAL,IAAgB,IAAnC;AACA,QAAM,CAAC,GAAG,KAAK,CAAC,YAAhB;AACA,MAAI,IAAI,GAAiB,EAAzB;;AACA,MAAI,IAAI,CAAC,OAAL,GAAe,CAAnB,EAAsB;AACpB,UAAM,IAAI,mBAAJ,CAAwB,sCAAxB,CAAN;AACD;;AAED,EAAA,GAAG,CAAC,IAAJ,CAAS,MAAT,CACI,CAAC,UAAD,IAAgB,IAAI,CAAC,OAAL,GAAe,CAAf,IAAoB,MAAM,CAAC,SAAP,CAAiB,IAAI,CAAC,OAAtB,CADxC,EAEI,MAAM,+DACF,YAAY,IAAI,CAAC,SAAL,CAAe,IAAI,CAAC,OAApB,CAA4B,EAHhD;AAIA,QAAM,YAAY,GAAG,oBAAoB,CAAC,OAAD,CAApB,GACjB,OADiB,GAEjB,MAAO,OAAsB,CAAC,QAAvB,EAFX,CAbgC,CAgBhC;;AACA,MAAI,WAAW,GAAG,CAAlB;AACA,MAAI,KAAK,GAAG,CAAZ;;AAEA,SAAO,UAAU,GAAG,KAAK,GAAG,IAAI,CAAC,OAAhB,GAA0B,IAA3C,EAAiD;AAC/C,UAAM,WAAW,GAAG,MAAM,YAAY,CAAC,IAAb,EAA1B;AACA,IAAA,IAAI,GAAG,GAAG,CAAC,IAAJ,CAAS,MAAK;AACnB,UAAI,WAAW,CAAC,KAAhB,EAAuB;AACrB;AACA;AACA,cAAM;AAAC,UAAA,EAAD;AAAK,UAAA;AAAL,YACF,6BAA6B,CAAC,KAAD,EAAQ,WAAW,CAAC,KAApB,CADjC;AAEA,cAAM,OAAO,GAAG,EAAE,CAAC,MAAH,CAAU,EAAV,CAAhB;AACA,cAAM,SAAS,GAAG,GAAG,CAAC,IAAJ,CAAS,MAAM,CAAC,CAAC,OAAD,CAAhB,CAAlB;AACA,QAAA,GAAG,CAAC,OAAJ,CAAY,OAAZ;;AAEA,YAAI,KAAK,KAAK,CAAd,EAAiB;AACf,eAAK,IAAI,CAAC,GAAG,CAAb,EAAgB,CAAC,GAAG,SAAS,CAAC,MAA9B,EAAsC,EAAE,CAAxC,EAA2C;AACzC,YAAA,IAAI,CAAC,IAAL,CAAU,MAAM,CAAC,CAAD,CAAhB;AACD;AACF;;AAED,cAAM,SAAS,GAAG,OAAO,CAAC,CAAD,CAAP,CAAW,KAAX,CAAiB,CAAjB,CAAlB;;AACA,aAAK,IAAI,CAAC,GAAG,CAAb,EAAgB,CAAC,GAAG,SAAS,CAAC,MAA9B,EAAsC,EAAE,CAAxC,EAA2C;AACzC,gBAAM,QAAQ,GAAG,SAAS,CAAC,CAAD,CAA1B;AACA,gBAAM,SAAS,GAAG,IAAI,CAAC,CAAD,CAAtB;AACA,UAAA,IAAI,CAAC,CAAD,CAAJ,GACI,GAAG,CAAC,IAAJ,CAAS,MAAM,GAAG,CAAC,GAAJ,CAAQ,IAAI,CAAC,CAAD,CAAZ,EAAiB,GAAG,CAAC,GAAJ,CAAQ,SAAR,EAAmB,QAAnB,CAAjB,CAAf,CADJ;;AAEA,cAAI,KAAK,GAAG,CAAZ,EAAe;AACb,YAAA,GAAG,CAAC,OAAJ,CAAY,SAAZ;AACD;AACF;;AACD,QAAA,GAAG,CAAC,OAAJ,CAAY,SAAZ;AACA,QAAA,WAAW,IAAI,SAAf;AAEA,UAAE,KAAF;AACD;;AACD,aAAO,IAAP;AACD,KAhCM,CAAP;;AAkCA,QAAI,WAAW,CAAC,IAAhB,EAAsB;AACpB,UAAI,UAAJ,EAAgB;AACd,QAAA,OAAO,CAAC,IAAR,CACI,qEACA,8CADA,GAEA,0CAFA,GAGA,0BAA0B,IAAI,CAAC,OAAO,aAHtC,GAIA,0DAJA,GAKA,eANJ;AAOD;;AACD;AACD;AACF;;AAED,OAAK,IAAI,CAAC,GAAG,CAAb,EAAgB,CAAC,GAAG,IAAI,CAAC,MAAzB,EAAiC,EAAE,CAAnC,EAAsC;AACpC,UAAM,SAAS,GAAG,IAAI,CAAC,CAAD,CAAtB;AACA,IAAA,IAAI,CAAC,CAAD,CAAJ,GAAU,GAAG,CAAC,GAAJ,CAAQ,IAAI,CAAC,CAAD,CAAZ,EAAiB,WAAjB,CAAV;AACA,IAAA,GAAG,CAAC,OAAJ,CAAY,SAAZ;AACD;;AAED,SAAO,gBAAgB,CAAC,IAAD,CAAvB;AACD","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * Interfaces and methods for training models using TensorFlow.js datasets.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { scalar } from '@tensorflow/tfjs-core';\nimport { configureCallbacks, standardizeCallbacks } from '../base_callbacks';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { disposeTensorsInLogs } from '../logs';\nimport { singletonOrArray, toList } from '../utils/generic_utils';\nimport { standardizeClassWeights, standardizeWeights } from './training_utils';\n// Default batch size used during tensor-based validation.\nconst DEFAULT_VALIDATION_BATCH_SIZE = 32;\n/**\n * Standardize the output of a dataset iterator for use by\n * LayersModel.fitDataset().\n *\n * @param model: A `tf.LayersModel` object.\n * @param iteratorOut The output of a dataset iterator. It is required to be\n *   an object of the form `{xs: TensorOrArrayOrMap, ys:\n * TensorOrArrayOrMap}`, where `TensorOrArrayOrMap` is a single `tf.Tensor`,\n * a `tf.Tensor[]`, or a flat map from string names to `tf.Tensor`s.\n * @returns A flat array of `tf.Tensor` objects: the input `tf.Tensor`s\n *   followed by the target `tf.Tensor`s.  When `tf.Tensor`s are provided\n *   as a map, the order in the resulting array is taken from the `inputNames`\n *   and `outputNames` of the model.\n */\nfunction standardizeDataIteratorOutput(\n// Type `model` as `any` here to avoid circular dependency w/\n// training.ts.\n// tslint:disable-next-line:no-any\nmodel, iteratorOut) {\n    let xs;\n    let ys;\n    const iteratorOutObj = iteratorOut;\n    xs = iteratorOutObj['xs'];\n    ys = iteratorOutObj['ys'];\n    tfc.util.assert(xs != null && ys != null, () => 'A Dataset iterator for fitDataset() is expected to generate ' +\n        'objects of the form `{xs: xVal, ys: yVal}`, where the two ' +\n        'values may be `tf.Tensor`, an array of Tensors, or a map of ' +\n        'string to Tensor.  The provided Dataset instead generates ' +\n        `${iteratorOut}`);\n    const flattenedXs = flattenTensorOrArrayOrMap('input', model.inputNames, xs);\n    const flattenedYs = flattenTensorOrArrayOrMap('output', model.outputNames, ys);\n    const batchSize = flattenedXs[0].shape[0];\n    tfc.util.assert(flattenedXs.length === model.inputs.length, () => `LayersModel has ${model.inputs.length} inputs, but the dataset ` +\n        `provides ${flattenedXs.length} inputs.  (Expected input keys: ` +\n        `${JSON.stringify(model.inputNames)})`);\n    tfc.util.assert(flattenedYs.length === model.outputs.length, () => `LayersModel has ${model.outputs.length} outputs, but the dataset ` +\n        `provides ${flattenedYs.length} outputs.  (Expected output keys: ` +\n        `${JSON.stringify(model.outputNames)})`);\n    for (let xIndex = 0; xIndex < flattenedXs.length; xIndex++) {\n        tfc.util.assert(flattenedXs[xIndex].shape[0] === batchSize, () => `Batch size mismatch: input ` +\n            `${model.inputNames[xIndex]} has ${flattenedXs[xIndex].shape[0]}; ` +\n            `expected  ${batchSize} based on input ${model.inputNames[0]}.`);\n    }\n    for (let yIndex = 0; yIndex < flattenedYs.length; yIndex++) {\n        tfc.util.assert(flattenedYs[yIndex].shape[0] === batchSize, () => `Batch size mismatch: output ` +\n            `${model.outputNames[yIndex]} has ${flattenedYs[yIndex].shape[0]}; ` +\n            `expected  ${batchSize} based on input ${model.inputNames[0]}.`);\n    }\n    return { xs: flattenedXs, ys: flattenedYs };\n}\nfunction flattenTensorOrArrayOrMap(inputOrOutput, names, values) {\n    if (values instanceof tfc.Tensor) {\n        return [values];\n    }\n    else if (Array.isArray(values)) {\n        tfc.util.assert(values.length === names.length, () => `Received an array of ${values.length} Tensors, but expected ${names.length} to match the ${inputOrOutput} keys ${names}.`);\n        return values;\n    }\n    else {\n        const result = [];\n        // Check that all the required keys are available.\n        for (const name of names) {\n            if (values[name] == null) {\n                throw new ValueError(`The feature data generated by the dataset lacks the required ` +\n                    `${inputOrOutput} key '${name}'.`);\n            }\n            result.push(values[name]);\n        }\n        return result;\n    }\n}\nfunction standardizeTensorValidationData(data) {\n    if (data.length === 3) {\n        throw new NotImplementedError('Validation with sample weights is not implemented yet.');\n    }\n    return { xs: data[0], ys: data[1] };\n}\nexport async function fitDataset(\n// Type `model` as `any` here to avoid circular dependency w/\n// training.ts.\n// tslint:disable-next-line:no-any\nmodel, dataset, args) {\n    const hasBatchesPerEpoch = args.batchesPerEpoch != null;\n    tfc.util.assert(model.optimizer != null, () => 'You must compile a model before training/testing. Use ' +\n        'LayersModel.compile(modelCompileConfig).');\n    tfc.util.assert(args != null, () => `For fitDataset(), the 2nd argument (config) is required, ` +\n        `but it is not provided in this call.`);\n    tfc.util.assert(args.epochs != null && args.epochs > 0 && Number.isInteger(args.epochs), () => `For fitDataset(), config.epochs is expected to be a positive ` +\n        `integer, but got ${args.epochs}`);\n    tfc.util.assert(!hasBatchesPerEpoch ||\n        (args.batchesPerEpoch > 0 && Number.isInteger(args.batchesPerEpoch)), () => `For fitDataset(), config.batchesPerEpoch is expected to be a ` +\n        `positive integer if specified, but got ${args.batchesPerEpoch}`);\n    tfc.util.assert(\n    // tslint:disable-next-line:no-any\n    args['validationSplit'] == null, () => '`validationSplit` is not supported by `fitDataset()`. ' +\n        'Use validationData instead.');\n    if (model.isTraining) {\n        throw new Error('Cannot start training because another fit() call is ongoing.');\n    }\n    model.isTraining = true;\n    try {\n        const doValidation = args.validationData != null;\n        let valXs;\n        let valYs;\n        if (doValidation) {\n            if (isDatasetObject(args.validationData)) {\n                tfc.util.assert(args.validationBatches == null ||\n                    (args.validationBatches > 0 &&\n                        Number.isInteger(args.validationBatches)), () => `For fitDataset() with dataset-based validation, ` +\n                    `config.validationBatches is expected not to be provided, ` +\n                    `or to be a positive integer, ` +\n                    `but got ${args.validationBatches}`);\n            }\n            else {\n                const validationData = standardizeTensorValidationData(args.validationData);\n                valXs = validationData.xs;\n                valYs = validationData.ys;\n            }\n        }\n        const trainFunction = model.makeTrainFunction();\n        const outLabels = model.getDedupedMetricsNames();\n        let callbackMetrics;\n        if (doValidation) {\n            callbackMetrics =\n                outLabels.slice().concat(outLabels.map(n => 'val_' + n));\n        }\n        else {\n            callbackMetrics = outLabels.slice();\n        }\n        const callbacks = standardizeCallbacks(args.callbacks, args.yieldEvery);\n        const verbose = args.verbose == null ? 1 : args.verbose;\n        const { callbackList, history } = configureCallbacks(callbacks, verbose, args.epochs, null, null, getStepsPerEpoch(dataset, args), null, // Batch size determined by the dataset itself.\n        doValidation, callbackMetrics);\n        callbackList.setModel(model);\n        model.history = history;\n        await callbackList.onTrainBegin();\n        model.stopTraining_ = false;\n        let epoch = args.initialEpoch == null ? 0 : args.initialEpoch;\n        let dataIterator = await dataset.iterator();\n        while (epoch < args.epochs) {\n            const epochLogs = {};\n            await callbackList.onEpochBegin(epoch);\n            let stepsDone = 0;\n            let batchIndex = 0;\n            if (!hasBatchesPerEpoch) {\n                dataIterator = await dataset.iterator();\n            }\n            while (hasBatchesPerEpoch ? stepsDone < args.batchesPerEpoch : true) {\n                const iteratorOut = await dataIterator.next();\n                // If `batchesPerEpoch` is specified, the dataset should not be\n                // exhausted until all epoches are done.\n                if (hasBatchesPerEpoch && iteratorOut.done) {\n                    console.warn('You provided `batchesPerEpoch` as ' +\n                        `${args.batchesPerEpoch}, ` +\n                        'but your dataset iterator ran out of data after ' +\n                        `${stepsDone} batches; ` +\n                        'interrupting training. Make sure that your ' +\n                        'dataset can generate at least `batchesPerEpoch * epochs` ' +\n                        'batches (in this case, ' +\n                        `${args.batchesPerEpoch * args.epochs} batches). ` +\n                        'You may need to use the repeat() function when building ' +\n                        'your dataset.');\n                    break;\n                }\n                if (iteratorOut.value != null) {\n                    const { xs, ys } = standardizeDataIteratorOutput(model, iteratorOut.value);\n                    const batchLogs = {};\n                    batchLogs['batch'] = batchIndex;\n                    batchLogs['size'] = xs[0].shape[0];\n                    await callbackList.onBatchBegin(batchIndex, batchLogs);\n                    const sampleWeights = [];\n                    if (args.classWeight != null) {\n                        const standardClassWeights = standardizeClassWeights(args.classWeight, model.outputNames);\n                        for (let i = 0; i < standardClassWeights.length; ++i) {\n                            sampleWeights.push(await standardizeWeights(ys[i], null, standardClassWeights[i]));\n                        }\n                    }\n                    // Train on batch.\n                    const ins = xs.concat(ys).concat(sampleWeights);\n                    const outs = trainFunction(ins);\n                    tfc.dispose(ins);\n                    for (let i = 0; i < outLabels.length; ++i) {\n                        const label = outLabels[i];\n                        const out = outs[i];\n                        batchLogs[label] = out;\n                        tfc.keep(out);\n                    }\n                    await callbackList.onBatchEnd(batchIndex, batchLogs);\n                    disposeTensorsInLogs(batchLogs);\n                    batchIndex++;\n                    stepsDone++;\n                }\n                if (hasBatchesPerEpoch ? stepsDone >= args.batchesPerEpoch :\n                    iteratorOut.done) {\n                    // Epoch finished. Perform validation.\n                    if (doValidation) {\n                        let valOuts;\n                        if (isDatasetObject(args.validationData)) {\n                            valOuts = toList(await model.evaluateDataset(args.validationData, { batches: args.validationBatches }));\n                        }\n                        else {\n                            valOuts = toList(model.evaluate(valXs, valYs, {\n                                batchSize: args.validationBatchSize == null ?\n                                    DEFAULT_VALIDATION_BATCH_SIZE :\n                                    args.validationBatchSize,\n                                verbose: 0\n                            }));\n                        }\n                        for (let i = 0; i < model.metricsNames.length; ++i) {\n                            epochLogs[`val_${model.metricsNames[i]}`] = valOuts[i];\n                        }\n                    }\n                    // Call `break` to exit one epoch lopp after validation is done. If\n                    // config.batchesPerEpoch is specified, an epoch while loop will\n                    // stop when `stepsDone >= config.batchesPerEpoch`. When\n                    // config.batchesPerEpoch is not provided, the following `break` is\n                    // required to exit the while lopp after dataset is exhausted.\n                    break;\n                }\n                if (model.stopTraining_) {\n                    break;\n                }\n            }\n            await callbackList.onEpochEnd(epoch, epochLogs);\n            epoch++;\n            if (model.stopTraining_) {\n                break;\n            }\n        }\n        await callbackList.onTrainEnd();\n        await model.history.syncData();\n        return model.history;\n    }\n    finally {\n        model.isTraining = false;\n    }\n}\n/** Helper function that determines number of steps (batches) per epoch. */\nfunction getStepsPerEpoch(dataset, args) {\n    // Attempt to determine # of batches in an epoch.\n    let stepsPerEpoch = null;\n    if (args.batchesPerEpoch != null) {\n        stepsPerEpoch = args.batchesPerEpoch;\n    }\n    else if (Number.isFinite(dataset.size)) {\n        stepsPerEpoch = dataset.size;\n    }\n    return stepsPerEpoch;\n}\n// Check if provided object is a Dataset object by checking its .iterator\n// element.\nfunction isDatasetObject(dataset) {\n    return (typeof dataset.iterator === 'function');\n}\n// Check if provided object is a LazyIterator object by checking it's .next\n// element.\nfunction isLazyIteratorObject(iterator) {\n    return (typeof iterator.next === 'function');\n}\nexport async function evaluateDataset(\n// Type `model` as `any` here to avoid circular dependency w/\n// training.ts.\n// tslint:disable-next-line:no-any\nmodel, dataset, args) {\n    args = args || {};\n    const hasBatches = args.batches != null;\n    const f = model.testFunction;\n    let outs = [];\n    if (args.verbose > 0) {\n        throw new NotImplementedError('Verbose mode is not implemented yet.');\n    }\n    tfc.util.assert(!hasBatches || (args.batches > 0 && Number.isInteger(args.batches)), () => 'Test loop expects `batches` to be a positive integer, but ' +\n        `received ${JSON.stringify(args.batches)}`);\n    const dataIterator = isLazyIteratorObject(dataset) ?\n        dataset :\n        await dataset.iterator();\n    // Keeps track of number of examples used in this evaluation.\n    let numExamples = 0;\n    let batch = 0;\n    while (hasBatches ? batch < args.batches : true) {\n        const iteratorOut = await dataIterator.next();\n        outs = tfc.tidy(() => {\n            if (iteratorOut.value) {\n                // TODO(cais): Once real dataset is available, use\n                //   `map(x => standardizeDataIteratorOutput(model, x).map(f)`.\n                const { xs, ys } = standardizeDataIteratorOutput(model, iteratorOut.value);\n                const xsAndYs = xs.concat(ys);\n                const batchOuts = tfc.tidy(() => f(xsAndYs));\n                tfc.dispose(xsAndYs);\n                if (batch === 0) {\n                    for (let i = 0; i < batchOuts.length; ++i) {\n                        outs.push(scalar(0));\n                    }\n                }\n                const batchSize = xsAndYs[0].shape[0];\n                for (let i = 0; i < batchOuts.length; ++i) {\n                    const batchOut = batchOuts[i];\n                    const oldScalar = outs[i];\n                    outs[i] =\n                        tfc.tidy(() => tfc.add(outs[i], tfc.mul(batchSize, batchOut)));\n                    if (batch > 0) {\n                        tfc.dispose(oldScalar);\n                    }\n                }\n                tfc.dispose(batchOuts);\n                numExamples += batchSize;\n                ++batch;\n            }\n            return outs;\n        });\n        if (iteratorOut.done) {\n            if (hasBatches) {\n                console.warn('Your dataset iterator ran out of data during evaluateDataset(). ' +\n                    'Interrupting evalution. Make sure that your ' +\n                    'dataset can generate at least `batches` ' +\n                    `batches (in this case, ${args.batches} batches). ` +\n                    'You may need to use the repeat() function when building ' +\n                    'your dataset.');\n            }\n            break;\n        }\n    }\n    for (let i = 0; i < outs.length; ++i) {\n        const oldScalar = outs[i];\n        outs[i] = tfc.div(outs[i], numExamples);\n        tfc.dispose(oldScalar);\n    }\n    return singletonOrArray(outs);\n}\n//# sourceMappingURL=training_dataset.js.map"]},"metadata":{},"sourceType":"module"}