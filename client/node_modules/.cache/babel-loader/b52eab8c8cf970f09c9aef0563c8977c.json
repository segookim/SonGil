{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Layers that augment the functionality of a base layer.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { serialization, tidy } from '@tensorflow/tfjs-core';\nimport * as K from '../backend/tfjs_backend';\nimport { nameScope } from '../common';\nimport { InputSpec, Layer, SymbolicTensor } from '../engine/topology';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { VALID_BIDIRECTIONAL_MERGE_MODES } from '../keras_format/common';\nimport * as generic_utils from '../utils/generic_utils';\nimport { getExactlyOneShape, getExactlyOneTensor } from '../utils/types_utils';\nimport { rnn, standardizeArgs } from './recurrent';\nimport { deserialize } from './serialization';\n/**\n * Abstract wrapper base class.\n *\n * Wrappers take another layer and augment it in various ways.\n * Do not use this class as a layer, it is only an abstract base class.\n * Two usable wrappers are the `TimeDistributed` and `Bidirectional` wrappers.\n */\n\nexport class Wrapper extends Layer {\n  constructor(args) {\n    // Porting Note: In PyKeras, `self.layer` is set prior to the calling\n    //   `super()`. But we can't do that here due to TypeScript's restriction.\n    //   See: https://github.com/Microsoft/TypeScript/issues/8277\n    //   As a result, we have to add checks in `get trainable()` and\n    //   `set trainable()` below in order to prevent using `this.layer` when\n    //   its value is `undefined`. The super constructor does use the getter\n    //   and the setter of `this.layer`.\n    super(args);\n    this.layer = args.layer;\n  }\n\n  build(inputShape) {\n    this.built = true;\n  } // TODO(cais): Implement activityRegularizer getter.\n\n\n  get trainable() {\n    // Porting Note: the check of `this.layer` here is necessary due to the\n    //   way the `constructor` of this class is written (see Porting Note\n    //   above).\n    if (this.layer != null) {\n      return this.layer.trainable;\n    } else {\n      return false;\n    }\n  }\n\n  set trainable(value) {\n    // Porting Note: the check of `this.layer` here is necessary due to the\n    //   way the `constructor` of this class is written (see Porting Note\n    //   above).\n    if (this.layer != null) {\n      this.layer.trainable = value;\n    }\n  }\n\n  get trainableWeights() {\n    return this.layer.trainableWeights;\n  } // TODO(cais): Implement setter for trainableWeights.\n\n\n  get nonTrainableWeights() {\n    return this.layer.nonTrainableWeights;\n  } // TODO(cais): Implement setter for nonTrainableWeights.\n\n\n  get updates() {\n    // tslint:disable-next-line:no-any\n    return this.layer._updates;\n  } // TODO(cais): Implement getUpdatesFor().\n\n\n  get losses() {\n    return this.layer.losses;\n  } // TODO(cais): Implement getLossesFor().\n\n\n  getWeights() {\n    return this.layer.getWeights();\n  }\n\n  setWeights(weights) {\n    this.layer.setWeights(weights);\n  }\n\n  getConfig() {\n    const config = {\n      'layer': {\n        'className': this.layer.getClassName(),\n        'config': this.layer.getConfig()\n      }\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n  setFastWeightInitDuringBuild(value) {\n    super.setFastWeightInitDuringBuild(value);\n\n    if (this.layer != null) {\n      this.layer.setFastWeightInitDuringBuild(value);\n    }\n  }\n  /** @nocollapse */\n\n\n  static fromConfig(cls, config, customObjects = {}) {\n    const layerConfig = config['layer'];\n    const layer = deserialize(layerConfig, customObjects);\n    delete config['layer'];\n    const newConfig = {\n      layer\n    };\n    Object.assign(newConfig, config);\n    return new cls(newConfig);\n  }\n\n}\nexport class TimeDistributed extends Wrapper {\n  constructor(args) {\n    super(args);\n    this.supportsMasking = true;\n  }\n\n  build(inputShape) {\n    inputShape = getExactlyOneShape(inputShape);\n\n    if (inputShape.length < 3) {\n      throw new ValueError(`TimeDistributed layer expects an input shape >= 3D, but received ` + `input shape ${JSON.stringify(inputShape)}`);\n    }\n\n    this.inputSpec = [{\n      shape: inputShape\n    }];\n    const childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n\n    if (!this.layer.built) {\n      this.layer.build(childInputShape);\n      this.layer.built = true;\n    }\n\n    super.build(inputShape);\n  }\n\n  computeOutputShape(inputShape) {\n    inputShape = getExactlyOneShape(inputShape);\n    const childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n    const childOutputShape = this.layer.computeOutputShape(childInputShape);\n    const timesteps = inputShape[1];\n    return [childOutputShape[0], timesteps].concat(childOutputShape.slice(1));\n  }\n\n  call(inputs, kwargs) {\n    return tidy(() => {\n      // TODO(cais): Add 'training' and 'useLearningPhase' to kwargs.\n      inputs = getExactlyOneTensor(inputs); // Porting Note: In tfjs-layers, `inputs` are always concrete tensor\n      // values. Hence the inputs can't have an undetermined first (batch)\n      // dimension, which is why we always use the K.rnn approach here.\n\n      const step = (inputs, states) => {\n        // TODO(cais): Add useLearningPhase.\n        // NOTE(cais): `layer.call` may return a length-1 array of Tensor in\n        //   some cases (e.g., `layer` is a `Sequential` instance), which is\n        //   why `getExactlyOneTensor` is used below.\n        const output = getExactlyOneTensor(this.layer.call(inputs, kwargs));\n        return [output, []];\n      };\n\n      const rnnOutputs = rnn(step, inputs, [], false\n      /* goBackwards */\n      , null\n      /* mask */\n      , null\n      /* constants */\n      , false\n      /* unroll */\n      , true\n      /* needPerStepOutputs */\n      );\n      const y = rnnOutputs[1]; // TODO(cais): Add activity regularization.\n      // TODO(cais): Add useLearningPhase.\n\n      return y;\n    });\n  }\n\n}\n/** @nocollapse */\n\nTimeDistributed.className = 'TimeDistributed';\nserialization.registerClass(TimeDistributed);\nexport function checkBidirectionalMergeMode(value) {\n  generic_utils.checkStringTypeUnionValue(VALID_BIDIRECTIONAL_MERGE_MODES, 'BidirectionalMergeMode', value);\n}\nconst DEFAULT_BIDIRECTIONAL_MERGE_MODE = 'concat';\nexport class Bidirectional extends Wrapper {\n  constructor(args) {\n    super(args); // Note: When creating `this.forwardLayer`, the original Layer object\n    //   (`config.layer`) ought to be cloned. This is why we call\n    //   `getConfig()` followed by `deserialize()`. Without this cloning,\n    //   the layer names saved during serialization will incorrectly contain\n    //   the 'forward_' prefix. In Python Keras, this is done using\n    //   `copy.copy` (shallow copy), which does not have a simple equivalent\n    //   in JavaScript. JavaScript's `Object.assign()` does not copy\n    //   methods.\n\n    const layerConfig = args.layer.getConfig();\n    const forwDict = {};\n    forwDict['className'] = args.layer.getClassName();\n    forwDict['config'] = layerConfig;\n    this.forwardLayer = deserialize(forwDict);\n    layerConfig['goBackwards'] = layerConfig['goBackwards'] === true ? false : true;\n    const backDict = {};\n    backDict['className'] = args.layer.getClassName();\n    backDict['config'] = layerConfig;\n    this.backwardLayer = deserialize(backDict);\n    this.forwardLayer.name = 'forward_' + this.forwardLayer.name;\n    this.backwardLayer.name = 'backward_' + this.backwardLayer.name;\n    this.mergeMode = args.mergeMode === undefined ? DEFAULT_BIDIRECTIONAL_MERGE_MODE : args.mergeMode;\n    checkBidirectionalMergeMode(this.mergeMode);\n\n    if (args.weights) {\n      throw new NotImplementedError('weights support is not implemented for Bidirectional layer yet.');\n    }\n\n    this._stateful = args.layer.stateful;\n    this.returnSequences = args.layer.returnSequences;\n    this.returnState = args.layer.returnState;\n    this.supportsMasking = true;\n    this._trainable = true;\n    this.inputSpec = args.layer.inputSpec;\n    this.numConstants = null;\n  }\n\n  get trainable() {\n    return this._trainable;\n  }\n\n  set trainable(value) {\n    // Porting Note: the check of `this.layer` here is necessary due to the\n    //   way the `constructor` of this class is written (see Porting Note\n    //   above).\n    this._trainable = value;\n\n    if (this.forwardLayer != null) {\n      this.forwardLayer.trainable = value;\n    }\n\n    if (this.backwardLayer != null) {\n      this.backwardLayer.trainable = value;\n    }\n  }\n\n  getWeights() {\n    return this.forwardLayer.getWeights().concat(this.backwardLayer.getWeights());\n  }\n\n  setWeights(weights) {\n    const numWeights = weights.length;\n    const numeightsOver2 = Math.floor(numWeights / 2);\n    this.forwardLayer.setWeights(weights.slice(0, numeightsOver2));\n    this.backwardLayer.setWeights(weights.slice(numeightsOver2));\n  }\n\n  computeOutputShape(inputShape) {\n    let layerShapes = this.forwardLayer.computeOutputShape(inputShape);\n\n    if (!(Array.isArray(layerShapes) && Array.isArray(layerShapes[0]))) {\n      layerShapes = [layerShapes];\n    }\n\n    layerShapes = layerShapes;\n    let outputShape;\n    let outputShapes;\n    let stateShape;\n\n    if (this.returnState) {\n      stateShape = layerShapes.slice(1);\n      outputShape = layerShapes[0];\n    } else {\n      outputShape = layerShapes[0];\n    }\n\n    outputShape = outputShape;\n\n    if (this.mergeMode === 'concat') {\n      outputShape[outputShape.length - 1] *= 2;\n      outputShapes = [outputShape];\n    } else if (this.mergeMode == null) {\n      outputShapes = [outputShape, outputShape.slice()];\n    } else {\n      outputShapes = [outputShape];\n    }\n\n    if (this.returnState) {\n      if (this.mergeMode == null) {\n        return outputShapes.concat(stateShape).concat(stateShape.slice());\n      }\n\n      return [outputShape].concat(stateShape).concat(stateShape.slice());\n    }\n\n    return generic_utils.singletonOrArray(outputShapes);\n  }\n\n  apply(inputs, kwargs) {\n    let initialState = kwargs == null ? null : kwargs['initialState'];\n    let constants = kwargs == null ? null : kwargs['constants'];\n\n    if (kwargs == null) {\n      kwargs = {};\n    }\n\n    const standardized = standardizeArgs(inputs, initialState, constants, this.numConstants);\n    inputs = standardized.inputs;\n    initialState = standardized.initialState;\n    constants = standardized.constants;\n\n    if (Array.isArray(inputs)) {\n      initialState = inputs.slice(1);\n      inputs = inputs[0];\n    }\n\n    if ((initialState == null || initialState.length === 0) && constants == null) {\n      return super.apply(inputs, kwargs);\n    }\n\n    const additionalInputs = [];\n    const additionalSpecs = [];\n\n    if (initialState != null) {\n      const numStates = initialState.length;\n\n      if (numStates % 2 > 0) {\n        throw new ValueError('When passing `initialState` to a Bidrectional RNN, ' + 'the state should be an Array containing the states of ' + 'the underlying RNNs.');\n      }\n\n      kwargs['initialState'] = initialState;\n      additionalInputs.push(...initialState);\n      const stateSpecs = initialState.map(state => new InputSpec({\n        shape: state.shape\n      }));\n      this.forwardLayer.stateSpec = stateSpecs.slice(0, numStates / 2);\n      this.backwardLayer.stateSpec = stateSpecs.slice(numStates / 2);\n      additionalSpecs.push(...stateSpecs);\n    }\n\n    if (constants != null) {\n      throw new NotImplementedError('Support for constants in Bidirectional layers is not ' + 'implemented yet.');\n    }\n\n    const isSymbolicTensor = additionalInputs[0] instanceof SymbolicTensor;\n\n    for (const tensor of additionalInputs) {\n      if (tensor instanceof SymbolicTensor !== isSymbolicTensor) {\n        throw new ValueError('The initial state of a Bidirectional layer cannot be ' + 'specified as a mix of symbolic and non-symbolic tensors');\n      }\n    }\n\n    if (isSymbolicTensor) {\n      // Compute the full input and specs, including the states.\n      const fullInput = [inputs].concat(additionalInputs);\n      const fullInputSpec = this.inputSpec.concat(additionalSpecs); // Perform the call temporarily and replace inputSpec.\n      // Note: with initial states symbolic calls and non-symbolic calls to\n      // this method differ in how the initial states are passed. For\n      // symbolic calls, the initial states are passed in the first arg, as\n      // an Array of SymbolicTensors; for non-symbolic calls, they are\n      // passed in the second arg as a part of the kwargs. Hence the need to\n      // temporarily modify inputSpec here.\n      // TODO(cais): Make refactoring so that this hacky code below is no\n      // longer needed.\n\n      const originalInputSpec = this.inputSpec;\n      this.inputSpec = fullInputSpec;\n      const output = super.apply(fullInput, kwargs);\n      this.inputSpec = originalInputSpec;\n      return output;\n    } else {\n      return super.apply(inputs, kwargs);\n    }\n  }\n\n  call(inputs, kwargs) {\n    return tidy(() => {\n      const initialState = kwargs['initialState'];\n      let y;\n      let yRev;\n\n      if (initialState == null) {\n        y = this.forwardLayer.call(inputs, kwargs);\n        yRev = this.backwardLayer.call(inputs, kwargs);\n      } else {\n        const forwardState = initialState.slice(0, initialState.length / 2);\n        const backwardState = initialState.slice(initialState.length / 2);\n        y = this.forwardLayer.call(inputs, Object.assign(kwargs, {\n          initialState: forwardState\n        }));\n        yRev = this.backwardLayer.call(inputs, Object.assign(kwargs, {\n          initialState: backwardState\n        }));\n      }\n\n      let states;\n\n      if (this.returnState) {\n        if (Array.isArray(y)) {\n          states = y.slice(1).concat(yRev.slice(1));\n        } else {}\n\n        y = y[0];\n        yRev = yRev[0];\n      }\n\n      if (this.returnSequences) {\n        yRev = tfc.reverse(yRev, 1);\n      }\n\n      let output;\n\n      if (this.mergeMode === 'concat') {\n        output = K.concatenate([y, yRev]);\n      } else if (this.mergeMode === 'sum') {\n        output = tfc.add(y, yRev);\n      } else if (this.mergeMode === 'ave') {\n        output = tfc.mul(.5, tfc.add(y, yRev));\n      } else if (this.mergeMode === 'mul') {\n        output = tfc.mul(y, yRev);\n      } else if (this.mergeMode == null) {\n        output = [y, yRev];\n      } // TODO(cais): Properly set learning phase.\n\n\n      if (this.returnState) {\n        if (this.mergeMode == null) {\n          return output.concat(states);\n        }\n\n        return [output].concat(states);\n      }\n\n      return output;\n    });\n  }\n\n  resetStates(states) {\n    this.forwardLayer.resetStates();\n    this.backwardLayer.resetStates();\n  }\n\n  build(inputShape) {\n    nameScope(this.forwardLayer.name, () => {\n      this.forwardLayer.build(inputShape);\n    });\n    nameScope(this.backwardLayer.name, () => {\n      this.backwardLayer.build(inputShape);\n    });\n    this.built = true;\n  }\n\n  computeMask(inputs, mask) {\n    if (Array.isArray(mask)) {\n      mask = mask[0];\n    }\n\n    let outputMask;\n\n    if (this.returnSequences) {\n      if (this.mergeMode == null) {\n        outputMask = [mask, mask];\n      } else {\n        outputMask = mask;\n      }\n    } else {\n      if (this.mergeMode == null) {\n        outputMask = [null, null];\n      } else {\n        outputMask = null;\n      }\n    }\n\n    if (this.returnState) {\n      const states = this.forwardLayer.states;\n      const stateMask = states.map(state => null);\n\n      if (Array.isArray(outputMask)) {\n        return outputMask.concat(stateMask).concat(stateMask);\n      } else {\n        return [outputMask].concat(stateMask).concat(stateMask);\n      }\n    } else {\n      return outputMask;\n    }\n  }\n\n  get trainableWeights() {\n    return this.forwardLayer.trainableWeights.concat(this.backwardLayer.trainableWeights);\n  }\n\n  get nonTrainableWeights() {\n    return this.forwardLayer.nonTrainableWeights.concat(this.backwardLayer.nonTrainableWeights);\n  } // TODO(cais): Implement constraints().\n\n\n  setFastWeightInitDuringBuild(value) {\n    super.setFastWeightInitDuringBuild(value);\n\n    if (this.forwardLayer != null) {\n      this.forwardLayer.setFastWeightInitDuringBuild(value);\n    }\n\n    if (this.backwardLayer != null) {\n      this.backwardLayer.setFastWeightInitDuringBuild(value);\n    }\n  }\n\n  getConfig() {\n    const config = {\n      'mergeMode': this.mergeMode\n    }; // TODO(cais): Add logic for `numConstants` once the property is added.\n\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n  /** @nocollapse */\n\n\n  static fromConfig(cls, config) {\n    const rnnLayer = deserialize(config['layer']);\n    delete config['layer']; // TODO(cais): Add logic for `numConstants` once the property is added.\n\n    if (config['numConstants'] != null) {\n      throw new NotImplementedError(`Deserialization of a Bidirectional layer with numConstants ` + `present is not supported yet.`);\n    } // tslint:disable-next-line:no-any\n\n\n    const newConfig = config;\n    newConfig['layer'] = rnnLayer;\n    return new cls(newConfig);\n  }\n\n}\n/** @nocollapse */\n\nBidirectional.className = 'Bidirectional';\nserialization.registerClass(Bidirectional);","map":{"version":3,"sources":["../../src/layers/wrappers.ts"],"names":[],"mappings":"AAAA;;;;;;;;AAQG;;AAEH;;AAEG;AAEH,OAAO,KAAK,GAAZ,MAAqB,uBAArB;AACA,SAAQ,aAAR,EAA+B,IAA/B,QAA0C,uBAA1C;AACA,OAAO,KAAK,CAAZ,MAAmB,yBAAnB;AACA,SAAQ,SAAR,QAAwB,WAAxB;AACA,SAAQ,SAAR,EAAmB,KAAnB,EAAqC,cAArC,QAA0D,oBAA1D;AACA,SAAQ,mBAAR,EAA6B,UAA7B,QAA8C,WAA9C;AACA,SAAuC,+BAAvC,QAA6E,wBAA7E;AAGA,OAAO,KAAK,aAAZ,MAA+B,wBAA/B;AACA,SAAQ,kBAAR,EAA4B,mBAA5B,QAAsD,sBAAtD;AAGA,SAAQ,GAAR,EAAkB,eAAlB,QAAwC,aAAxC;AACA,SAAQ,WAAR,QAA0B,iBAA1B;AASA;;;;;;AAMG;;AACH,OAAM,MAAgB,OAAhB,SAAgC,KAAhC,CAAqC;AAGzC,EAAA,WAAA,CAAY,IAAZ,EAAkC;AAChC;AACA;AACA;AACA;AACA;AACA;AACA;AACA,UAAM,IAAN;AACA,SAAK,KAAL,GAAa,IAAI,CAAC,KAAlB;AACD;;AAED,EAAA,KAAK,CAAC,UAAD,EAA0B;AAC7B,SAAK,KAAL,GAAa,IAAb;AACD,GAjBwC,CAmBzC;;;AAEa,MAAT,SAAS,GAAA;AACX;AACA;AACA;AACA,QAAI,KAAK,KAAL,IAAc,IAAlB,EAAwB;AACtB,aAAO,KAAK,KAAL,CAAW,SAAlB;AACD,KAFD,MAEO;AACL,aAAO,KAAP;AACD;AACF;;AAEY,MAAT,SAAS,CAAC,KAAD,EAAe;AAC1B;AACA;AACA;AACA,QAAI,KAAK,KAAL,IAAc,IAAlB,EAAwB;AACtB,WAAK,KAAL,CAAW,SAAX,GAAuB,KAAvB;AACD;AACF;;AAEmB,MAAhB,gBAAgB,GAAA;AAClB,WAAO,KAAK,KAAL,CAAW,gBAAlB;AACD,GA3CwC,CA4CzC;;;AAEuB,MAAnB,mBAAmB,GAAA;AACrB,WAAO,KAAK,KAAL,CAAW,mBAAlB;AACD,GAhDwC,CAiDzC;;;AAEW,MAAP,OAAO,GAAA;AACT;AACA,WAAQ,KAAK,KAAL,CAAmB,QAA3B;AACD,GAtDwC,CAwDzC;;;AAEU,MAAN,MAAM,GAAA;AACR,WAAO,KAAK,KAAL,CAAW,MAAlB;AACD,GA5DwC,CA8DzC;;;AAEA,EAAA,UAAU,GAAA;AACR,WAAO,KAAK,KAAL,CAAW,UAAX,EAAP;AACD;;AAED,EAAA,UAAU,CAAC,OAAD,EAAkB;AAC1B,SAAK,KAAL,CAAW,UAAX,CAAsB,OAAtB;AACD;;AAED,EAAA,SAAS,GAAA;AACP,UAAM,MAAM,GAA6B;AACvC,eAAS;AACP,qBAAa,KAAK,KAAL,CAAW,YAAX,EADN;AAEP,kBAAU,KAAK,KAAL,CAAW,SAAX;AAFH;AAD8B,KAAzC;AAMA,UAAM,UAAU,GAAG,MAAM,SAAN,EAAnB;AACA,IAAA,MAAM,CAAC,MAAP,CAAc,MAAd,EAAsB,UAAtB;AACA,WAAO,MAAP;AACD;;AAED,EAAA,4BAA4B,CAAC,KAAD,EAAe;AACzC,UAAM,4BAAN,CAAmC,KAAnC;;AACA,QAAI,KAAK,KAAL,IAAc,IAAlB,EAAwB;AACtB,WAAK,KAAL,CAAW,4BAAX,CAAwC,KAAxC;AACD;AACF;AAED;;;AACiB,SAAV,UAAU,CACb,GADa,EAEb,MAFa,EAGb,aAAA,GAAgB,EAHH,EAGiC;AAChD,UAAM,WAAW,GAAG,MAAM,CAAC,OAAD,CAA1B;AACA,UAAM,KAAK,GAAG,WAAW,CAAC,WAAD,EAAc,aAAd,CAAzB;AACA,WAAO,MAAM,CAAC,OAAD,CAAb;AACA,UAAM,SAAS,GAAG;AAAC,MAAA;AAAD,KAAlB;AACA,IAAA,MAAM,CAAC,MAAP,CAAc,SAAd,EAAyB,MAAzB;AACA,WAAO,IAAI,GAAJ,CAAQ,SAAR,CAAP;AACD;;AAtGwC;AAyG3C,OAAM,MAAO,eAAP,SAA+B,OAA/B,CAAsC;AAG1C,EAAA,WAAA,CAAY,IAAZ,EAAkC;AAChC,UAAM,IAAN;AACA,SAAK,eAAL,GAAuB,IAAvB;AACD;;AAED,EAAA,KAAK,CAAC,UAAD,EAA0B;AAC7B,IAAA,UAAU,GAAG,kBAAkB,CAAC,UAAD,CAA/B;;AACA,QAAI,UAAU,CAAC,MAAX,GAAoB,CAAxB,EAA2B;AACzB,YAAM,IAAI,UAAJ,CACF,mEAAA,GACA,eAAe,IAAI,CAAC,SAAL,CAAe,UAAf,CAA0B,EAFvC,CAAN;AAGD;;AACD,SAAK,SAAL,GAAiB,CAAC;AAAC,MAAA,KAAK,EAAE;AAAR,KAAD,CAAjB;AACA,UAAM,eAAe,GAAG,CAAC,UAAU,CAAC,CAAD,CAAX,EAAgB,MAAhB,CAAuB,UAAU,CAAC,KAAX,CAAiB,CAAjB,CAAvB,CAAxB;;AACA,QAAI,CAAC,KAAK,KAAL,CAAW,KAAhB,EAAuB;AACrB,WAAK,KAAL,CAAW,KAAX,CAAiB,eAAjB;AACA,WAAK,KAAL,CAAW,KAAX,GAAmB,IAAnB;AACD;;AACD,UAAM,KAAN,CAAY,UAAZ;AACD;;AAED,EAAA,kBAAkB,CAAC,UAAD,EAA0B;AAC1C,IAAA,UAAU,GAAG,kBAAkB,CAAC,UAAD,CAA/B;AACA,UAAM,eAAe,GAAG,CAAC,UAAU,CAAC,CAAD,CAAX,EAAgB,MAAhB,CAAuB,UAAU,CAAC,KAAX,CAAiB,CAAjB,CAAvB,CAAxB;AACA,UAAM,gBAAgB,GAClB,KAAK,KAAL,CAAW,kBAAX,CAA8B,eAA9B,CADJ;AAEA,UAAM,SAAS,GAAG,UAAU,CAAC,CAAD,CAA5B;AACA,WAAO,CAAC,gBAAgB,CAAC,CAAD,CAAjB,EAAsB,SAAtB,EAAiC,MAAjC,CAAwC,gBAAgB,CAAC,KAAjB,CAAuB,CAAvB,CAAxC,CAAP;AACD;;AAED,EAAA,IAAI,CAAC,MAAD,EAA0B,MAA1B,EAAwC;AAC1C,WAAO,IAAI,CAAC,MAAK;AACf;AACA,MAAA,MAAM,GAAG,mBAAmB,CAAC,MAAD,CAA5B,CAFe,CAGf;AACA;AACA;;AACA,YAAM,IAAI,GAAoB,CAAC,MAAD,EAAiB,MAAjB,KAAqC;AACjE;AACA;AACA;AACA;AACA,cAAM,MAAM,GAAG,mBAAmB,CAAC,KAAK,KAAL,CAAW,IAAX,CAAgB,MAAhB,EAAwB,MAAxB,CAAD,CAAlC;AACA,eAAO,CAAC,MAAD,EAAS,EAAT,CAAP;AACD,OAPD;;AAQA,YAAM,UAAU,GACZ,GAAG,CAAC,IAAD,EAAO,MAAP,EAAe,EAAf,EAAmB;AAAM;AAAzB,QAA4C;AAAK;AAAjD,QACC;AAAK;AADN,QACuB;AAAM;AAD7B,QAEC;AAAK;AAFN,OADP;AAIA,YAAM,CAAC,GAAG,UAAU,CAAC,CAAD,CAApB,CAlBe,CAmBf;AACA;;AACA,aAAO,CAAP;AACD,KAtBU,CAAX;AAuBD;;AAzDyC;AAC1C;;AACO,eAAA,CAAA,SAAA,GAAY,iBAAZ;AA2DT,aAAa,CAAC,aAAd,CAA4B,eAA5B;AAEA,OAAM,SAAU,2BAAV,CAAsC,KAAtC,EAAoD;AACxD,EAAA,aAAa,CAAC,yBAAd,CACI,+BADJ,EACqC,wBADrC,EAC+D,KAD/D;AAED;AAkBD,MAAM,gCAAgC,GAA2B,QAAjE;AAEA,OAAM,MAAO,aAAP,SAA6B,OAA7B,CAAoC;AAWxC,EAAA,WAAA,CAAY,IAAZ,EAAwC;AACtC,UAAM,IAAN,EADsC,CAGtC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA,UAAM,WAAW,GAAG,IAAI,CAAC,KAAL,CAAW,SAAX,EAApB;AACA,UAAM,QAAQ,GAA6B,EAA3C;AACA,IAAA,QAAQ,CAAC,WAAD,CAAR,GAAwB,IAAI,CAAC,KAAL,CAAW,YAAX,EAAxB;AACA,IAAA,QAAQ,CAAC,QAAD,CAAR,GAAqB,WAArB;AACA,SAAK,YAAL,GAAoB,WAAW,CAAC,QAAD,CAA/B;AACA,IAAA,WAAW,CAAC,aAAD,CAAX,GACI,WAAW,CAAC,aAAD,CAAX,KAA+B,IAA/B,GAAsC,KAAtC,GAA8C,IADlD;AAEA,UAAM,QAAQ,GAA6B,EAA3C;AACA,IAAA,QAAQ,CAAC,WAAD,CAAR,GAAwB,IAAI,CAAC,KAAL,CAAW,YAAX,EAAxB;AACA,IAAA,QAAQ,CAAC,QAAD,CAAR,GAAqB,WAArB;AACA,SAAK,aAAL,GAAqB,WAAW,CAAC,QAAD,CAAhC;AACA,SAAK,YAAL,CAAkB,IAAlB,GAAyB,aAAa,KAAK,YAAL,CAAkB,IAAxD;AACA,SAAK,aAAL,CAAmB,IAAnB,GAA0B,cAAc,KAAK,aAAL,CAAmB,IAA3D;AAEA,SAAK,SAAL,GAAiB,IAAI,CAAC,SAAL,KAAmB,SAAnB,GACb,gCADa,GAEb,IAAI,CAAC,SAFT;AAGA,IAAA,2BAA2B,CAAC,KAAK,SAAN,CAA3B;;AACA,QAAI,IAAI,CAAC,OAAT,EAAkB;AAChB,YAAM,IAAI,mBAAJ,CACF,iEADE,CAAN;AAED;;AACD,SAAK,SAAL,GAAiB,IAAI,CAAC,KAAL,CAAW,QAA5B;AACA,SAAK,eAAL,GAAuB,IAAI,CAAC,KAAL,CAAW,eAAlC;AACA,SAAK,WAAL,GAAmB,IAAI,CAAC,KAAL,CAAW,WAA9B;AACA,SAAK,eAAL,GAAuB,IAAvB;AACA,SAAK,UAAL,GAAkB,IAAlB;AACA,SAAK,SAAL,GAAiB,IAAI,CAAC,KAAL,CAAW,SAA5B;AACA,SAAK,YAAL,GAAoB,IAApB;AACD;;AAEY,MAAT,SAAS,GAAA;AACX,WAAO,KAAK,UAAZ;AACD;;AAEY,MAAT,SAAS,CAAC,KAAD,EAAe;AAC1B;AACA;AACA;AACA,SAAK,UAAL,GAAkB,KAAlB;;AACA,QAAI,KAAK,YAAL,IAAqB,IAAzB,EAA+B;AAC7B,WAAK,YAAL,CAAkB,SAAlB,GAA8B,KAA9B;AACD;;AACD,QAAI,KAAK,aAAL,IAAsB,IAA1B,EAAgC;AAC9B,WAAK,aAAL,CAAmB,SAAnB,GAA+B,KAA/B;AACD;AACF;;AAED,EAAA,UAAU,GAAA;AACR,WAAO,KAAK,YAAL,CAAkB,UAAlB,GAA+B,MAA/B,CACH,KAAK,aAAL,CAAmB,UAAnB,EADG,CAAP;AAED;;AAED,EAAA,UAAU,CAAC,OAAD,EAAkB;AAC1B,UAAM,UAAU,GAAG,OAAO,CAAC,MAA3B;AACA,UAAM,cAAc,GAAG,IAAI,CAAC,KAAL,CAAW,UAAU,GAAG,CAAxB,CAAvB;AACA,SAAK,YAAL,CAAkB,UAAlB,CAA6B,OAAO,CAAC,KAAR,CAAc,CAAd,EAAiB,cAAjB,CAA7B;AACA,SAAK,aAAL,CAAmB,UAAnB,CAA8B,OAAO,CAAC,KAAR,CAAc,cAAd,CAA9B;AACD;;AAED,EAAA,kBAAkB,CAAC,UAAD,EAA0B;AAC1C,QAAI,WAAW,GACX,KAAK,YAAL,CAAkB,kBAAlB,CAAqC,UAArC,CADJ;;AAEA,QAAI,EAAE,KAAK,CAAC,OAAN,CAAc,WAAd,KAA8B,KAAK,CAAC,OAAN,CAAc,WAAW,CAAC,CAAD,CAAzB,CAAhC,CAAJ,EAAoE;AAClE,MAAA,WAAW,GAAG,CAAC,WAAD,CAAd;AACD;;AACD,IAAA,WAAW,GAAG,WAAd;AAEA,QAAI,WAAJ;AACA,QAAI,YAAJ;AACA,QAAI,UAAJ;;AACA,QAAI,KAAK,WAAT,EAAsB;AACpB,MAAA,UAAU,GAAG,WAAW,CAAC,KAAZ,CAAkB,CAAlB,CAAb;AACA,MAAA,WAAW,GAAG,WAAW,CAAC,CAAD,CAAzB;AACD,KAHD,MAGO;AACL,MAAA,WAAW,GAAG,WAAW,CAAC,CAAD,CAAzB;AACD;;AACD,IAAA,WAAW,GAAG,WAAd;;AACA,QAAI,KAAK,SAAL,KAAmB,QAAvB,EAAiC;AAC/B,MAAA,WAAW,CAAC,WAAW,CAAC,MAAZ,GAAqB,CAAtB,CAAX,IAAuC,CAAvC;AACA,MAAA,YAAY,GAAG,CAAC,WAAD,CAAf;AACD,KAHD,MAGO,IAAI,KAAK,SAAL,IAAkB,IAAtB,EAA4B;AACjC,MAAA,YAAY,GAAG,CAAC,WAAD,EAAc,WAAW,CAAC,KAAZ,EAAd,CAAf;AACD,KAFM,MAEA;AACL,MAAA,YAAY,GAAG,CAAC,WAAD,CAAf;AACD;;AAED,QAAI,KAAK,WAAT,EAAsB;AACpB,UAAI,KAAK,SAAL,IAAkB,IAAtB,EAA4B;AAC1B,eAAO,YAAY,CAAC,MAAb,CAAoB,UAApB,EAAgC,MAAhC,CAAuC,UAAU,CAAC,KAAX,EAAvC,CAAP;AACD;;AACD,aAAO,CAAC,WAAD,EAAc,MAAd,CAAqB,UAArB,EAAiC,MAAjC,CAAwC,UAAU,CAAC,KAAX,EAAxC,CAAP;AACD;;AACD,WAAO,aAAa,CAAC,gBAAd,CAA+B,YAA/B,CAAP;AACD;;AAED,EAAA,KAAK,CACD,MADC,EAED,MAFC,EAEc;AACjB,QAAI,YAAY,GACZ,MAAM,IAAI,IAAV,GAAiB,IAAjB,GAAwB,MAAM,CAAC,cAAD,CADlC;AAEA,QAAI,SAAS,GACT,MAAM,IAAI,IAAV,GAAiB,IAAjB,GAAwB,MAAM,CAAC,WAAD,CADlC;;AAEA,QAAI,MAAM,IAAI,IAAd,EAAoB;AAClB,MAAA,MAAM,GAAG,EAAT;AACD;;AACD,UAAM,YAAY,GACd,eAAe,CAAC,MAAD,EAAS,YAAT,EAAuB,SAAvB,EAAkC,KAAK,YAAvC,CADnB;AAEA,IAAA,MAAM,GAAG,YAAY,CAAC,MAAtB;AACA,IAAA,YAAY,GAAG,YAAY,CAAC,YAA5B;AACA,IAAA,SAAS,GAAG,YAAY,CAAC,SAAzB;;AAEA,QAAI,KAAK,CAAC,OAAN,CAAc,MAAd,CAAJ,EAA2B;AACzB,MAAA,YAAY,GAAI,MAAsC,CAAC,KAAvC,CAA6C,CAA7C,CAAhB;AACA,MAAA,MAAM,GAAI,MAAsC,CAAC,CAAD,CAAhD;AACD;;AAED,QAAI,CAAC,YAAY,IAAI,IAAhB,IAAwB,YAAY,CAAC,MAAb,KAAwB,CAAjD,KACA,SAAS,IAAI,IADjB,EACuB;AACrB,aAAO,MAAM,KAAN,CAAY,MAAZ,EAAoB,MAApB,CAAP;AACD;;AACD,UAAM,gBAAgB,GAAiC,EAAvD;AACA,UAAM,eAAe,GAAgB,EAArC;;AACA,QAAI,YAAY,IAAI,IAApB,EAA0B;AACxB,YAAM,SAAS,GAAG,YAAY,CAAC,MAA/B;;AACA,UAAI,SAAS,GAAG,CAAZ,GAAgB,CAApB,EAAuB;AACrB,cAAM,IAAI,UAAJ,CACF,wDACA,wDADA,GAEA,sBAHE,CAAN;AAID;;AACD,MAAA,MAAM,CAAC,cAAD,CAAN,GAAyB,YAAzB;AACA,MAAA,gBAAgB,CAAC,IAAjB,CAAsB,GAAG,YAAzB;AACA,YAAM,UAAU,GAAI,YAA6C,CACzC,GADJ,CACQ,KAAK,IAAI,IAAI,SAAJ,CAAc;AAAC,QAAA,KAAK,EAAE,KAAK,CAAC;AAAd,OAAd,CADjB,CAApB;AAEA,WAAK,YAAL,CAAkB,SAAlB,GAA8B,UAAU,CAAC,KAAX,CAAiB,CAAjB,EAAoB,SAAS,GAAG,CAAhC,CAA9B;AACA,WAAK,aAAL,CAAmB,SAAnB,GAA+B,UAAU,CAAC,KAAX,CAAiB,SAAS,GAAG,CAA7B,CAA/B;AACA,MAAA,eAAe,CAAC,IAAhB,CAAqB,GAAG,UAAxB;AACD;;AACD,QAAI,SAAS,IAAI,IAAjB,EAAuB;AACrB,YAAM,IAAI,mBAAJ,CACF,0DACA,kBAFE,CAAN;AAGD;;AAED,UAAM,gBAAgB,GAAG,gBAAgB,CAAC,CAAD,CAAhB,YAA+B,cAAxD;;AACA,SAAK,MAAM,MAAX,IAAqB,gBAArB,EAAuC;AACrC,UAAI,MAAM,YAAY,cAAlB,KAAqC,gBAAzC,EAA2D;AACzD,cAAM,IAAI,UAAJ,CACF,0DACA,yDAFE,CAAN;AAGD;AACF;;AAED,QAAI,gBAAJ,EAAsB;AACpB;AACA,YAAM,SAAS,GAAG,CAAC,MAAD,EAAS,MAAT,CAAgB,gBAAhB,CAAlB;AACA,YAAM,aAAa,GAAG,KAAK,SAAL,CAAe,MAAf,CAAsB,eAAtB,CAAtB,CAHoB,CAIpB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA,YAAM,iBAAiB,GAAG,KAAK,SAA/B;AACA,WAAK,SAAL,GAAiB,aAAjB;AACA,YAAM,MAAM,GACR,MAAM,KAAN,CAAY,SAAZ,EAAsD,MAAtD,CADJ;AAEA,WAAK,SAAL,GAAiB,iBAAjB;AACA,aAAO,MAAP;AACD,KAnBD,MAmBO;AACL,aAAO,MAAM,KAAN,CAAY,MAAZ,EAAoB,MAApB,CAAP;AACD;AACF;;AAED,EAAA,IAAI,CAAC,MAAD,EAA0B,MAA1B,EAAwC;AAC1C,WAAO,IAAI,CAAC,MAAK;AACf,YAAM,YAAY,GAAG,MAAM,CAAC,cAAD,CAA3B;AAEA,UAAI,CAAJ;AACA,UAAI,IAAJ;;AACA,UAAI,YAAY,IAAI,IAApB,EAA0B;AACxB,QAAA,CAAC,GAAG,KAAK,YAAL,CAAkB,IAAlB,CAAuB,MAAvB,EAA+B,MAA/B,CAAJ;AACA,QAAA,IAAI,GAAG,KAAK,aAAL,CAAmB,IAAnB,CAAwB,MAAxB,EAAgC,MAAhC,CAAP;AACD,OAHD,MAGO;AACL,cAAM,YAAY,GAAG,YAAY,CAAC,KAAb,CAAmB,CAAnB,EAAsB,YAAY,CAAC,MAAb,GAAsB,CAA5C,CAArB;AACA,cAAM,aAAa,GAAG,YAAY,CAAC,KAAb,CAAmB,YAAY,CAAC,MAAb,GAAsB,CAAzC,CAAtB;AACA,QAAA,CAAC,GAAG,KAAK,YAAL,CAAkB,IAAlB,CACA,MADA,EACQ,MAAM,CAAC,MAAP,CAAc,MAAd,EAAsB;AAAC,UAAA,YAAY,EAAE;AAAf,SAAtB,CADR,CAAJ;AAEA,QAAA,IAAI,GAAG,KAAK,aAAL,CAAmB,IAAnB,CACH,MADG,EACK,MAAM,CAAC,MAAP,CAAc,MAAd,EAAsB;AAAC,UAAA,YAAY,EAAE;AAAf,SAAtB,CADL,CAAP;AAED;;AAED,UAAI,MAAJ;;AACA,UAAI,KAAK,WAAT,EAAsB;AACpB,YAAI,KAAK,CAAC,OAAN,CAAc,CAAd,CAAJ,EAAsB;AACpB,UAAA,MAAM,GAAG,CAAC,CAAC,KAAF,CAAQ,CAAR,EAAW,MAAX,CAAmB,IAAiB,CAAC,KAAlB,CAAwB,CAAxB,CAAnB,CAAT;AACD,SAFD,MAEO,CACN;;AACD,QAAA,CAAC,GAAI,CAAc,CAAC,CAAD,CAAnB;AACA,QAAA,IAAI,GAAI,IAAiB,CAAC,CAAD,CAAzB;AACD;;AAED,UAAI,KAAK,eAAT,EAA0B;AACxB,QAAA,IAAI,GAAG,GAAG,CAAC,OAAJ,CAAY,IAAZ,EAA4B,CAA5B,CAAP;AACD;;AAED,UAAI,MAAJ;;AACA,UAAI,KAAK,SAAL,KAAmB,QAAvB,EAAiC;AAC/B,QAAA,MAAM,GAAG,CAAC,CAAC,WAAF,CAAc,CAAC,CAAD,EAAc,IAAd,CAAd,CAAT;AACD,OAFD,MAEO,IAAI,KAAK,SAAL,KAAmB,KAAvB,EAA8B;AACnC,QAAA,MAAM,GAAG,GAAG,CAAC,GAAJ,CAAQ,CAAR,EAAqB,IAArB,CAAT;AACD,OAFM,MAEA,IAAI,KAAK,SAAL,KAAmB,KAAvB,EAA8B;AACnC,QAAA,MAAM,GAAG,GAAG,CAAC,GAAJ,CAAQ,EAAR,EAAY,GAAG,CAAC,GAAJ,CAAQ,CAAR,EAAqB,IAArB,CAAZ,CAAT;AACD,OAFM,MAEA,IAAI,KAAK,SAAL,KAAmB,KAAvB,EAA8B;AACnC,QAAA,MAAM,GAAG,GAAG,CAAC,GAAJ,CAAQ,CAAR,EAAqB,IAArB,CAAT;AACD,OAFM,MAEA,IAAI,KAAK,SAAL,IAAkB,IAAtB,EAA4B;AACjC,QAAA,MAAM,GAAG,CAAC,CAAD,EAAc,IAAd,CAAT;AACD,OA1Cc,CA4Cf;;;AACA,UAAI,KAAK,WAAT,EAAsB;AACpB,YAAI,KAAK,SAAL,IAAkB,IAAtB,EAA4B;AAC1B,iBAAQ,MAAmB,CAAC,MAApB,CAA2B,MAA3B,CAAR;AACD;;AACD,eAAO,CAAC,MAAD,EAAmB,MAAnB,CAA0B,MAA1B,CAAP;AACD;;AACD,aAAO,MAAP;AACD,KApDU,CAAX;AAqDD;;AAED,EAAA,WAAW,CAAC,MAAD,EAAyB;AAClC,SAAK,YAAL,CAAkB,WAAlB;AACA,SAAK,aAAL,CAAmB,WAAnB;AACD;;AAED,EAAA,KAAK,CAAC,UAAD,EAA0B;AAC7B,IAAA,SAAS,CAAC,KAAK,YAAL,CAAkB,IAAnB,EAAyB,MAAK;AACrC,WAAK,YAAL,CAAkB,KAAlB,CAAwB,UAAxB;AACD,KAFQ,CAAT;AAGA,IAAA,SAAS,CAAC,KAAK,aAAL,CAAmB,IAApB,EAA0B,MAAK;AACtC,WAAK,aAAL,CAAmB,KAAnB,CAAyB,UAAzB;AACD,KAFQ,CAAT;AAGA,SAAK,KAAL,GAAa,IAAb;AACD;;AAED,EAAA,WAAW,CAAC,MAAD,EAA0B,IAA1B,EAAgD;AAEzD,QAAI,KAAK,CAAC,OAAN,CAAc,IAAd,CAAJ,EAAyB;AACvB,MAAA,IAAI,GAAG,IAAI,CAAC,CAAD,CAAX;AACD;;AACD,QAAI,UAAJ;;AACA,QAAI,KAAK,eAAT,EAA0B;AACxB,UAAI,KAAK,SAAL,IAAkB,IAAtB,EAA4B;AAC1B,QAAA,UAAU,GAAG,CAAC,IAAD,EAAO,IAAP,CAAb;AACD,OAFD,MAEO;AACL,QAAA,UAAU,GAAG,IAAb;AACD;AACF,KAND,MAMO;AACL,UAAI,KAAK,SAAL,IAAkB,IAAtB,EAA4B;AAC1B,QAAA,UAAU,GAAG,CAAC,IAAD,EAAO,IAAP,CAAb;AACD,OAFD,MAEO;AACL,QAAA,UAAU,GAAG,IAAb;AACD;AACF;;AACD,QAAI,KAAK,WAAT,EAAsB;AACpB,YAAM,MAAM,GAAG,KAAK,YAAL,CAAkB,MAAjC;AACA,YAAM,SAAS,GAAa,MAAM,CAAC,GAAP,CAAW,KAAK,IAAI,IAApB,CAA5B;;AACA,UAAI,KAAK,CAAC,OAAN,CAAc,UAAd,CAAJ,EAA+B;AAC7B,eAAO,UAAU,CAAC,MAAX,CAAkB,SAAlB,EAA6B,MAA7B,CAAoC,SAApC,CAAP;AACD,OAFD,MAEO;AACL,eAAO,CAAC,UAAD,EAAa,MAAb,CAAoB,SAApB,EAA+B,MAA/B,CAAsC,SAAtC,CAAP;AACD;AACF,KARD,MAQO;AACL,aAAO,UAAP;AACD;AACF;;AAEmB,MAAhB,gBAAgB,GAAA;AAClB,WAAO,KAAK,YAAL,CAAkB,gBAAlB,CAAmC,MAAnC,CACH,KAAK,aAAL,CAAmB,gBADhB,CAAP;AAED;;AAEsB,MAAnB,mBAAmB,GAAA;AACrB,WAAO,KAAK,YAAL,CAAkB,mBAAlB,CAAsC,MAAtC,CACH,KAAK,aAAL,CAAmB,mBADhB,CAAP;AAED,GAvTuC,CAyTxC;;;AAEA,EAAA,4BAA4B,CAAC,KAAD,EAAe;AACzC,UAAM,4BAAN,CAAmC,KAAnC;;AACA,QAAI,KAAK,YAAL,IAAqB,IAAzB,EAA+B;AAC7B,WAAK,YAAL,CAAkB,4BAAlB,CAA+C,KAA/C;AACD;;AACD,QAAI,KAAK,aAAL,IAAsB,IAA1B,EAAgC;AAC9B,WAAK,aAAL,CAAmB,4BAAnB,CAAgD,KAAhD;AACD;AACF;;AAED,EAAA,SAAS,GAAA;AACP,UAAM,MAAM,GAA6B;AACvC,mBAAa,KAAK;AADqB,KAAzC,CADO,CAIP;;AACA,UAAM,UAAU,GAAG,MAAM,SAAN,EAAnB;AACA,IAAA,MAAM,CAAC,MAAP,CAAc,MAAd,EAAsB,UAAtB;AACA,WAAO,MAAP;AACD;AAED;;;AACiB,SAAV,UAAU,CACb,GADa,EAEb,MAFa,EAEmB;AAClC,UAAM,QAAQ,GACV,WAAW,CAAC,MAAM,CAAC,OAAD,CAAP,CADf;AAEA,WAAO,MAAM,CAAC,OAAD,CAAb,CAHkC,CAIlC;;AACA,QAAI,MAAM,CAAC,cAAD,CAAN,IAA0B,IAA9B,EAAoC;AAClC,YAAM,IAAI,mBAAJ,CACF,6DAAA,GACA,+BAFE,CAAN;AAGD,KATiC,CAUlC;;;AACA,UAAM,SAAS,GAAyB,MAAxC;AACA,IAAA,SAAS,CAAC,OAAD,CAAT,GAAqB,QAArB;AACA,WAAO,IAAI,GAAJ,CAAQ,SAAR,CAAP;AACD;;AAhWuC;AACxC;;AACO,aAAA,CAAA,SAAA,GAAY,eAAZ;AAgWT,aAAa,CAAC,aAAd,CAA4B,aAA5B","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * Layers that augment the functionality of a base layer.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { serialization, tidy } from '@tensorflow/tfjs-core';\nimport * as K from '../backend/tfjs_backend';\nimport { nameScope } from '../common';\nimport { InputSpec, Layer, SymbolicTensor } from '../engine/topology';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { VALID_BIDIRECTIONAL_MERGE_MODES } from '../keras_format/common';\nimport * as generic_utils from '../utils/generic_utils';\nimport { getExactlyOneShape, getExactlyOneTensor } from '../utils/types_utils';\nimport { rnn, standardizeArgs } from './recurrent';\nimport { deserialize } from './serialization';\n/**\n * Abstract wrapper base class.\n *\n * Wrappers take another layer and augment it in various ways.\n * Do not use this class as a layer, it is only an abstract base class.\n * Two usable wrappers are the `TimeDistributed` and `Bidirectional` wrappers.\n */\nexport class Wrapper extends Layer {\n    constructor(args) {\n        // Porting Note: In PyKeras, `self.layer` is set prior to the calling\n        //   `super()`. But we can't do that here due to TypeScript's restriction.\n        //   See: https://github.com/Microsoft/TypeScript/issues/8277\n        //   As a result, we have to add checks in `get trainable()` and\n        //   `set trainable()` below in order to prevent using `this.layer` when\n        //   its value is `undefined`. The super constructor does use the getter\n        //   and the setter of `this.layer`.\n        super(args);\n        this.layer = args.layer;\n    }\n    build(inputShape) {\n        this.built = true;\n    }\n    // TODO(cais): Implement activityRegularizer getter.\n    get trainable() {\n        // Porting Note: the check of `this.layer` here is necessary due to the\n        //   way the `constructor` of this class is written (see Porting Note\n        //   above).\n        if (this.layer != null) {\n            return this.layer.trainable;\n        }\n        else {\n            return false;\n        }\n    }\n    set trainable(value) {\n        // Porting Note: the check of `this.layer` here is necessary due to the\n        //   way the `constructor` of this class is written (see Porting Note\n        //   above).\n        if (this.layer != null) {\n            this.layer.trainable = value;\n        }\n    }\n    get trainableWeights() {\n        return this.layer.trainableWeights;\n    }\n    // TODO(cais): Implement setter for trainableWeights.\n    get nonTrainableWeights() {\n        return this.layer.nonTrainableWeights;\n    }\n    // TODO(cais): Implement setter for nonTrainableWeights.\n    get updates() {\n        // tslint:disable-next-line:no-any\n        return this.layer._updates;\n    }\n    // TODO(cais): Implement getUpdatesFor().\n    get losses() {\n        return this.layer.losses;\n    }\n    // TODO(cais): Implement getLossesFor().\n    getWeights() {\n        return this.layer.getWeights();\n    }\n    setWeights(weights) {\n        this.layer.setWeights(weights);\n    }\n    getConfig() {\n        const config = {\n            'layer': {\n                'className': this.layer.getClassName(),\n                'config': this.layer.getConfig(),\n            }\n        };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n    setFastWeightInitDuringBuild(value) {\n        super.setFastWeightInitDuringBuild(value);\n        if (this.layer != null) {\n            this.layer.setFastWeightInitDuringBuild(value);\n        }\n    }\n    /** @nocollapse */\n    static fromConfig(cls, config, customObjects = {}) {\n        const layerConfig = config['layer'];\n        const layer = deserialize(layerConfig, customObjects);\n        delete config['layer'];\n        const newConfig = { layer };\n        Object.assign(newConfig, config);\n        return new cls(newConfig);\n    }\n}\nexport class TimeDistributed extends Wrapper {\n    constructor(args) {\n        super(args);\n        this.supportsMasking = true;\n    }\n    build(inputShape) {\n        inputShape = getExactlyOneShape(inputShape);\n        if (inputShape.length < 3) {\n            throw new ValueError(`TimeDistributed layer expects an input shape >= 3D, but received ` +\n                `input shape ${JSON.stringify(inputShape)}`);\n        }\n        this.inputSpec = [{ shape: inputShape }];\n        const childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n        if (!this.layer.built) {\n            this.layer.build(childInputShape);\n            this.layer.built = true;\n        }\n        super.build(inputShape);\n    }\n    computeOutputShape(inputShape) {\n        inputShape = getExactlyOneShape(inputShape);\n        const childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n        const childOutputShape = this.layer.computeOutputShape(childInputShape);\n        const timesteps = inputShape[1];\n        return [childOutputShape[0], timesteps].concat(childOutputShape.slice(1));\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            // TODO(cais): Add 'training' and 'useLearningPhase' to kwargs.\n            inputs = getExactlyOneTensor(inputs);\n            // Porting Note: In tfjs-layers, `inputs` are always concrete tensor\n            // values. Hence the inputs can't have an undetermined first (batch)\n            // dimension, which is why we always use the K.rnn approach here.\n            const step = (inputs, states) => {\n                // TODO(cais): Add useLearningPhase.\n                // NOTE(cais): `layer.call` may return a length-1 array of Tensor in\n                //   some cases (e.g., `layer` is a `Sequential` instance), which is\n                //   why `getExactlyOneTensor` is used below.\n                const output = getExactlyOneTensor(this.layer.call(inputs, kwargs));\n                return [output, []];\n            };\n            const rnnOutputs = rnn(step, inputs, [], false /* goBackwards */, null /* mask */, null /* constants */, false /* unroll */, true /* needPerStepOutputs */);\n            const y = rnnOutputs[1];\n            // TODO(cais): Add activity regularization.\n            // TODO(cais): Add useLearningPhase.\n            return y;\n        });\n    }\n}\n/** @nocollapse */\nTimeDistributed.className = 'TimeDistributed';\nserialization.registerClass(TimeDistributed);\nexport function checkBidirectionalMergeMode(value) {\n    generic_utils.checkStringTypeUnionValue(VALID_BIDIRECTIONAL_MERGE_MODES, 'BidirectionalMergeMode', value);\n}\nconst DEFAULT_BIDIRECTIONAL_MERGE_MODE = 'concat';\nexport class Bidirectional extends Wrapper {\n    constructor(args) {\n        super(args);\n        // Note: When creating `this.forwardLayer`, the original Layer object\n        //   (`config.layer`) ought to be cloned. This is why we call\n        //   `getConfig()` followed by `deserialize()`. Without this cloning,\n        //   the layer names saved during serialization will incorrectly contain\n        //   the 'forward_' prefix. In Python Keras, this is done using\n        //   `copy.copy` (shallow copy), which does not have a simple equivalent\n        //   in JavaScript. JavaScript's `Object.assign()` does not copy\n        //   methods.\n        const layerConfig = args.layer.getConfig();\n        const forwDict = {};\n        forwDict['className'] = args.layer.getClassName();\n        forwDict['config'] = layerConfig;\n        this.forwardLayer = deserialize(forwDict);\n        layerConfig['goBackwards'] =\n            layerConfig['goBackwards'] === true ? false : true;\n        const backDict = {};\n        backDict['className'] = args.layer.getClassName();\n        backDict['config'] = layerConfig;\n        this.backwardLayer = deserialize(backDict);\n        this.forwardLayer.name = 'forward_' + this.forwardLayer.name;\n        this.backwardLayer.name = 'backward_' + this.backwardLayer.name;\n        this.mergeMode = args.mergeMode === undefined ?\n            DEFAULT_BIDIRECTIONAL_MERGE_MODE :\n            args.mergeMode;\n        checkBidirectionalMergeMode(this.mergeMode);\n        if (args.weights) {\n            throw new NotImplementedError('weights support is not implemented for Bidirectional layer yet.');\n        }\n        this._stateful = args.layer.stateful;\n        this.returnSequences = args.layer.returnSequences;\n        this.returnState = args.layer.returnState;\n        this.supportsMasking = true;\n        this._trainable = true;\n        this.inputSpec = args.layer.inputSpec;\n        this.numConstants = null;\n    }\n    get trainable() {\n        return this._trainable;\n    }\n    set trainable(value) {\n        // Porting Note: the check of `this.layer` here is necessary due to the\n        //   way the `constructor` of this class is written (see Porting Note\n        //   above).\n        this._trainable = value;\n        if (this.forwardLayer != null) {\n            this.forwardLayer.trainable = value;\n        }\n        if (this.backwardLayer != null) {\n            this.backwardLayer.trainable = value;\n        }\n    }\n    getWeights() {\n        return this.forwardLayer.getWeights().concat(this.backwardLayer.getWeights());\n    }\n    setWeights(weights) {\n        const numWeights = weights.length;\n        const numeightsOver2 = Math.floor(numWeights / 2);\n        this.forwardLayer.setWeights(weights.slice(0, numeightsOver2));\n        this.backwardLayer.setWeights(weights.slice(numeightsOver2));\n    }\n    computeOutputShape(inputShape) {\n        let layerShapes = this.forwardLayer.computeOutputShape(inputShape);\n        if (!(Array.isArray(layerShapes) && Array.isArray(layerShapes[0]))) {\n            layerShapes = [layerShapes];\n        }\n        layerShapes = layerShapes;\n        let outputShape;\n        let outputShapes;\n        let stateShape;\n        if (this.returnState) {\n            stateShape = layerShapes.slice(1);\n            outputShape = layerShapes[0];\n        }\n        else {\n            outputShape = layerShapes[0];\n        }\n        outputShape = outputShape;\n        if (this.mergeMode === 'concat') {\n            outputShape[outputShape.length - 1] *= 2;\n            outputShapes = [outputShape];\n        }\n        else if (this.mergeMode == null) {\n            outputShapes = [outputShape, outputShape.slice()];\n        }\n        else {\n            outputShapes = [outputShape];\n        }\n        if (this.returnState) {\n            if (this.mergeMode == null) {\n                return outputShapes.concat(stateShape).concat(stateShape.slice());\n            }\n            return [outputShape].concat(stateShape).concat(stateShape.slice());\n        }\n        return generic_utils.singletonOrArray(outputShapes);\n    }\n    apply(inputs, kwargs) {\n        let initialState = kwargs == null ? null : kwargs['initialState'];\n        let constants = kwargs == null ? null : kwargs['constants'];\n        if (kwargs == null) {\n            kwargs = {};\n        }\n        const standardized = standardizeArgs(inputs, initialState, constants, this.numConstants);\n        inputs = standardized.inputs;\n        initialState = standardized.initialState;\n        constants = standardized.constants;\n        if (Array.isArray(inputs)) {\n            initialState = inputs.slice(1);\n            inputs = inputs[0];\n        }\n        if ((initialState == null || initialState.length === 0) &&\n            constants == null) {\n            return super.apply(inputs, kwargs);\n        }\n        const additionalInputs = [];\n        const additionalSpecs = [];\n        if (initialState != null) {\n            const numStates = initialState.length;\n            if (numStates % 2 > 0) {\n                throw new ValueError('When passing `initialState` to a Bidrectional RNN, ' +\n                    'the state should be an Array containing the states of ' +\n                    'the underlying RNNs.');\n            }\n            kwargs['initialState'] = initialState;\n            additionalInputs.push(...initialState);\n            const stateSpecs = initialState\n                .map(state => new InputSpec({ shape: state.shape }));\n            this.forwardLayer.stateSpec = stateSpecs.slice(0, numStates / 2);\n            this.backwardLayer.stateSpec = stateSpecs.slice(numStates / 2);\n            additionalSpecs.push(...stateSpecs);\n        }\n        if (constants != null) {\n            throw new NotImplementedError('Support for constants in Bidirectional layers is not ' +\n                'implemented yet.');\n        }\n        const isSymbolicTensor = additionalInputs[0] instanceof SymbolicTensor;\n        for (const tensor of additionalInputs) {\n            if (tensor instanceof SymbolicTensor !== isSymbolicTensor) {\n                throw new ValueError('The initial state of a Bidirectional layer cannot be ' +\n                    'specified as a mix of symbolic and non-symbolic tensors');\n            }\n        }\n        if (isSymbolicTensor) {\n            // Compute the full input and specs, including the states.\n            const fullInput = [inputs].concat(additionalInputs);\n            const fullInputSpec = this.inputSpec.concat(additionalSpecs);\n            // Perform the call temporarily and replace inputSpec.\n            // Note: with initial states symbolic calls and non-symbolic calls to\n            // this method differ in how the initial states are passed. For\n            // symbolic calls, the initial states are passed in the first arg, as\n            // an Array of SymbolicTensors; for non-symbolic calls, they are\n            // passed in the second arg as a part of the kwargs. Hence the need to\n            // temporarily modify inputSpec here.\n            // TODO(cais): Make refactoring so that this hacky code below is no\n            // longer needed.\n            const originalInputSpec = this.inputSpec;\n            this.inputSpec = fullInputSpec;\n            const output = super.apply(fullInput, kwargs);\n            this.inputSpec = originalInputSpec;\n            return output;\n        }\n        else {\n            return super.apply(inputs, kwargs);\n        }\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            const initialState = kwargs['initialState'];\n            let y;\n            let yRev;\n            if (initialState == null) {\n                y = this.forwardLayer.call(inputs, kwargs);\n                yRev = this.backwardLayer.call(inputs, kwargs);\n            }\n            else {\n                const forwardState = initialState.slice(0, initialState.length / 2);\n                const backwardState = initialState.slice(initialState.length / 2);\n                y = this.forwardLayer.call(inputs, Object.assign(kwargs, { initialState: forwardState }));\n                yRev = this.backwardLayer.call(inputs, Object.assign(kwargs, { initialState: backwardState }));\n            }\n            let states;\n            if (this.returnState) {\n                if (Array.isArray(y)) {\n                    states = y.slice(1).concat(yRev.slice(1));\n                }\n                else {\n                }\n                y = y[0];\n                yRev = yRev[0];\n            }\n            if (this.returnSequences) {\n                yRev = tfc.reverse(yRev, 1);\n            }\n            let output;\n            if (this.mergeMode === 'concat') {\n                output = K.concatenate([y, yRev]);\n            }\n            else if (this.mergeMode === 'sum') {\n                output = tfc.add(y, yRev);\n            }\n            else if (this.mergeMode === 'ave') {\n                output = tfc.mul(.5, tfc.add(y, yRev));\n            }\n            else if (this.mergeMode === 'mul') {\n                output = tfc.mul(y, yRev);\n            }\n            else if (this.mergeMode == null) {\n                output = [y, yRev];\n            }\n            // TODO(cais): Properly set learning phase.\n            if (this.returnState) {\n                if (this.mergeMode == null) {\n                    return output.concat(states);\n                }\n                return [output].concat(states);\n            }\n            return output;\n        });\n    }\n    resetStates(states) {\n        this.forwardLayer.resetStates();\n        this.backwardLayer.resetStates();\n    }\n    build(inputShape) {\n        nameScope(this.forwardLayer.name, () => {\n            this.forwardLayer.build(inputShape);\n        });\n        nameScope(this.backwardLayer.name, () => {\n            this.backwardLayer.build(inputShape);\n        });\n        this.built = true;\n    }\n    computeMask(inputs, mask) {\n        if (Array.isArray(mask)) {\n            mask = mask[0];\n        }\n        let outputMask;\n        if (this.returnSequences) {\n            if (this.mergeMode == null) {\n                outputMask = [mask, mask];\n            }\n            else {\n                outputMask = mask;\n            }\n        }\n        else {\n            if (this.mergeMode == null) {\n                outputMask = [null, null];\n            }\n            else {\n                outputMask = null;\n            }\n        }\n        if (this.returnState) {\n            const states = this.forwardLayer.states;\n            const stateMask = states.map(state => null);\n            if (Array.isArray(outputMask)) {\n                return outputMask.concat(stateMask).concat(stateMask);\n            }\n            else {\n                return [outputMask].concat(stateMask).concat(stateMask);\n            }\n        }\n        else {\n            return outputMask;\n        }\n    }\n    get trainableWeights() {\n        return this.forwardLayer.trainableWeights.concat(this.backwardLayer.trainableWeights);\n    }\n    get nonTrainableWeights() {\n        return this.forwardLayer.nonTrainableWeights.concat(this.backwardLayer.nonTrainableWeights);\n    }\n    // TODO(cais): Implement constraints().\n    setFastWeightInitDuringBuild(value) {\n        super.setFastWeightInitDuringBuild(value);\n        if (this.forwardLayer != null) {\n            this.forwardLayer.setFastWeightInitDuringBuild(value);\n        }\n        if (this.backwardLayer != null) {\n            this.backwardLayer.setFastWeightInitDuringBuild(value);\n        }\n    }\n    getConfig() {\n        const config = {\n            'mergeMode': this.mergeMode,\n        };\n        // TODO(cais): Add logic for `numConstants` once the property is added.\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n    /** @nocollapse */\n    static fromConfig(cls, config) {\n        const rnnLayer = deserialize(config['layer']);\n        delete config['layer'];\n        // TODO(cais): Add logic for `numConstants` once the property is added.\n        if (config['numConstants'] != null) {\n            throw new NotImplementedError(`Deserialization of a Bidirectional layer with numConstants ` +\n                `present is not supported yet.`);\n        }\n        // tslint:disable-next-line:no-any\n        const newConfig = config;\n        newConfig['layer'] = rnnLayer;\n        return new cls(newConfig);\n    }\n}\n/** @nocollapse */\nBidirectional.className = 'Bidirectional';\nserialization.registerClass(Bidirectional);\n//# sourceMappingURL=wrappers.js.map"]},"metadata":{},"sourceType":"module"}